{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction What is Dynamite Network Security Monitor? DynamiteNSM is a lightweight, versatile network security monitor designed to make securing your network environment simple and intuitive. It has two main components: agent and monitor . Dynamite agents can easily be deployed to monitor dedicated network segments while the monitor provides a powerful query interface and in-depth analytics. Read More \u00bb Checkout the Dashboards DynamiteNSM ships with a powerful set of visualizations, saved queries, and dashboards purpose built to provide useful analytics for a variety of operational and security use-cases. Read More \u00bb Dive Right In. Learn by doing. Read our QuickStart Guide , and quickly set up a working environment perfect for monitoring a home network or lab environment. Read More \u00bb Contribute Whether you have a cool detection, visualisation, or SDK contribution we welcome collaboration! Read More \u00bb","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#what-is-dynamite-network-security-monitor","text":"DynamiteNSM is a lightweight, versatile network security monitor designed to make securing your network environment simple and intuitive. It has two main components: agent and monitor . Dynamite agents can easily be deployed to monitor dedicated network segments while the monitor provides a powerful query interface and in-depth analytics. Read More \u00bb","title":"What is Dynamite Network Security Monitor?"},{"location":"#checkout-the-dashboards","text":"DynamiteNSM ships with a powerful set of visualizations, saved queries, and dashboards purpose built to provide useful analytics for a variety of operational and security use-cases. Read More \u00bb","title":"Checkout the Dashboards"},{"location":"#dive-right-in","text":"Learn by doing. Read our QuickStart Guide , and quickly set up a working environment perfect for monitoring a home network or lab environment. Read More \u00bb","title":"Dive Right In."},{"location":"#contribute","text":"Whether you have a cool detection, visualisation, or SDK contribution we welcome collaboration! Read More \u00bb","title":"Contribute"},{"location":"about/01_project_goals/","text":"Project Goals Passive network monitoring is an approach to network monitoring where traffic is sniffed via strategically placed sensors on critical junctions of a network. DynamiteNSM aims to make the process of setting up and managing this infrastructure as seamless as possible. Simple Keep interfaces simple and intuitive. Users should be able to get to a working state with minimal documentation. Lightweight Give users the option to install and configure only the components they need without requiring them to download a huge image. Extensible Provide intuitive programming interfaces and utilities where ever possible, giving users multiple paths of integration and extension. Adaptable Provide variety of ways users can interact with their data, ranging from tracking operational network metrics to threat-hunting. Secure Install components pre-configured with security in mind. Wherever possible encrypt data in transit and at rest. Optimized Install components to best utilize system resources. When needed, make changes to the underlying system to improve tasks like packet acquisition and memory management. Centralized Provide users with the tools needed to remotely manage their deployments. Users should be able to perform remote operations against multiple instances in parallel. Users should be able to copy the configuration state of one node to another.","title":"Project Goals"},{"location":"about/01_project_goals/#project-goals","text":"Passive network monitoring is an approach to network monitoring where traffic is sniffed via strategically placed sensors on critical junctions of a network. DynamiteNSM aims to make the process of setting up and managing this infrastructure as seamless as possible.","title":"Project Goals"},{"location":"about/01_project_goals/#simple","text":"Keep interfaces simple and intuitive. Users should be able to get to a working state with minimal documentation.","title":"Simple"},{"location":"about/01_project_goals/#lightweight","text":"Give users the option to install and configure only the components they need without requiring them to download a huge image.","title":"Lightweight"},{"location":"about/01_project_goals/#extensible","text":"Provide intuitive programming interfaces and utilities where ever possible, giving users multiple paths of integration and extension.","title":"Extensible"},{"location":"about/01_project_goals/#adaptable","text":"Provide variety of ways users can interact with their data, ranging from tracking operational network metrics to threat-hunting.","title":"Adaptable"},{"location":"about/01_project_goals/#secure","text":"Install components pre-configured with security in mind. Wherever possible encrypt data in transit and at rest.","title":"Secure"},{"location":"about/01_project_goals/#optimized","text":"Install components to best utilize system resources. When needed, make changes to the underlying system to improve tasks like packet acquisition and memory management.","title":"Optimized"},{"location":"about/01_project_goals/#centralized","text":"Provide users with the tools needed to remotely manage their deployments. Users should be able to perform remote operations against multiple instances in parallel. Users should be able to copy the configuration state of one node to another.","title":"Centralized"},{"location":"about/02_architecture/","text":"General Architecture Within DynamiteNSM a service is a collection of wrappers around a utility that provide the ability to perform actions like: installation configuration process management performance monitoring troubleshooting Services can be grouped together into components. There are two primary components: the Agent and the Monitor . Agents run on dedicated hardware that inspects mirrored traffic and forwards logs on to a downstream collector . The monitor is Dynamite's solution for indexing and presenting network events and insights forwarded from agents in a way useful to security analysts and threat hunters. The Dynamite team also developed a very simple remote management utility called dynamite-remote that allows administrators remotely manage remote enabled instances. Agent Services The agent (sensor) is responsible for generating JSON events from raw network packets and forwarding these events to a monitor. Service Project Link Version Description License Zeek Github 3.0.3 Zeek (formerly Bro) is a free and open-source software network analysis framework. It provides an extremely powerful scripting language that can be used for everything from protocol parsing to file carving. BSD Suricata Github 4.1.4 Suricata is an Intrusion Detection System (IDS), powered by the latest open EmergingThreat rule-sets. GPL 2.0 Filebeat Github 7.11.1 Filebeat-OSS is a free and open-source log shipper written in GoLang. The utility is capable of forwarding logs to a variety of destination types. Apache 2.0 Monitor Services The monitor is responsible for collecting these events, enriching and normalizing them, and presenting them to the end-user through intuitive visualizations and a powerful search user interface Services Project Link Version Description License Logstash Github 7.11.1 A server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it. Apache 2.0 Elasticsearch Github 1.13.0 A distributed, RESTful search and analytics engine. Apache 2.0 Kibana Github 1.13.0 A web-app that allows you to visualize your Elasticsearch data Apache 2.0","title":"Architechture"},{"location":"about/02_architecture/#general-architecture","text":"Within DynamiteNSM a service is a collection of wrappers around a utility that provide the ability to perform actions like: installation configuration process management performance monitoring troubleshooting Services can be grouped together into components. There are two primary components: the Agent and the Monitor . Agents run on dedicated hardware that inspects mirrored traffic and forwards logs on to a downstream collector . The monitor is Dynamite's solution for indexing and presenting network events and insights forwarded from agents in a way useful to security analysts and threat hunters. The Dynamite team also developed a very simple remote management utility called dynamite-remote that allows administrators remotely manage remote enabled instances.","title":"General Architecture"},{"location":"about/02_architecture/#agent-services","text":"The agent (sensor) is responsible for generating JSON events from raw network packets and forwarding these events to a monitor. Service Project Link Version Description License Zeek Github 3.0.3 Zeek (formerly Bro) is a free and open-source software network analysis framework. It provides an extremely powerful scripting language that can be used for everything from protocol parsing to file carving. BSD Suricata Github 4.1.4 Suricata is an Intrusion Detection System (IDS), powered by the latest open EmergingThreat rule-sets. GPL 2.0 Filebeat Github 7.11.1 Filebeat-OSS is a free and open-source log shipper written in GoLang. The utility is capable of forwarding logs to a variety of destination types. Apache 2.0","title":"Agent Services"},{"location":"about/02_architecture/#monitor-services","text":"The monitor is responsible for collecting these events, enriching and normalizing them, and presenting them to the end-user through intuitive visualizations and a powerful search user interface Services Project Link Version Description License Logstash Github 7.11.1 A server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it. Apache 2.0 Elasticsearch Github 1.13.0 A distributed, RESTful search and analytics engine. Apache 2.0 Kibana Github 1.13.0 A web-app that allows you to visualize your Elasticsearch data Apache 2.0","title":"Monitor Services"},{"location":"about/data_model/01_overview/","text":"Data Model Overview Agents provide two primary kinds of normalized output: events and alerts. In the context of this guide an event is a completely neutral transaction that by itself is neither malicious nor benign. An alert on the other hand is not neutral. Instead, alerts are typically generated by making inferences against acquired traffic, and often highlight activity that seems suspicious or even malicious. Both events and alerts share several key fields, which make navigation between the two straightforward when it comes time to visualize or search the data. The Elastic Common Schema Out-of-the-box DynamiteNSM will normalize all events (and alerts) to the Elastic Common Schema. Elastic Common Schema or ECS for short is an open source specification, developed with the support of the Elastic User Community. Mapping of Network Events Network events capture the details of one device communicating with another. The initiator is referred to as the source, and the recipient as the destination. Depending on the data source, a network event can contain details of addresses, protocols, headers, and device roles. When an event contains details about the sending and receiving hosts, the baseline for capturing these values will be the source and destination fields. Source Fields Example { \"source\": { \"ip\": \"192.168.86.222\", \"port\": 54162 } } Descriptions Field Description Level source.address Some event source addresses are defined ambiguously. The event will sometimes list an IP, a domain or a unix socket. You should always store the raw address in the .address field. Then it should be duplicated to .ip or .domain, depending on which one it is. type: keyword extended source.bytes Bytes sent from the source to the destination. type: long example: 184 core source.domain Source domain. type: keyword core source.ip IP address of the source (IPv4 or IPv6). type: ip core source.mac MAC address of the source. type: keyword core source.nat.ip Translated ip of source based NAT sessions (e.g. internal client to internet) Typically connections traversing load balancers, firewalls, or routers. type: ip extended source.nat.port Translated port of source based NAT sessions. (e.g. internal client to internet) Typically used with load balancers, firewalls, or routers. type: long extended source.packets Packets sent from the source to the destination. type: long example: 12 core source.port Port of the source. type: long core source.registered_domain The highest registered source domain, stripped of the subdomain. For example, the registered domain for \"foo.example.com\" is \"example.com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last two labels will not work well for TLDs such as \"co.uk\". type: keyword example: example.com extended source.subdomain The subdomain portion of a fully qualified domain name includes all of the names except the host name under the registered_domain. In a partially qualified domain, or if the the qualification level of the full name cannot be determined, subdomain contains all of the names below the registered domain. For example the subdomain portion of \"www.east.mydomain.co.uk\" is \"east\". If the domain has multiple levels of subdomain, such as \"sub2.sub1.example.com\", the subdomain field should contain \"sub2.sub1\", with no trailing period. type: keyword example: east extended source.top_level_domain The effective top level domain (eTLD), also known as the domain suffix, is the last part of the domain name. For example, the top level domain for example.com is \"com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last label will not work well for effective TLDs such as \"co.uk\". type: keyword example: co.uk extended Destination Fields Example { \"destination\": { \"ip\": \"192.168.86.1\", \"port\": 53 } } Descriptions Field Description Level destination.address Some event destination addresses are defined ambiguously. The event will sometimes list an IP, a domain or a unix socket. You should always store the raw address in the .address field. Then it should be duplicated to .ip or .domain, depending on which one it is. type: keyword extended destination.bytes Bytes sent from the destination to the source. type: long example: 184 core destination.domain Destination domain. type: keyword core destination.ip IP address of the destination (IPv4 or IPv6). type: ip core destination.mac MAC address of the destination. type: keyword core destination.nat.ip Translated ip of destination based NAT sessions (e.g. internet to private DMZ) Typically used with load balancers, firewalls, or routers. type: ip extended destination.nat.port Port the source session is translated to by NAT Device. Typically used with load balancers, firewalls, or routers. type: long extended destination.packets Packets sent from the destination to the source. type: long example: 12 core destination.port Port of the destination. type: long core destination.registered_domain The highest registered destination domain, stripped of the subdomain. For example, the registered domain for \"foo.example.com\" is \"example.com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last two labels will not work well for TLDs such as \"co.uk\". type: keyword example: example.com extended destination.subdomain The subdomain portion of a fully qualified domain name includes all of the names except the host name under the registered_domain. In a partially qualified domain, or if the the qualification level of the full name cannot be determined, subdomain contains all of the names below the registered domain. For example the subdomain portion of \"www.east.mydomain.co.uk\" is \"east\". If the domain has multiple levels of subdomain, such as \"sub2.sub1.example.com\", the subdomain field should contain \"sub2.sub1\", with no trailing period. type: keyword example: east extended destination.top_level_domain The effective top level domain (eTLD), also known as the domain suffix, is the last part of the domain name. For example, the top level domain for example.com is \"com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last label will not work well for effective TLDs such as \"co.uk\". type: keyword example: co.uk extended Client/Server Fields In the examples above an analyst may notice that this event is likely a DNS request. The values mapped under source and destination are copied and mapped under client and server. Example { \"client\": { \"ip\": \"192.168.86.222\", \"port\": 64734 }, \"server\": { \"ip\": \"192.168.86.1\", \"port\": 53 } } It\u2019s important to note that while the values for the source and destination fields may reverse between events in a single network transaction, the values for client and server typically will not. Event Categorization Fields When considering the event categorization fields, the category and type fields are populated using their respective allowed values which best classify the source network event. Suricata Alert, Event Section { \"event\": { \"severity\": 3, \"created\": \"2021-01-15T03:34:20.671Z\", \"kind\": \"alert\", \"module\": \"suricata\", \"start\": \"2021-01-02T02:49:08.152Z\", \"category\": [ \"network\", \"intrusion_detection\" ], \"type\": [ \"allowed\" ], \"dataset\": \"suricata.eve\" } } Zeek HTTP Event Section { \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:34.280666497Z\", \"module\": \"zeek\", \"action\": \"post\", \"id\": \"CMcIc31sqwZSUKQP5j\", \"category\": [ \"network\", \"web\" ], \"type\": [ \"connection\", \"info\", \"protocol\" ], \"dataset\": \"zeek.http\", \"outcome\": \"success\" } } Most event.category/event.type ECS pairings are complete on their own. However, the pairing of event.category:network and event.type:protocol is an exception. When these two fields/value pairs both used to categorize an event, the network.protocol field should also be populated Example { \"network\": { \"protocol\": \"http\", \"type\": \"ipv4\", \"transport\": \"tcp\" } }","title":"Data Model Overview"},{"location":"about/data_model/01_overview/#data-model-overview","text":"Agents provide two primary kinds of normalized output: events and alerts. In the context of this guide an event is a completely neutral transaction that by itself is neither malicious nor benign. An alert on the other hand is not neutral. Instead, alerts are typically generated by making inferences against acquired traffic, and often highlight activity that seems suspicious or even malicious. Both events and alerts share several key fields, which make navigation between the two straightforward when it comes time to visualize or search the data.","title":"Data Model Overview"},{"location":"about/data_model/01_overview/#the-elastic-common-schema","text":"Out-of-the-box DynamiteNSM will normalize all events (and alerts) to the Elastic Common Schema. Elastic Common Schema or ECS for short is an open source specification, developed with the support of the Elastic User Community.","title":"The Elastic Common Schema"},{"location":"about/data_model/01_overview/#mapping-of-network-events","text":"Network events capture the details of one device communicating with another. The initiator is referred to as the source, and the recipient as the destination. Depending on the data source, a network event can contain details of addresses, protocols, headers, and device roles. When an event contains details about the sending and receiving hosts, the baseline for capturing these values will be the source and destination fields.","title":"Mapping of Network Events"},{"location":"about/data_model/01_overview/#source-fields","text":"","title":"Source Fields"},{"location":"about/data_model/01_overview/#example","text":"{ \"source\": { \"ip\": \"192.168.86.222\", \"port\": 54162 } }","title":"Example"},{"location":"about/data_model/01_overview/#descriptions","text":"Field Description Level source.address Some event source addresses are defined ambiguously. The event will sometimes list an IP, a domain or a unix socket. You should always store the raw address in the .address field. Then it should be duplicated to .ip or .domain, depending on which one it is. type: keyword extended source.bytes Bytes sent from the source to the destination. type: long example: 184 core source.domain Source domain. type: keyword core source.ip IP address of the source (IPv4 or IPv6). type: ip core source.mac MAC address of the source. type: keyword core source.nat.ip Translated ip of source based NAT sessions (e.g. internal client to internet) Typically connections traversing load balancers, firewalls, or routers. type: ip extended source.nat.port Translated port of source based NAT sessions. (e.g. internal client to internet) Typically used with load balancers, firewalls, or routers. type: long extended source.packets Packets sent from the source to the destination. type: long example: 12 core source.port Port of the source. type: long core source.registered_domain The highest registered source domain, stripped of the subdomain. For example, the registered domain for \"foo.example.com\" is \"example.com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last two labels will not work well for TLDs such as \"co.uk\". type: keyword example: example.com extended source.subdomain The subdomain portion of a fully qualified domain name includes all of the names except the host name under the registered_domain. In a partially qualified domain, or if the the qualification level of the full name cannot be determined, subdomain contains all of the names below the registered domain. For example the subdomain portion of \"www.east.mydomain.co.uk\" is \"east\". If the domain has multiple levels of subdomain, such as \"sub2.sub1.example.com\", the subdomain field should contain \"sub2.sub1\", with no trailing period. type: keyword example: east extended source.top_level_domain The effective top level domain (eTLD), also known as the domain suffix, is the last part of the domain name. For example, the top level domain for example.com is \"com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last label will not work well for effective TLDs such as \"co.uk\". type: keyword example: co.uk extended","title":"Descriptions"},{"location":"about/data_model/01_overview/#destination-fields","text":"","title":"Destination Fields"},{"location":"about/data_model/01_overview/#example_1","text":"{ \"destination\": { \"ip\": \"192.168.86.1\", \"port\": 53 } }","title":"Example"},{"location":"about/data_model/01_overview/#descriptions_1","text":"Field Description Level destination.address Some event destination addresses are defined ambiguously. The event will sometimes list an IP, a domain or a unix socket. You should always store the raw address in the .address field. Then it should be duplicated to .ip or .domain, depending on which one it is. type: keyword extended destination.bytes Bytes sent from the destination to the source. type: long example: 184 core destination.domain Destination domain. type: keyword core destination.ip IP address of the destination (IPv4 or IPv6). type: ip core destination.mac MAC address of the destination. type: keyword core destination.nat.ip Translated ip of destination based NAT sessions (e.g. internet to private DMZ) Typically used with load balancers, firewalls, or routers. type: ip extended destination.nat.port Port the source session is translated to by NAT Device. Typically used with load balancers, firewalls, or routers. type: long extended destination.packets Packets sent from the destination to the source. type: long example: 12 core destination.port Port of the destination. type: long core destination.registered_domain The highest registered destination domain, stripped of the subdomain. For example, the registered domain for \"foo.example.com\" is \"example.com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last two labels will not work well for TLDs such as \"co.uk\". type: keyword example: example.com extended destination.subdomain The subdomain portion of a fully qualified domain name includes all of the names except the host name under the registered_domain. In a partially qualified domain, or if the the qualification level of the full name cannot be determined, subdomain contains all of the names below the registered domain. For example the subdomain portion of \"www.east.mydomain.co.uk\" is \"east\". If the domain has multiple levels of subdomain, such as \"sub2.sub1.example.com\", the subdomain field should contain \"sub2.sub1\", with no trailing period. type: keyword example: east extended destination.top_level_domain The effective top level domain (eTLD), also known as the domain suffix, is the last part of the domain name. For example, the top level domain for example.com is \"com\". This value can be determined precisely with a list like the public suffix list (http://publicsuffix.org). Trying to approximate this by simply taking the last label will not work well for effective TLDs such as \"co.uk\". type: keyword example: co.uk extended","title":"Descriptions"},{"location":"about/data_model/01_overview/#clientserver-fields","text":"In the examples above an analyst may notice that this event is likely a DNS request. The values mapped under source and destination are copied and mapped under client and server.","title":"Client/Server Fields"},{"location":"about/data_model/01_overview/#example_2","text":"{ \"client\": { \"ip\": \"192.168.86.222\", \"port\": 64734 }, \"server\": { \"ip\": \"192.168.86.1\", \"port\": 53 } } It\u2019s important to note that while the values for the source and destination fields may reverse between events in a single network transaction, the values for client and server typically will not.","title":"Example"},{"location":"about/data_model/01_overview/#event-categorization-fields","text":"When considering the event categorization fields, the category and type fields are populated using their respective allowed values which best classify the source network event.","title":"Event Categorization Fields"},{"location":"about/data_model/01_overview/#suricata-alert-event-section","text":"{ \"event\": { \"severity\": 3, \"created\": \"2021-01-15T03:34:20.671Z\", \"kind\": \"alert\", \"module\": \"suricata\", \"start\": \"2021-01-02T02:49:08.152Z\", \"category\": [ \"network\", \"intrusion_detection\" ], \"type\": [ \"allowed\" ], \"dataset\": \"suricata.eve\" } }","title":"Suricata Alert, Event Section"},{"location":"about/data_model/01_overview/#zeek-http-event-section","text":"{ \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:34.280666497Z\", \"module\": \"zeek\", \"action\": \"post\", \"id\": \"CMcIc31sqwZSUKQP5j\", \"category\": [ \"network\", \"web\" ], \"type\": [ \"connection\", \"info\", \"protocol\" ], \"dataset\": \"zeek.http\", \"outcome\": \"success\" } } Most event.category/event.type ECS pairings are complete on their own. However, the pairing of event.category:network and event.type:protocol is an exception. When these two fields/value pairs both used to categorize an event, the network.protocol field should also be populated","title":"Zeek HTTP Event Section"},{"location":"about/data_model/01_overview/#example_3","text":"{ \"network\": { \"protocol\": \"http\", \"type\": \"ipv4\", \"transport\": \"tcp\" } }","title":"Example"},{"location":"about/data_model/samples/alert.eve.json/","text":"Suricata eve.json alert Alerts generated by enabled Suricata rule-sets. Minor Alert (Spotify P2P Traffic) { \"@timestamp\": \"2021-01-02T06:44:09.398Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"d25ef98c-b717-47de-b61e-71da4df4a2df\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"172.16.23.255\", \"port\": 57621, \"bytes\": 0, \"ip\": \"172.16.23.255\", \"packets\": 0 }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"severity\": 3, \"original\": \"{\\\"timestamp\\\":\\\"2021-01-02T01:44:09.398482-0500\\\",\\\"flow_id\\\":918014348514287,\\\"in_iface\\\":\\\"ens37\\\",\\\"event_type\\\":\\\"alert\\\",\\\"src_ip\\\":\\\"172.16.23.1\\\",\\\"src_port\\\":57621,\\\"dest_ip\\\":\\\"172.16.23.255\\\",\\\"dest_port\\\":57621,\\\"proto\\\":\\\"UDP\\\",\\\"community_id\\\":\\\"1:MAZK8VOhlED0IWtc4eWEUm\\\\/Gb8A=\\\",\\\"alert\\\":{\\\"action\\\":\\\"allowed\\\",\\\"gid\\\":1,\\\"signature_id\\\":2027397,\\\"rev\\\":1,\\\"signature\\\":\\\"ET POLICY Spotify P2P Client\\\",\\\"category\\\":\\\"Not Suspicious Traffic\\\",\\\"severity\\\":3,\\\"metadata\\\":{\\\"updated_at\\\":[\\\"2019_05_30\\\"],\\\"signature_severity\\\":[\\\"Minor\\\"],\\\"performance_impact\\\":[\\\"Low\\\"],\\\"deployment\\\":[\\\"Internal\\\"],\\\"created_at\\\":[\\\"2019_05_30\\\"],\\\"attack_target\\\":[\\\"Client_Endpoint\\\"],\\\"affected_product\\\":[\\\"Windows_Client_Apps\\\"]}},\\\"app_proto\\\":\\\"failed\\\",\\\"flow\\\":{\\\"pkts_toserver\\\":471,\\\"pkts_toclient\\\":0,\\\"bytes_toserver\\\":40506,\\\"bytes_toclient\\\":0,\\\"start\\\":\\\"2021-01-01T21:49:08.152559-0500\\\"}}\", \"created\": \"2021-01-15T03:34:20.671Z\", \"kind\": \"alert\", \"module\": \"suricata\", \"start\": \"2021-01-02T02:49:08.152Z\", \"category\": [ \"network\", \"intrusion_detection\" ], \"type\": [ \"allowed\" ], \"dataset\": \"suricata.eve\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"eve\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/suricata/logs/eve.json\" }, \"offset\": 14848090 }, \"message\": \"Not Suspicious Traffic\", \"network\": { \"community_id\": \"1:MAZK8VOhlED0IWtc4eWEUm/Gb8A=\", \"bytes\": 40506, \"transport\": \"udp\", \"packets\": 471 }, \"related\": { \"ip\": [ \"172.16.23.1\", \"172.16.23.255\" ] }, \"rule\": { \"name\": \"ET POLICY Spotify P2P Client\", \"id\": \"2027397\", \"category\": \"Not Suspicious Traffic\" }, \"service\": { \"type\": \"suricata\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 57621, \"bytes\": 40506, \"ip\": \"172.16.23.1\", \"packets\": 471 }, \"suricata\": { \"eve\": { \"in_iface\": \"ens37\", \"community_id\": \"1:MAZK8VOhlED0IWtc4eWEUm/Gb8A=\", \"event_type\": \"alert\", \"alert\": { \"metadata\": { \"performance_impact\": [ \"Low\" ], \"affected_product\": [ \"Windows_Client_Apps\" ], \"updated_at\": [ \"2019_05_30\" ], \"attack_target\": [ \"Client_Endpoint\" ], \"created_at\": [ \"2019_05_30\" ], \"signature_severity\": [ \"Minor\" ], \"deployment\": [ \"Internal\" ] }, \"signature_id\": 2027397, \"rev\": 1, \"gid\": 1, \"signature\": \"ET POLICY Spotify P2P Client\", \"category\": \"Not Suspicious Traffic\" }, \"flow_id\": 918014348514287, \"flow\": {} } }, \"tags\": [ \"suricata\" ] }","title":"Suricata Alert"},{"location":"about/data_model/samples/alert.eve.json/#suricata-evejson-alert","text":"Alerts generated by enabled Suricata rule-sets.","title":"Suricata eve.json alert"},{"location":"about/data_model/samples/alert.eve.json/#minor-alert-spotify-p2p-traffic","text":"{ \"@timestamp\": \"2021-01-02T06:44:09.398Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"d25ef98c-b717-47de-b61e-71da4df4a2df\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"172.16.23.255\", \"port\": 57621, \"bytes\": 0, \"ip\": \"172.16.23.255\", \"packets\": 0 }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"severity\": 3, \"original\": \"{\\\"timestamp\\\":\\\"2021-01-02T01:44:09.398482-0500\\\",\\\"flow_id\\\":918014348514287,\\\"in_iface\\\":\\\"ens37\\\",\\\"event_type\\\":\\\"alert\\\",\\\"src_ip\\\":\\\"172.16.23.1\\\",\\\"src_port\\\":57621,\\\"dest_ip\\\":\\\"172.16.23.255\\\",\\\"dest_port\\\":57621,\\\"proto\\\":\\\"UDP\\\",\\\"community_id\\\":\\\"1:MAZK8VOhlED0IWtc4eWEUm\\\\/Gb8A=\\\",\\\"alert\\\":{\\\"action\\\":\\\"allowed\\\",\\\"gid\\\":1,\\\"signature_id\\\":2027397,\\\"rev\\\":1,\\\"signature\\\":\\\"ET POLICY Spotify P2P Client\\\",\\\"category\\\":\\\"Not Suspicious Traffic\\\",\\\"severity\\\":3,\\\"metadata\\\":{\\\"updated_at\\\":[\\\"2019_05_30\\\"],\\\"signature_severity\\\":[\\\"Minor\\\"],\\\"performance_impact\\\":[\\\"Low\\\"],\\\"deployment\\\":[\\\"Internal\\\"],\\\"created_at\\\":[\\\"2019_05_30\\\"],\\\"attack_target\\\":[\\\"Client_Endpoint\\\"],\\\"affected_product\\\":[\\\"Windows_Client_Apps\\\"]}},\\\"app_proto\\\":\\\"failed\\\",\\\"flow\\\":{\\\"pkts_toserver\\\":471,\\\"pkts_toclient\\\":0,\\\"bytes_toserver\\\":40506,\\\"bytes_toclient\\\":0,\\\"start\\\":\\\"2021-01-01T21:49:08.152559-0500\\\"}}\", \"created\": \"2021-01-15T03:34:20.671Z\", \"kind\": \"alert\", \"module\": \"suricata\", \"start\": \"2021-01-02T02:49:08.152Z\", \"category\": [ \"network\", \"intrusion_detection\" ], \"type\": [ \"allowed\" ], \"dataset\": \"suricata.eve\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"eve\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/suricata/logs/eve.json\" }, \"offset\": 14848090 }, \"message\": \"Not Suspicious Traffic\", \"network\": { \"community_id\": \"1:MAZK8VOhlED0IWtc4eWEUm/Gb8A=\", \"bytes\": 40506, \"transport\": \"udp\", \"packets\": 471 }, \"related\": { \"ip\": [ \"172.16.23.1\", \"172.16.23.255\" ] }, \"rule\": { \"name\": \"ET POLICY Spotify P2P Client\", \"id\": \"2027397\", \"category\": \"Not Suspicious Traffic\" }, \"service\": { \"type\": \"suricata\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 57621, \"bytes\": 40506, \"ip\": \"172.16.23.1\", \"packets\": 471 }, \"suricata\": { \"eve\": { \"in_iface\": \"ens37\", \"community_id\": \"1:MAZK8VOhlED0IWtc4eWEUm/Gb8A=\", \"event_type\": \"alert\", \"alert\": { \"metadata\": { \"performance_impact\": [ \"Low\" ], \"affected_product\": [ \"Windows_Client_Apps\" ], \"updated_at\": [ \"2019_05_30\" ], \"attack_target\": [ \"Client_Endpoint\" ], \"created_at\": [ \"2019_05_30\" ], \"signature_severity\": [ \"Minor\" ], \"deployment\": [ \"Internal\" ] }, \"signature_id\": 2027397, \"rev\": 1, \"gid\": 1, \"signature\": \"ET POLICY Spotify P2P Client\", \"category\": \"Not Suspicious Traffic\" }, \"flow_id\": 918014348514287, \"flow\": {} } }, \"tags\": [ \"suricata\" ] }","title":"Minor Alert (Spotify P2P Traffic)"},{"location":"about/data_model/samples/conn.log/","text":"Zeek conn.log General information regarding TCP, UDP, and ICMP traffic. Likely DNS Multicast Traffic { \"@timestamp\": \"2021-01-15T03:09:58.604Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"401fd4f5-0c05-4bbe-967c-89e7ba50a218\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"224.0.0.251\", \"port\": 5353, \"bytes\": 0, \"ip\": \"224.0.0.251\", \"packets\": 0, \"mac\": \"01:00:5e:00:00:fb\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-15T03:10:11.676987153Z\", \"module\": \"zeek\", \"id\": \"Cheuyi0axMSZadhHg\", \"category\": [ \"network\", \"network\" ], \"type\": [ \"connection\", \"start\" ], \"dataset\": \"zeek.connection\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"connection\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/conn.log\" }, \"offset\": 474 }, \"network\": { \"protocol\": \"dns\", \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"bytes\": 73, \"transport\": \"udp\", \"packets\": 1, \"direction\": \"outbound\" }, \"related\": { \"ip\": [ \"172.16.23.1\", \"224.0.0.251\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 5353, \"bytes\": 73, \"ip\": \"172.16.23.1\", \"packets\": 1, \"mac\": \"00:50:56:c0:00:01\" }, \"tags\": [ \"zeek.connection\", \"local_orig\" ], \"zeek\": { \"session_id\": \"Cheuyi0axMSZadhHg\", \"connection\": { \"local_resp\": false, \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"orientation\": \"multicast\", \"local_orig\": true, \"missed_bytes\": 0, \"history\": \"D\", \"state\": \"S0\", \"state_message\": \"Connection attempt seen, no reply.\", \"pcr\": 1 } } }","title":"Zeek Connection"},{"location":"about/data_model/samples/conn.log/#zeek-connlog","text":"General information regarding TCP, UDP, and ICMP traffic.","title":"Zeek conn.log"},{"location":"about/data_model/samples/conn.log/#likely-dns-multicast-traffic","text":"{ \"@timestamp\": \"2021-01-15T03:09:58.604Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"401fd4f5-0c05-4bbe-967c-89e7ba50a218\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"224.0.0.251\", \"port\": 5353, \"bytes\": 0, \"ip\": \"224.0.0.251\", \"packets\": 0, \"mac\": \"01:00:5e:00:00:fb\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-15T03:10:11.676987153Z\", \"module\": \"zeek\", \"id\": \"Cheuyi0axMSZadhHg\", \"category\": [ \"network\", \"network\" ], \"type\": [ \"connection\", \"start\" ], \"dataset\": \"zeek.connection\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"connection\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/conn.log\" }, \"offset\": 474 }, \"network\": { \"protocol\": \"dns\", \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"bytes\": 73, \"transport\": \"udp\", \"packets\": 1, \"direction\": \"outbound\" }, \"related\": { \"ip\": [ \"172.16.23.1\", \"224.0.0.251\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 5353, \"bytes\": 73, \"ip\": \"172.16.23.1\", \"packets\": 1, \"mac\": \"00:50:56:c0:00:01\" }, \"tags\": [ \"zeek.connection\", \"local_orig\" ], \"zeek\": { \"session_id\": \"Cheuyi0axMSZadhHg\", \"connection\": { \"local_resp\": false, \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"orientation\": \"multicast\", \"local_orig\": true, \"missed_bytes\": 0, \"history\": \"D\", \"state\": \"S0\", \"state_message\": \"Connection attempt seen, no reply.\", \"pcr\": 1 } } }","title":"Likely DNS Multicast Traffic"},{"location":"about/data_model/samples/dhcp.log/","text":"Zeek dhcp.log DHCP \u201cconversation\u201d defined by messages exchanged within a relatively short period of time using the same transaction ID DHCP REQUEST and ACK { \"@timestamp\": \"2021-01-12T20:08:18.367Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"437ed064-9295-43af-9e84-e5bb38665cd8\", \"version\": \"7.9.2\" }, \"client\": { \"address\": \"172.16.23.128\" }, \"destination\": { \"address\": \"172.16.23.254\", \"port\": 67, \"ip\": \"172.16.23.254\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-12T20:15:26.551936060Z\", \"module\": \"zeek\", \"id\": \"{0=CcO3R42oSYdJMEIeS5}\", \"category\": [ \"network\" ], \"type\": [ \"connection\", \"protocol\", \"info\" ], \"dataset\": \"zeek.dhcp\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"dhcp\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/dhcp.log\" }, \"offset\": 0 }, \"network\": { \"community_id\": \"1:fwVMujs9487i/LsEdet5jezcpFc=\", \"protocol\": \"dhcp\", \"name\": \"localdomain\", \"transport\": \"udp\" }, \"related\": { \"ip\": [ \"172.16.23.128\", \"172.16.23.254\" ] }, \"server\": { \"address\": \"172.16.23.254\" }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.128\", \"port\": 68, \"ip\": \"172.16.23.128\" }, \"tags\": [ \"zeek.dhcp\" ], \"zeek\": { \"session_id\": [ \"CcO3R42oSYdJMEIeS5\" ], \"dhcp\": { \"msg\": { \"types\": [ \"REQUEST\", \"ACK\" ], \"origin\": [ \"172.16.23.128\", \"172.16.23.254\" ] }, \"duration\": 0, \"hostname\": \"sensor-dev\", \"address\": { \"server\": \"172.16.23.254\", \"client\": \"172.16.23.128\", \"assigned\": \"172.16.23.128\", \"mac\": \"00:0c:29:c6:7e:2c\" }, \"lease_time\": 1800, \"domain\": \"localdomain\" } } }","title":"Zeek DHCP"},{"location":"about/data_model/samples/dhcp.log/#zeek-dhcplog","text":"DHCP \u201cconversation\u201d defined by messages exchanged within a relatively short period of time using the same transaction ID","title":"Zeek dhcp.log"},{"location":"about/data_model/samples/dhcp.log/#dhcp-request-and-ack","text":"{ \"@timestamp\": \"2021-01-12T20:08:18.367Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"437ed064-9295-43af-9e84-e5bb38665cd8\", \"version\": \"7.9.2\" }, \"client\": { \"address\": \"172.16.23.128\" }, \"destination\": { \"address\": \"172.16.23.254\", \"port\": 67, \"ip\": \"172.16.23.254\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-12T20:15:26.551936060Z\", \"module\": \"zeek\", \"id\": \"{0=CcO3R42oSYdJMEIeS5}\", \"category\": [ \"network\" ], \"type\": [ \"connection\", \"protocol\", \"info\" ], \"dataset\": \"zeek.dhcp\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"dhcp\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/dhcp.log\" }, \"offset\": 0 }, \"network\": { \"community_id\": \"1:fwVMujs9487i/LsEdet5jezcpFc=\", \"protocol\": \"dhcp\", \"name\": \"localdomain\", \"transport\": \"udp\" }, \"related\": { \"ip\": [ \"172.16.23.128\", \"172.16.23.254\" ] }, \"server\": { \"address\": \"172.16.23.254\" }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.128\", \"port\": 68, \"ip\": \"172.16.23.128\" }, \"tags\": [ \"zeek.dhcp\" ], \"zeek\": { \"session_id\": [ \"CcO3R42oSYdJMEIeS5\" ], \"dhcp\": { \"msg\": { \"types\": [ \"REQUEST\", \"ACK\" ], \"origin\": [ \"172.16.23.128\", \"172.16.23.254\" ] }, \"duration\": 0, \"hostname\": \"sensor-dev\", \"address\": { \"server\": \"172.16.23.254\", \"client\": \"172.16.23.128\", \"assigned\": \"172.16.23.128\", \"mac\": \"00:0c:29:c6:7e:2c\" }, \"lease_time\": 1800, \"domain\": \"localdomain\" } } }","title":"DHCP REQUEST and ACK"},{"location":"about/data_model/samples/dns.log/","text":"Zeek dns.log DNS queries along with their responses. PTR Record Lookup on Local Network { \"@timestamp\": \"2021-01-12T19:59:52.595Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"437ed064-9295-43af-9e84-e5bb38665cd8\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"224.0.0.251\", \"port\": 5353, \"ip\": \"224.0.0.251\" }, \"dns\": { \"question\": { \"registered_domain\": \"_tcp.local\", \"top_level_domain\": \"local\", \"name\": \"_spotify-connect._tcp.local\", \"type\": \"PTR\", \"class\": \"IN\" }, \"id\": 0, \"type\": \"query\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"created\": \"2021-01-12T20:15:19.225Z\", \"kind\": \"event\", \"module\": \"zeek\", \"id\": \"CqfE721wPELl1yUjt7\", \"type\": [ \"connection\", \"info\", \"protocol\" ], \"category\": [ \"network\" ], \"dataset\": \"zeek.dns\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"dns\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/dns.log\" }, \"offset\": 0 }, \"network\": { \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"transport\": \"udp\" }, \"related\": { \"ip\": [ \"172.16.23.1\", \"224.0.0.251\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 5353, \"ip\": \"172.16.23.1\" }, \"tags\": [ \"zeek.dns\" ], \"zeek\": { \"dns\": { \"AA\": false, \"qclass_name\": \"C_INTERNET\", \"RD\": false, \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"qtype_name\": \"PTR\", \"qtype\": 12, \"rejected\": false, \"query\": \"_spotify-connect._tcp.local\", \"trans_id\": 0, \"qclass\": 1, \"TC\": false, \"RA\": false }, \"session_id\": \"CqfE721wPELl1yUjt7\" } }","title":"Zeek DNS"},{"location":"about/data_model/samples/dns.log/#zeek-dnslog","text":"DNS queries along with their responses.","title":"Zeek dns.log"},{"location":"about/data_model/samples/dns.log/#ptr-record-lookup-on-local-network","text":"{ \"@timestamp\": \"2021-01-12T19:59:52.595Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"437ed064-9295-43af-9e84-e5bb38665cd8\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"224.0.0.251\", \"port\": 5353, \"ip\": \"224.0.0.251\" }, \"dns\": { \"question\": { \"registered_domain\": \"_tcp.local\", \"top_level_domain\": \"local\", \"name\": \"_spotify-connect._tcp.local\", \"type\": \"PTR\", \"class\": \"IN\" }, \"id\": 0, \"type\": \"query\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"created\": \"2021-01-12T20:15:19.225Z\", \"kind\": \"event\", \"module\": \"zeek\", \"id\": \"CqfE721wPELl1yUjt7\", \"type\": [ \"connection\", \"info\", \"protocol\" ], \"category\": [ \"network\" ], \"dataset\": \"zeek.dns\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"dns\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/dns.log\" }, \"offset\": 0 }, \"network\": { \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"transport\": \"udp\" }, \"related\": { \"ip\": [ \"172.16.23.1\", \"224.0.0.251\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 5353, \"ip\": \"172.16.23.1\" }, \"tags\": [ \"zeek.dns\" ], \"zeek\": { \"dns\": { \"AA\": false, \"qclass_name\": \"C_INTERNET\", \"RD\": false, \"community_id\": \"1:L7sPAjk4l04Uq1b+1PF2pGezp/c=\", \"qtype_name\": \"PTR\", \"qtype\": 12, \"rejected\": false, \"query\": \"_spotify-connect._tcp.local\", \"trans_id\": 0, \"qclass\": 1, \"TC\": false, \"RA\": false }, \"session_id\": \"CqfE721wPELl1yUjt7\" } }","title":"PTR Record Lookup on Local Network"},{"location":"about/data_model/samples/files.log/","text":"Zeek files.log An interface for driving the analysis of files, possibly independent of any network protocol over which they\u2019re transported. X509 Certificate Exchange { \"@timestamp\": \"2021-01-18T19:58:25.728Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"client\": { \"ip\": \"192.168.194.128\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:34.279540379Z\", \"module\": \"zeek\", \"id\": \"C4AHgq1UaIgSiE12C4\", \"category\": [ \"file\" ], \"type\": [ \"info\" ], \"dataset\": \"zeek.files\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"file\": { \"mime_type\": \"application/x-x509-user-cert\", \"hash\": { \"sha1\": \"6d3c6aa45f46eb8bb6fb8f0844020161a025c3c8\", \"md5\": \"329956dbb75e522e0931d34576914a1d\" } }, \"fileset\": { \"name\": \"files\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/files.log\" }, \"offset\": 1077 }, \"related\": { \"ip\": [ \"44.227.11.155\", \"192.168.194.128\" ], \"hash\": [ \"329956dbb75e522e0931d34576914a1d\", \"6d3c6aa45f46eb8bb6fb8f0844020161a025c3c8\" ] }, \"server\": { \"ip\": \"44.227.11.155\" }, \"service\": { \"type\": \"zeek\" }, \"tags\": [ \"zeek.files\" ], \"zeek\": { \"files\": { \"session_ids\": [ \"C4AHgq1UaIgSiE12C4\" ], \"timedout\": false, \"local_orig\": false, \"tx_host\": \"44.227.11.155\", \"source\": \"SSL\", \"is_orig\": false, \"overflow_bytes\": 0, \"duration\": 0, \"sha1\": \"6d3c6aa45f46eb8bb6fb8f0844020161a025c3c8\", \"depth\": 0, \"analyzers\": [ \"SHA1\", \"X509\", \"MD5\" ], \"mime_type\": \"application/x-x509-user-cert\", \"rx_host\": \"192.168.194.128\", \"fuid\": \"F8TQ9LErOrU0jX7i3\", \"seen_bytes\": 1766, \"missing_bytes\": 0, \"md5\": \"329956dbb75e522e0931d34576914a1d\" }, \"session_id\": \"C4AHgq1UaIgSiE12C4\" } }","title":"Zeek Files"},{"location":"about/data_model/samples/files.log/#zeek-fileslog","text":"An interface for driving the analysis of files, possibly independent of any network protocol over which they\u2019re transported.","title":"Zeek files.log"},{"location":"about/data_model/samples/files.log/#x509-certificate-exchange","text":"{ \"@timestamp\": \"2021-01-18T19:58:25.728Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"client\": { \"ip\": \"192.168.194.128\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:34.279540379Z\", \"module\": \"zeek\", \"id\": \"C4AHgq1UaIgSiE12C4\", \"category\": [ \"file\" ], \"type\": [ \"info\" ], \"dataset\": \"zeek.files\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"file\": { \"mime_type\": \"application/x-x509-user-cert\", \"hash\": { \"sha1\": \"6d3c6aa45f46eb8bb6fb8f0844020161a025c3c8\", \"md5\": \"329956dbb75e522e0931d34576914a1d\" } }, \"fileset\": { \"name\": \"files\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/files.log\" }, \"offset\": 1077 }, \"related\": { \"ip\": [ \"44.227.11.155\", \"192.168.194.128\" ], \"hash\": [ \"329956dbb75e522e0931d34576914a1d\", \"6d3c6aa45f46eb8bb6fb8f0844020161a025c3c8\" ] }, \"server\": { \"ip\": \"44.227.11.155\" }, \"service\": { \"type\": \"zeek\" }, \"tags\": [ \"zeek.files\" ], \"zeek\": { \"files\": { \"session_ids\": [ \"C4AHgq1UaIgSiE12C4\" ], \"timedout\": false, \"local_orig\": false, \"tx_host\": \"44.227.11.155\", \"source\": \"SSL\", \"is_orig\": false, \"overflow_bytes\": 0, \"duration\": 0, \"sha1\": \"6d3c6aa45f46eb8bb6fb8f0844020161a025c3c8\", \"depth\": 0, \"analyzers\": [ \"SHA1\", \"X509\", \"MD5\" ], \"mime_type\": \"application/x-x509-user-cert\", \"rx_host\": \"192.168.194.128\", \"fuid\": \"F8TQ9LErOrU0jX7i3\", \"seen_bytes\": 1766, \"missing_bytes\": 0, \"md5\": \"329956dbb75e522e0931d34576914a1d\" }, \"session_id\": \"C4AHgq1UaIgSiE12C4\" } }","title":"X509 Certificate Exchange"},{"location":"about/data_model/samples/http.log/","text":"Zeek http.log HTTP requests and replies POST Request with Response { \"@timestamp\": \"2021-01-18T19:58:25.832Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"destination\": { \"geo\": { \"continent_name\": \"North America\", \"country_iso_code\": \"US\", \"country_name\": \"United States\", \"location\": { \"lon\": -97.822, \"lat\": 37.751 } }, \"as\": { \"number\": 15133, \"organization\": { \"name\": \"MCI Communications Services, Inc. d/b/a Verizon Business\" } }, \"address\": \"72.21.91.29\", \"port\": 80, \"ip\": \"72.21.91.29\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:34.280666497Z\", \"module\": \"zeek\", \"action\": \"post\", \"id\": \"CMcIc31sqwZSUKQP5j\", \"category\": [ \"network\", \"web\" ], \"type\": [ \"connection\", \"info\", \"protocol\" ], \"dataset\": \"zeek.http\", \"outcome\": \"success\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"http\" }, \"host\": { \"name\": \"sensor-dev\" }, \"http\": { \"request\": { \"method\": \"POST\", \"body\": { \"bytes\": 83 } }, \"response\": { \"status_code\": 200, \"body\": { \"bytes\": 471 } }, \"version\": \"1.1\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/http.log\" }, \"offset\": 2984 }, \"network\": { \"community_id\": \"1:V7YJnKQL1/XSRE6bx4UxmzX5NnA=\", \"transport\": \"tcp\" }, \"related\": { \"ip\": [ \"192.168.194.128\", \"72.21.91.29\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"192.168.194.128\", \"port\": 34942, \"ip\": \"192.168.194.128\" }, \"tags\": [ \"zeek.http\" ], \"url\": { \"original\": \"/\", \"port\": 80, \"domain\": \"ocsp.digicert.com\" }, \"user_agent\": { \"original\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0\", \"os\": { \"name\": \"Ubuntu\" }, \"name\": \"Firefox\", \"device\": { \"name\": \"Other\" }, \"version\": \"84.0.\" }, \"zeek\": { \"http\": { \"uri_vars\": [ \"/\" ], \"resp_mime_types\": [ \"application/ocsp-response\" ], \"client_header_names\": [ \"HOST\", \"USER-AGENT\", \"ACCEPT\", \"ACCEPT-LANGUAGE\", \"ACCEPT-ENCODING\", \"CONTENT-TYPE\", \"CONTENT-LENGTH\", \"CONNECTION\" ], \"community_id\": \"1:V7YJnKQL1/XSRE6bx4UxmzX5NnA=\", \"trans_depth\": 1, \"orig_fuids\": [ \"FHbW6v2ACWtzPXSmn2\" ], \"status_msg\": \"OK\", \"orig_mime_types\": [ \"application/ocsp-request\" ], \"tags\": [], \"resp_fuids\": [ \"F8IRy32mB5ft7uqVx\" ] }, \"session_id\": \"CMcIc31sqwZSUKQP5j\" } }","title":"Zeek HTTP"},{"location":"about/data_model/samples/http.log/#zeek-httplog","text":"HTTP requests and replies","title":"Zeek http.log"},{"location":"about/data_model/samples/http.log/#post-request-with-response","text":"{ \"@timestamp\": \"2021-01-18T19:58:25.832Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"destination\": { \"geo\": { \"continent_name\": \"North America\", \"country_iso_code\": \"US\", \"country_name\": \"United States\", \"location\": { \"lon\": -97.822, \"lat\": 37.751 } }, \"as\": { \"number\": 15133, \"organization\": { \"name\": \"MCI Communications Services, Inc. d/b/a Verizon Business\" } }, \"address\": \"72.21.91.29\", \"port\": 80, \"ip\": \"72.21.91.29\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:34.280666497Z\", \"module\": \"zeek\", \"action\": \"post\", \"id\": \"CMcIc31sqwZSUKQP5j\", \"category\": [ \"network\", \"web\" ], \"type\": [ \"connection\", \"info\", \"protocol\" ], \"dataset\": \"zeek.http\", \"outcome\": \"success\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"http\" }, \"host\": { \"name\": \"sensor-dev\" }, \"http\": { \"request\": { \"method\": \"POST\", \"body\": { \"bytes\": 83 } }, \"response\": { \"status_code\": 200, \"body\": { \"bytes\": 471 } }, \"version\": \"1.1\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/http.log\" }, \"offset\": 2984 }, \"network\": { \"community_id\": \"1:V7YJnKQL1/XSRE6bx4UxmzX5NnA=\", \"transport\": \"tcp\" }, \"related\": { \"ip\": [ \"192.168.194.128\", \"72.21.91.29\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"192.168.194.128\", \"port\": 34942, \"ip\": \"192.168.194.128\" }, \"tags\": [ \"zeek.http\" ], \"url\": { \"original\": \"/\", \"port\": 80, \"domain\": \"ocsp.digicert.com\" }, \"user_agent\": { \"original\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0\", \"os\": { \"name\": \"Ubuntu\" }, \"name\": \"Firefox\", \"device\": { \"name\": \"Other\" }, \"version\": \"84.0.\" }, \"zeek\": { \"http\": { \"uri_vars\": [ \"/\" ], \"resp_mime_types\": [ \"application/ocsp-response\" ], \"client_header_names\": [ \"HOST\", \"USER-AGENT\", \"ACCEPT\", \"ACCEPT-LANGUAGE\", \"ACCEPT-ENCODING\", \"CONTENT-TYPE\", \"CONTENT-LENGTH\", \"CONNECTION\" ], \"community_id\": \"1:V7YJnKQL1/XSRE6bx4UxmzX5NnA=\", \"trans_depth\": 1, \"orig_fuids\": [ \"FHbW6v2ACWtzPXSmn2\" ], \"status_msg\": \"OK\", \"orig_mime_types\": [ \"application/ocsp-request\" ], \"tags\": [], \"resp_fuids\": [ \"F8IRy32mB5ft7uqVx\" ] }, \"session_id\": \"CMcIc31sqwZSUKQP5j\" } }","title":"POST Request with Response"},{"location":"about/data_model/samples/pe.log/","text":"Zeek pe.log Information about Portable Executable (PE) extracted from various application layer protocols. Windows Executable (Extracted from HTTP transaction) { \"@timestamp\": \"2021-01-18T20:33:50.188Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"version\": \"7.9.2\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T20:33:55.433112556Z\", \"module\": \"zeek\", \"type\": [ \"info\" ], \"category\": [ \"file\" ], \"dataset\": \"zeek.pe\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"pe\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/pe.log\" }, \"offset\": 0 }, \"service\": { \"type\": \"zeek\" }, \"tags\": [ \"zeek.pe\" ], \"zeek\": { \"pe\": { \"compile_time\": \"2006-04-29T09:56:31.000Z\", \"uses_aslr\": false, \"os\": \"Windows 95 or NT 4.0\", \"subsystem\": \"WINDOWS_GUI\", \"section_names\": [ \".text\", \".rdata\", \".data\", \".rsrc\" ], \"has_export_table\": false, \"uses_dep\": false, \"is_64bit\": false, \"has_cert_table\": true, \"has_debug_data\": false, \"has_import_table\": true, \"uses_seh\": true, \"is_exe\": true, \"machine\": \"I386\", \"id\": \"FnhRQ63qMsSOfIGoWl\", \"uses_code_integrity\": false } } }","title":"Zeek Portable Executables"},{"location":"about/data_model/samples/pe.log/#zeek-pelog","text":"Information about Portable Executable (PE) extracted from various application layer protocols.","title":"Zeek pe.log"},{"location":"about/data_model/samples/pe.log/#windows-executable-extracted-from-http-transaction","text":"{ \"@timestamp\": \"2021-01-18T20:33:50.188Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"version\": \"7.9.2\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T20:33:55.433112556Z\", \"module\": \"zeek\", \"type\": [ \"info\" ], \"category\": [ \"file\" ], \"dataset\": \"zeek.pe\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"pe\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/pe.log\" }, \"offset\": 0 }, \"service\": { \"type\": \"zeek\" }, \"tags\": [ \"zeek.pe\" ], \"zeek\": { \"pe\": { \"compile_time\": \"2006-04-29T09:56:31.000Z\", \"uses_aslr\": false, \"os\": \"Windows 95 or NT 4.0\", \"subsystem\": \"WINDOWS_GUI\", \"section_names\": [ \".text\", \".rdata\", \".data\", \".rsrc\" ], \"has_export_table\": false, \"uses_dep\": false, \"is_64bit\": false, \"has_cert_table\": true, \"has_debug_data\": false, \"has_import_table\": true, \"uses_seh\": true, \"is_exe\": true, \"machine\": \"I386\", \"id\": \"FnhRQ63qMsSOfIGoWl\", \"uses_code_integrity\": false } } }","title":"Windows Executable (Extracted from HTTP transaction)"},{"location":"about/data_model/samples/ssh.log/","text":"Zeek ssh.log SSH connections with authentication attempts Successful SSH Authentication { \"@timestamp\": \"2021-01-18T20:25:14.252Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"192.168.194.128\", \"port\": 22, \"ip\": \"192.168.194.128\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T20:25:23.314784943Z\", \"module\": \"zeek\", \"id\": \"CTmQup3tXKmgr92ECk\", \"category\": [ \"network\" ], \"type\": [ \"connection\", \"protocol\" ], \"dataset\": \"zeek.ssh\", \"outcome\": \"success\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"ssh\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/ssh.log\" }, \"offset\": 0 }, \"network\": { \"protocol\": \"ssh\", \"community_id\": \"1:9u7Q4Aw1yFu7z67axSzldRRGJJ4=\", \"transport\": \"tcp\" }, \"related\": { \"ip\": [ \"192.168.194.1\", \"192.168.194.128\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"192.168.194.1\", \"port\": 49760, \"ip\": \"192.168.194.1\" }, \"tags\": [ \"zeek.ssh\" ], \"zeek\": { \"ssh\": { \"server\": \"SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.1\", \"host_key\": \"24:c3:65:22:da:ed:29:48:80:ae:df:de:74:25:cb:b6\", \"community_id\": \"1:9u7Q4Aw1yFu7z67axSzldRRGJJ4=\", \"auth\": { \"success\": true, \"attempts\": 1 }, \"client\": \"SSH-2.0-OpenSSH_8.1\", \"version\": 2, \"algorithm\": { \"cipher\": \"chacha20-poly1305@openssh.com\", \"host_key\": \"ecdsa-sha2-nistp256\", \"compression\": \"none\", \"key_exchange\": \"curve25519-sha256\", \"mac\": \"umac-64-etm@openssh.com\" } }, \"session_id\": \"CTmQup3tXKmgr92ECk\" } }","title":"Zeek SSH"},{"location":"about/data_model/samples/ssh.log/#zeek-sshlog","text":"SSH connections with authentication attempts","title":"Zeek ssh.log"},{"location":"about/data_model/samples/ssh.log/#successful-ssh-authentication","text":"{ \"@timestamp\": \"2021-01-18T20:25:14.252Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"192.168.194.128\", \"port\": 22, \"ip\": \"192.168.194.128\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T20:25:23.314784943Z\", \"module\": \"zeek\", \"id\": \"CTmQup3tXKmgr92ECk\", \"category\": [ \"network\" ], \"type\": [ \"connection\", \"protocol\" ], \"dataset\": \"zeek.ssh\", \"outcome\": \"success\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"ssh\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/ssh.log\" }, \"offset\": 0 }, \"network\": { \"protocol\": \"ssh\", \"community_id\": \"1:9u7Q4Aw1yFu7z67axSzldRRGJJ4=\", \"transport\": \"tcp\" }, \"related\": { \"ip\": [ \"192.168.194.1\", \"192.168.194.128\" ] }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"192.168.194.1\", \"port\": 49760, \"ip\": \"192.168.194.1\" }, \"tags\": [ \"zeek.ssh\" ], \"zeek\": { \"ssh\": { \"server\": \"SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.1\", \"host_key\": \"24:c3:65:22:da:ed:29:48:80:ae:df:de:74:25:cb:b6\", \"community_id\": \"1:9u7Q4Aw1yFu7z67axSzldRRGJJ4=\", \"auth\": { \"success\": true, \"attempts\": 1 }, \"client\": \"SSH-2.0-OpenSSH_8.1\", \"version\": 2, \"algorithm\": { \"cipher\": \"chacha20-poly1305@openssh.com\", \"host_key\": \"ecdsa-sha2-nistp256\", \"compression\": \"none\", \"key_exchange\": \"curve25519-sha256\", \"mac\": \"umac-64-etm@openssh.com\" } }, \"session_id\": \"CTmQup3tXKmgr92ECk\" } }","title":"Successful SSH Authentication"},{"location":"about/data_model/samples/ssl.log/","text":"Zeek ssl.log SSL/TLS handshake info Failed to Establish TLS Session { \"@timestamp\": \"2021-01-18T19:19:19.760Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"client\": { \"address\": \"127.0.0.1\" }, \"destination\": { \"address\": \"127.0.0.1\", \"port\": 47763, \"ip\": \"127.0.0.1\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": [ \"connection\", \"protocol\" ], \"created\": \"2021-01-18T19:35:11.917623174Z\", \"module\": \"zeek\", \"id\": \"CgJFJV0S7TpYJkc1e\", \"category\": [ \"network\" ], \"dataset\": \"zeek.ssl\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"ssl\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/ssl.log\" }, \"offset\": 0 }, \"network\": { \"community_id\": \"1:MIn0vYshYL45/ZjBgofGuA/a4fY=\", \"transport\": \"tcp\" }, \"related\": { \"ip\": [ \"127.0.0.1\", \"127.0.0.1\" ] }, \"server\": { \"address\": \"127.0.0.1\" }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"127.0.0.1\", \"port\": 60872, \"ip\": \"127.0.0.1\" }, \"tags\": [ \"zeek.ssl\" ], \"tls\": { \"cipher\": \"TLS_ECDH_ANON_WITH_AES_256_CBC_SHA\", \"established\": false, \"curve\": \"secp384r1\", \"resumed\": false, \"version\": \"1.2\", \"version_protocol\": \"tls\" }, \"zeek\": { \"session_id\": \"CgJFJV0S7TpYJkc1e\", \"ssl\": { \"cipher\": \"TLS_ECDH_ANON_WITH_AES_256_CBC_SHA\", \"established\": false, \"community_id\": \"1:MIn0vYshYL45/ZjBgofGuA/a4fY=\", \"curve\": \"secp384r1\", \"resumed\": false, \"version\": \"TLSv12\" } } }","title":"Zeek SSL"},{"location":"about/data_model/samples/ssl.log/#zeek-ssllog","text":"SSL/TLS handshake info","title":"Zeek ssl.log"},{"location":"about/data_model/samples/ssl.log/#failed-to-establish-tls-session","text":"{ \"@timestamp\": \"2021-01-18T19:19:19.760Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"client\": { \"address\": \"127.0.0.1\" }, \"destination\": { \"address\": \"127.0.0.1\", \"port\": 47763, \"ip\": \"127.0.0.1\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": [ \"connection\", \"protocol\" ], \"created\": \"2021-01-18T19:35:11.917623174Z\", \"module\": \"zeek\", \"id\": \"CgJFJV0S7TpYJkc1e\", \"category\": [ \"network\" ], \"dataset\": \"zeek.ssl\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"ssl\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/ssl.log\" }, \"offset\": 0 }, \"network\": { \"community_id\": \"1:MIn0vYshYL45/ZjBgofGuA/a4fY=\", \"transport\": \"tcp\" }, \"related\": { \"ip\": [ \"127.0.0.1\", \"127.0.0.1\" ] }, \"server\": { \"address\": \"127.0.0.1\" }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"127.0.0.1\", \"port\": 60872, \"ip\": \"127.0.0.1\" }, \"tags\": [ \"zeek.ssl\" ], \"tls\": { \"cipher\": \"TLS_ECDH_ANON_WITH_AES_256_CBC_SHA\", \"established\": false, \"curve\": \"secp384r1\", \"resumed\": false, \"version\": \"1.2\", \"version_protocol\": \"tls\" }, \"zeek\": { \"session_id\": \"CgJFJV0S7TpYJkc1e\", \"ssl\": { \"cipher\": \"TLS_ECDH_ANON_WITH_AES_256_CBC_SHA\", \"established\": false, \"community_id\": \"1:MIn0vYshYL45/ZjBgofGuA/a4fY=\", \"curve\": \"secp384r1\", \"resumed\": false, \"version\": \"TLSv12\" } } }","title":"Failed to Establish TLS Session"},{"location":"about/data_model/samples/weird.log/","text":"Zeek weird.log Unexpected network-level activity DNS_Conn_count_too_large { \"@timestamp\": \"2021-01-13T17:02:32.864Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"c6462bb8-8609-4620-bd3d-4f8a0cd4f025\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"224.0.0.251\", \"port\": 5353, \"ip\": \"224.0.0.251\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"alert\", \"created\": \"2021-01-13T17:02:35.501488569Z\", \"module\": \"zeek\", \"id\": \"CjSsHj4wXCoOPjhHll\", \"category\": [ \"network\" ], \"type\": [ \"info\" ], \"dataset\": \"zeek.weird\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"weird\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/weird.log\" }, \"offset\": 0 }, \"related\": { \"ip\": [ \"172.16.23.1\", \"224.0.0.251\" ] }, \"rule\": { \"name\": \"DNS_Conn_count_too_large\" }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 5353, \"ip\": \"172.16.23.1\" }, \"tags\": [ \"zeek.weird\" ], \"zeek\": { \"weird\": { \"peer\": \"dynamite-worker-ens37-7\", \"name\": \"DNS_Conn_count_too_large\", \"notice\": false }, \"session_id\": \"CjSsHj4wXCoOPjhHll\" } }","title":"Zeek Weird"},{"location":"about/data_model/samples/weird.log/#zeek-weirdlog","text":"Unexpected network-level activity","title":"Zeek weird.log"},{"location":"about/data_model/samples/weird.log/#dns_conn_count_too_large","text":"{ \"@timestamp\": \"2021-01-13T17:02:32.864Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"ephemeral_id\": \"c6462bb8-8609-4620-bd3d-4f8a0cd4f025\", \"type\": \"filebeat\", \"version\": \"7.9.2\" }, \"destination\": { \"address\": \"224.0.0.251\", \"port\": 5353, \"ip\": \"224.0.0.251\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"alert\", \"created\": \"2021-01-13T17:02:35.501488569Z\", \"module\": \"zeek\", \"id\": \"CjSsHj4wXCoOPjhHll\", \"category\": [ \"network\" ], \"type\": [ \"info\" ], \"dataset\": \"zeek.weird\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"weird\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/weird.log\" }, \"offset\": 0 }, \"related\": { \"ip\": [ \"172.16.23.1\", \"224.0.0.251\" ] }, \"rule\": { \"name\": \"DNS_Conn_count_too_large\" }, \"service\": { \"type\": \"zeek\" }, \"source\": { \"address\": \"172.16.23.1\", \"port\": 5353, \"ip\": \"172.16.23.1\" }, \"tags\": [ \"zeek.weird\" ], \"zeek\": { \"weird\": { \"peer\": \"dynamite-worker-ens37-7\", \"name\": \"DNS_Conn_count_too_large\", \"notice\": false }, \"session_id\": \"CjSsHj4wXCoOPjhHll\" } }","title":"DNS_Conn_count_too_large"},{"location":"about/data_model/samples/x509.log/","text":"Zeek x509.log X.509 certificate info DigiCert Inc Certificate { \"@timestamp\": \"2021-01-18T19:58:25.728Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"version\": \"7.9.2\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:33.189116150Z\", \"module\": \"zeek\", \"id\": \"F8TQ9LErOrU0jX7i3\", \"type\": [ \"info\" ], \"dataset\": \"zeek.x509\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"x509\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/x509.log\" }, \"offset\": 0 }, \"service\": { \"type\": \"zeek\" }, \"tags\": [ \"zeek.x509\" ], \"zeek\": { \"x509\": { \"san\": { \"dns\": [ \"*.telemetry.mozilla.org\", \"telemetry.mozilla.org\" ] }, \"certificate\": { \"valid\": { \"from\": \"2020-08-24T04:00:00.000Z\", \"until\": \"2022-10-28T16:00:00.000Z\" }, \"serial\": \"0CE6B5FD8FB1B07CD4D54CAEFE4DBF57\", \"subject\": { \"country\": \"US\", \"organization\": \"Mozilla Corporation\", \"locality\": \"Mountain View\", \"state\": \"California\", \"common_name\": \"*.telemetry.mozilla.org\", \"organizational_unit\": \"Cloud Services\" }, \"signature_algorithm\": \"sha256WithRSAEncryption\", \"version\": 3, \"key\": { \"length\": 2048, \"type\": \"rsa\", \"algorithm\": \"rsaEncryption\" }, \"issuer\": { \"country\": \"US\", \"organization\": \"DigiCert Inc\", \"common_name\": \"DigiCert SHA2 Secure Server CA\" }, \"exponent\": \"65537\" }, \"basic_constraints\": { \"certificate_authority\": false } }, \"session_id\": \"F8TQ9LErOrU0jX7i3\" } }","title":"Zeek X509"},{"location":"about/data_model/samples/x509.log/#zeek-x509log","text":"X.509 certificate info","title":"Zeek x509.log"},{"location":"about/data_model/samples/x509.log/#digicert-inc-certificate","text":"{ \"@timestamp\": \"2021-01-18T19:58:25.728Z\", \"agent\": { \"hostname\": \"sensor-dev\", \"name\": \"sensor-dev\", \"id\": \"6bf5192e-e2f1-49bb-ab7a-c04c26381e7e\", \"type\": \"filebeat\", \"ephemeral_id\": \"9b5aa2d4-1b54-4c25-bd2d-61cd592d34f4\", \"version\": \"7.9.2\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"event\": { \"kind\": \"event\", \"created\": \"2021-01-18T19:58:33.189116150Z\", \"module\": \"zeek\", \"id\": \"F8TQ9LErOrU0jX7i3\", \"type\": [ \"info\" ], \"dataset\": \"zeek.x509\" }, \"fields\": { \"originating_agent_tag\": \"sensordev_agt\" }, \"fileset\": { \"name\": \"x509\" }, \"host\": { \"name\": \"sensor-dev\" }, \"input\": { \"type\": \"log\" }, \"log\": { \"file\": { \"path\": \"/opt/dynamite/zeek/logs/current/x509.log\" }, \"offset\": 0 }, \"service\": { \"type\": \"zeek\" }, \"tags\": [ \"zeek.x509\" ], \"zeek\": { \"x509\": { \"san\": { \"dns\": [ \"*.telemetry.mozilla.org\", \"telemetry.mozilla.org\" ] }, \"certificate\": { \"valid\": { \"from\": \"2020-08-24T04:00:00.000Z\", \"until\": \"2022-10-28T16:00:00.000Z\" }, \"serial\": \"0CE6B5FD8FB1B07CD4D54CAEFE4DBF57\", \"subject\": { \"country\": \"US\", \"organization\": \"Mozilla Corporation\", \"locality\": \"Mountain View\", \"state\": \"California\", \"common_name\": \"*.telemetry.mozilla.org\", \"organizational_unit\": \"Cloud Services\" }, \"signature_algorithm\": \"sha256WithRSAEncryption\", \"version\": 3, \"key\": { \"length\": 2048, \"type\": \"rsa\", \"algorithm\": \"rsaEncryption\" }, \"issuer\": { \"country\": \"US\", \"organization\": \"DigiCert Inc\", \"common_name\": \"DigiCert SHA2 Secure Server CA\" }, \"exponent\": \"65537\" }, \"basic_constraints\": { \"certificate_authority\": false } }, \"session_id\": \"F8TQ9LErOrU0jX7i3\" } }","title":"DigiCert Inc Certificate"},{"location":"configuration/01_overview/","text":"Configuration Overview The dynamite commandline utility exposes convenient wrappers around installed services . Commandline Tips The dynamite commandline is modular by design. You add run the -h argument at any time to get usage information about the currently selected module. For example, sudo dynamite -h will give you general information about the top-level services, and the actions that can be performed against each. positional arguments: {agent,monitor,zeek,suricata,filebeat,elasticsearch,logstash,kibana,updates,remote} A component within the Dynamite stack to manage. {install,uninstall,config,logs,process} An action or set of actions that can be performed against a specified component. optional arguments: -h, --help show this help message and exit You can get usage information about a specific service such as elasticsearch by running sudo dynamite elasticsearch -h usage: dynamite [-h] {install,uninstall,process,config} ... Elasticsearch @ 192.168.194.143 positional arguments: {install,uninstall,process,config} install Install Elasticsearch as a standalone component. uninstall Uninstall Elasticsearch on this machine. process Manage local Elasticsearch node processes. config Modify Elasticsearch configurations. optional arguments: -h, --help show this help message and exit Navigating Service Configs At any time simply typing sudo dynamite will generate a table similar to the one below which installed and running services. \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 Installed \u2502 Running \u2502 Service Role \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 elasticsearch \u2502 X \u2502 X \u2502 Monitor \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 logstash \u2502 X \u2502 X \u2502 Monitor \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana \u2502 X \u2502 X \u2502 Monitor \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 zeek \u2502 \u2713 \u2502 X \u2502 Agent \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 suricata \u2502 \u2713 \u2502 X \u2502 Agent \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 filebeat \u2502 \u2713 \u2502 \u2713 \u2502 Agent \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Some services have multiple config interfaces. To list available ones check the services as below. Elasticsearch for example has two configs accessible config interfaces: java and main . $ sudo dynamite elasticsearch config -h usage: dynamite config [-h] {main,java} ... positional arguments: {main,java} main Configure Elasticsearch on this machine. java Configure Java heap allocation for Elasticsearch on this machine. optional arguments: -h, --help show this help message and exit You can view a configuration simply by appending the name of the sub-menu to the config command. $ sudo dynamite elasticsearch config java \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 initial_memory \u2502 8g \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 maximum_memory \u2502 8g \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Most configurations are key-value pair based, and allow you to specify a list of arguments and their values. sudo dynamite elasticsearch config java --initial-memory 12g --maximum-memory 12g Configuration Modules If you run a command like sudo dynamite filebeat config main you'll be given a table like this: \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_targets \u2502 Configuration Module \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 logstash_targets \u2502 Configuration Module \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kafka_targets \u2502 Configuration Module \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 redis_targets \u2502 Configuration Module \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b The Configuration Module value simply means that you can access the Config Object as a sub-menu. $ sudo dynamite filebeat config main redis_targets -h usage: dynamite config main redis_targets [-h] [--target-strings TARGET_STRINGS [TARGET_STRINGS ...]] [--ssl-certificate-authorities SSL_CERTIFICATE_AUTHORITIES [SSL_CERTIFICATE_AUTHORITIES ...]] [--ssl-certificate SSL_CERTIFICATE] [--ssl-key SSL_KEY] [--ssl-verification-mode SSL_VERIFICATION_MODE] [--index INDEX] [--socks-5-proxy-url SOCKS_5_PROXY_URL] [--workers WORKERS] [--max-batch-size MAX_BATCH_SIZE] [--db DB] [--load-balance] [--password PASSWORD] [--enable] [--disable] optional arguments: -h, --help show this help message and exit target options: --target-strings TARGET_STRINGS [TARGET_STRINGS ...] A list of Redis hosts, and their service port (E.G [\"192.168.0.9 6379\"] --ssl-certificate-authorities SSL_CERTIFICATE_AUTHORITIES [SSL_CERTIFICATE_AUTHORITIES ...] The list of root certificates for server verifications. --ssl-certificate SSL_CERTIFICATE The path to the certificate for SSL client authentication. --ssl-key SSL_KEY The client certificate key used for client authentication. --ssl-verification-mode SSL_VERIFICATION_MODE This option controls whether the client verifies server certificates and host names. --index INDEX The key format string to use. --socks-5-proxy-url SOCKS_5_PROXY_URL The full url to the SOCKS5 proxy used for encapsulating the beat protocol --workers WORKERS The number of workers to use for each host configured to publish events to Redis. --max-batch-size MAX_BATCH_SIZE The maximum number of events to bulk in a single Redis request or pipeline. --db DB The Redis database number where the events are published. The default is 0. --load-balance If included and multiple Redis hosts are configured load-balance between them --password PASSWORD The password to authenticate with. The default is no authentication. --enable Enable selected target. --disable Disable selected target","title":"Configuration Overview"},{"location":"configuration/01_overview/#configuration-overview","text":"The dynamite commandline utility exposes convenient wrappers around installed services .","title":"Configuration Overview"},{"location":"configuration/01_overview/#commandline-tips","text":"The dynamite commandline is modular by design. You add run the -h argument at any time to get usage information about the currently selected module. For example, sudo dynamite -h will give you general information about the top-level services, and the actions that can be performed against each. positional arguments: {agent,monitor,zeek,suricata,filebeat,elasticsearch,logstash,kibana,updates,remote} A component within the Dynamite stack to manage. {install,uninstall,config,logs,process} An action or set of actions that can be performed against a specified component. optional arguments: -h, --help show this help message and exit You can get usage information about a specific service such as elasticsearch by running sudo dynamite elasticsearch -h usage: dynamite [-h] {install,uninstall,process,config} ... Elasticsearch @ 192.168.194.143 positional arguments: {install,uninstall,process,config} install Install Elasticsearch as a standalone component. uninstall Uninstall Elasticsearch on this machine. process Manage local Elasticsearch node processes. config Modify Elasticsearch configurations. optional arguments: -h, --help show this help message and exit","title":"Commandline Tips"},{"location":"configuration/01_overview/#navigating-service-configs","text":"At any time simply typing sudo dynamite will generate a table similar to the one below which installed and running services. \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 Installed \u2502 Running \u2502 Service Role \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 elasticsearch \u2502 X \u2502 X \u2502 Monitor \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 logstash \u2502 X \u2502 X \u2502 Monitor \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana \u2502 X \u2502 X \u2502 Monitor \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 zeek \u2502 \u2713 \u2502 X \u2502 Agent \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 suricata \u2502 \u2713 \u2502 X \u2502 Agent \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 filebeat \u2502 \u2713 \u2502 \u2713 \u2502 Agent \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Some services have multiple config interfaces. To list available ones check the services as below. Elasticsearch for example has two configs accessible config interfaces: java and main . $ sudo dynamite elasticsearch config -h usage: dynamite config [-h] {main,java} ... positional arguments: {main,java} main Configure Elasticsearch on this machine. java Configure Java heap allocation for Elasticsearch on this machine. optional arguments: -h, --help show this help message and exit You can view a configuration simply by appending the name of the sub-menu to the config command. $ sudo dynamite elasticsearch config java \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 initial_memory \u2502 8g \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 maximum_memory \u2502 8g \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Most configurations are key-value pair based, and allow you to specify a list of arguments and their values. sudo dynamite elasticsearch config java --initial-memory 12g --maximum-memory 12g","title":"Navigating Service Configs"},{"location":"configuration/01_overview/#configuration-modules","text":"If you run a command like sudo dynamite filebeat config main you'll be given a table like this: \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_targets \u2502 Configuration Module \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 logstash_targets \u2502 Configuration Module \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kafka_targets \u2502 Configuration Module \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 redis_targets \u2502 Configuration Module \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b The Configuration Module value simply means that you can access the Config Object as a sub-menu. $ sudo dynamite filebeat config main redis_targets -h usage: dynamite config main redis_targets [-h] [--target-strings TARGET_STRINGS [TARGET_STRINGS ...]] [--ssl-certificate-authorities SSL_CERTIFICATE_AUTHORITIES [SSL_CERTIFICATE_AUTHORITIES ...]] [--ssl-certificate SSL_CERTIFICATE] [--ssl-key SSL_KEY] [--ssl-verification-mode SSL_VERIFICATION_MODE] [--index INDEX] [--socks-5-proxy-url SOCKS_5_PROXY_URL] [--workers WORKERS] [--max-batch-size MAX_BATCH_SIZE] [--db DB] [--load-balance] [--password PASSWORD] [--enable] [--disable] optional arguments: -h, --help show this help message and exit target options: --target-strings TARGET_STRINGS [TARGET_STRINGS ...] A list of Redis hosts, and their service port (E.G [\"192.168.0.9 6379\"] --ssl-certificate-authorities SSL_CERTIFICATE_AUTHORITIES [SSL_CERTIFICATE_AUTHORITIES ...] The list of root certificates for server verifications. --ssl-certificate SSL_CERTIFICATE The path to the certificate for SSL client authentication. --ssl-key SSL_KEY The client certificate key used for client authentication. --ssl-verification-mode SSL_VERIFICATION_MODE This option controls whether the client verifies server certificates and host names. --index INDEX The key format string to use. --socks-5-proxy-url SOCKS_5_PROXY_URL The full url to the SOCKS5 proxy used for encapsulating the beat protocol --workers WORKERS The number of workers to use for each host configured to publish events to Redis. --max-batch-size MAX_BATCH_SIZE The maximum number of events to bulk in a single Redis request or pipeline. --db DB The Redis database number where the events are published. The default is 0. --load-balance If included and multiple Redis hosts are configured load-balance between them --password PASSWORD The password to authenticate with. The default is no authentication. --enable Enable selected target. --disable Disable selected target","title":"Configuration Modules"},{"location":"configuration/agent/01_connectors/","text":"Connectors \u26a0\ufe0f Changes made within these interfaces require that Filebeat be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite filebeat process restart Dynamite agents rely on filebeat for sending events and alerts to a downstream collector. The following are currently supported. Logstash Elasticsearch (default) Redis Kafka Logstash dynamite filebeat config logstash_targets -h Logstash is powerful data-processing pipeline by Elastic.co. It provides a versatile set of configuration driven ingestion, filtering, and transformation functions. Logstash can be paired with a multitude of upstream retention solutions such as ElasticSearch and InfluxDB. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:5044, logstash-2-box:5044 Proxy URL The URL of the SOCKS5 proxy to use when connecting to the Logstash servers. The value must be a URL with a scheme of socks5://. socks5://192.168.0.55:1080 Index Name The index root name to write events to. dynamite_events-%{+yyyy.MM.dd} Async Pipelines Configures the number of batches to be sent asynchronously to Logstash while waiting for ACK from Logstash. 2 Max Batch Size The maximum number of events to bulk in a single LogStash request. 2048 ElasticSearch sudo dynamite filebeat config elasticsearch_targets -h Elasticsearch is a search engine by Elastic.co that provides distributed, full-text search and supports retention of schemaless data. Elasticsearch is often deployed behind one or more Logstash instances, however in this configuration ingestion is achieved by directly connecting to Elasticsearch's index API. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:9200, elastic-02-box:9200 Index Name The index name to write events to when you\u2019re using daily indices. dynamite_events-%{+yyyy.MM.dd} Username The basic authentication username for connecting to Elasticsearch. elastic Password The basic authentication password for connecting to ElasticSearch. \u00a1my_$ecure_p@ssw0rd! Kafka sudo dynamite filebeat config kafka_targets -h Apache Kafka is a distributed event streaming platform and brokering solution providing high-performance data pipelines for realtime feeds. If extremely high volumes of network traffic are expected, Kafka is a good option. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:5044, logstash-2-box:5044 Topic The Kafka topic used for produced events. dynamite_events Username The username for connecting to Kafka. If username is configured, the password must be configured as well. Only SASL/PLAIN is supported. dynamite_pub Password The password for connecting to Kafka. \u00a1my_$ecure_p@ssw0rd! Redis sudo dynamite filebeat config redis_targets -h Redis is an open source, in-memory data structure store, used as a database, cache, and message broker. If your goal is short-lived visualizations of realtime network analytics, Redis is a great fit. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:5044, logstash-2-box:5044 Proxy URL The URL of the SOCKS5 proxy to use when connecting to the Logstash servers. The value must be a URL with a scheme of socks5://. socks5://192.168.0.55:1080 Workers The number of workers to use for each host configured to publish events to Redis. Use this setting along with the load-balance option. For example, if you have 2 hosts and 3 workers, in total 6 workers are started (3 for each host). 2 Load Balancing If set to true and multiple hosts or workers are configured, the output plugin load balances published events onto all Redis hosts. true Max Batch Size The maximum number of events to bulk in a single Redis request or pipeline. 2048 Db The Redis database number where the events are published. 0 Password The password to authenticate with. The default is no authentication. \u00a1my_$ecure_p@ssw0rd!","title":"Connectors"},{"location":"configuration/agent/01_connectors/#connectors","text":"\u26a0\ufe0f Changes made within these interfaces require that Filebeat be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite filebeat process restart Dynamite agents rely on filebeat for sending events and alerts to a downstream collector. The following are currently supported. Logstash Elasticsearch (default) Redis Kafka","title":"Connectors"},{"location":"configuration/agent/01_connectors/#logstash","text":"dynamite filebeat config logstash_targets -h Logstash is powerful data-processing pipeline by Elastic.co. It provides a versatile set of configuration driven ingestion, filtering, and transformation functions. Logstash can be paired with a multitude of upstream retention solutions such as ElasticSearch and InfluxDB. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:5044, logstash-2-box:5044 Proxy URL The URL of the SOCKS5 proxy to use when connecting to the Logstash servers. The value must be a URL with a scheme of socks5://. socks5://192.168.0.55:1080 Index Name The index root name to write events to. dynamite_events-%{+yyyy.MM.dd} Async Pipelines Configures the number of batches to be sent asynchronously to Logstash while waiting for ACK from Logstash. 2 Max Batch Size The maximum number of events to bulk in a single LogStash request. 2048","title":"Logstash"},{"location":"configuration/agent/01_connectors/#elasticsearch","text":"sudo dynamite filebeat config elasticsearch_targets -h Elasticsearch is a search engine by Elastic.co that provides distributed, full-text search and supports retention of schemaless data. Elasticsearch is often deployed behind one or more Logstash instances, however in this configuration ingestion is achieved by directly connecting to Elasticsearch's index API. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:9200, elastic-02-box:9200 Index Name The index name to write events to when you\u2019re using daily indices. dynamite_events-%{+yyyy.MM.dd} Username The basic authentication username for connecting to Elasticsearch. elastic Password The basic authentication password for connecting to ElasticSearch. \u00a1my_$ecure_p@ssw0rd!","title":"ElasticSearch"},{"location":"configuration/agent/01_connectors/#kafka","text":"sudo dynamite filebeat config kafka_targets -h Apache Kafka is a distributed event streaming platform and brokering solution providing high-performance data pipelines for realtime feeds. If extremely high volumes of network traffic are expected, Kafka is a good option. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:5044, logstash-2-box:5044 Topic The Kafka topic used for produced events. dynamite_events Username The username for connecting to Kafka. If username is configured, the password must be configured as well. Only SASL/PLAIN is supported. dynamite_pub Password The password for connecting to Kafka. \u00a1my_$ecure_p@ssw0rd!","title":"Kafka"},{"location":"configuration/agent/01_connectors/#redis","text":"sudo dynamite filebeat config redis_targets -h Redis is an open source, in-memory data structure store, used as a database, cache, and message broker. If your goal is short-lived visualizations of realtime network analytics, Redis is a great fit. Option Name Description Example Host/Ports A comma delimited list of host:port pairs. This list represents where to send the logs. localhost:5044, logstash-2-box:5044 Proxy URL The URL of the SOCKS5 proxy to use when connecting to the Logstash servers. The value must be a URL with a scheme of socks5://. socks5://192.168.0.55:1080 Workers The number of workers to use for each host configured to publish events to Redis. Use this setting along with the load-balance option. For example, if you have 2 hosts and 3 workers, in total 6 workers are started (3 for each host). 2 Load Balancing If set to true and multiple hosts or workers are configured, the output plugin load balances published events onto all Redis hosts. true Max Batch Size The maximum number of events to bulk in a single Redis request or pipeline. 2048 Db The Redis database number where the events are published. 0 Password The password to authenticate with. The default is no authentication. \u00a1my_$ecure_p@ssw0rd!","title":"Redis"},{"location":"configuration/agent/02_event_normalization/","text":"Event Normalization \u26a0\ufe0f Changes made within these interfaces require that Zeek be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite zeek process restart DynamiteNSM relies on the open Elastic Common Schema (ECS). The Filebeat service is configured to normalize all events and alerts generated by Zeek in Suricata to this log format. In order for event normalization to function properly ensure that the policy/tuning/json-logs is enabled. dynamite zeek config site scripts --ids 7093e8e --enable \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Id \u2502 Name \u2502 Enabled \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 1144 \u2502 policy/tuning/json-logs \u2502 True \u2502 N/A \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b","title":"Event Normalization"},{"location":"configuration/agent/02_event_normalization/#event-normalization","text":"\u26a0\ufe0f Changes made within these interfaces require that Zeek be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite zeek process restart DynamiteNSM relies on the open Elastic Common Schema (ECS). The Filebeat service is configured to normalize all events and alerts generated by Zeek in Suricata to this log format. In order for event normalization to function properly ensure that the policy/tuning/json-logs is enabled. dynamite zeek config site scripts --ids 7093e8e --enable \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Id \u2502 Name \u2502 Enabled \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 1144 \u2502 policy/tuning/json-logs \u2502 True \u2502 N/A \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b","title":"Event Normalization"},{"location":"configuration/agent/03_scripts_and_rules/","text":"Scripts and Rules \u26a0\ufe0f Changes made within these interfaces require that Zeek and/or Suricata be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite agent process restart Both Zeek and Suricata provide unique methods for extracting information from raw network traffic and building detections. Zeek ships with an event-driven scripting language that gives end-users direct access to traffic flowing over the wire. Zeek Scripts can be used to accomplish anything from protocol parsing to alerting on suspicious activity. Suricata is a more conventional intrusion detection system. Suricata's rules-engine is capable of extremely fast pattern matching, and is excellent for identifying known malicious activity. Working with Scripts and Rules The dynamite commandline utility provides a relatively simple interface for enabling disabling scripts and rules. Zeek You can list all the available scripts with the below command. Note the Id column as these can be used for selection. dynamite zeek config site scripts Alternatively, advanced users may wish to interact directly with the configuration file. If installed with default options it can be found here: /etc/dynamite/zeek/site/local.zeek Users enumerate choice scripts by specifying their --ids dynamite zeek config site scripts --ids 0fd1658 20d7e67 2878963 99e00b3 \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Id \u2502 Name \u2502 Enabled \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 99e00b3 \u2502 protocols/ssh/detect-bruteforcing \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2878963 \u2502 protocols/ssl/log-hostcerts-only \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 0fd1658 \u2502 policy/frameworks/notice/extend-email/hostnames \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 20d7e67 \u2502 packages/cve-2021-44228.git \u2502 False \u2502 N/A \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Enabling and disabling Zeek Scripts can be done simply by adding an --enable or --disable flag. dynamite zeek config site scripts --ids 0fd1658 20d7e67 2878963 99e00b3 --disable Suricata Suricata's rule interface functions almost exactly as Zeek's. To list all available Suricata rules use the below command. dynamite suricata config main rules Specific --ids can also be selected and enabled or disabled. dynamite suricata config main rules --ids 1aeb6e4 e649d09 0bbab15 bdfac29 --enable \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Id \u2502 Name \u2502 Enabled \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 bdfac29 \u2502 ciarmy.rules \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 0bbab15 \u2502 emerging-scada.rules \u2502 False \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 e649d09 \u2502 tls-events.rules \u2502 False \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1aeb6e4 \u2502 emerging-mobile_malware.rules \u2502 False \u2502 N/A \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Advanced users can find the suricata.yaml configuration at /etc/dynamite/suricata/suricata.yaml assuming default options have not been changed. Updating Suricata Rules We leverage Open Emerging Threat Signatures to identify the latest malicious attacks. To update your ruleset simply run: sudo dynamite suricata update By default, DynamiteNSM will install a root user cronjob that runs twice daily to update these rule-sets.","title":"Scripts and Rules"},{"location":"configuration/agent/03_scripts_and_rules/#scripts-and-rules","text":"\u26a0\ufe0f Changes made within these interfaces require that Zeek and/or Suricata be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite agent process restart Both Zeek and Suricata provide unique methods for extracting information from raw network traffic and building detections. Zeek ships with an event-driven scripting language that gives end-users direct access to traffic flowing over the wire. Zeek Scripts can be used to accomplish anything from protocol parsing to alerting on suspicious activity. Suricata is a more conventional intrusion detection system. Suricata's rules-engine is capable of extremely fast pattern matching, and is excellent for identifying known malicious activity.","title":"Scripts and Rules"},{"location":"configuration/agent/03_scripts_and_rules/#working-with-scripts-and-rules","text":"The dynamite commandline utility provides a relatively simple interface for enabling disabling scripts and rules.","title":"Working with Scripts and Rules"},{"location":"configuration/agent/03_scripts_and_rules/#zeek","text":"You can list all the available scripts with the below command. Note the Id column as these can be used for selection. dynamite zeek config site scripts Alternatively, advanced users may wish to interact directly with the configuration file. If installed with default options it can be found here: /etc/dynamite/zeek/site/local.zeek Users enumerate choice scripts by specifying their --ids dynamite zeek config site scripts --ids 0fd1658 20d7e67 2878963 99e00b3 \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Id \u2502 Name \u2502 Enabled \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 99e00b3 \u2502 protocols/ssh/detect-bruteforcing \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2878963 \u2502 protocols/ssl/log-hostcerts-only \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 0fd1658 \u2502 policy/frameworks/notice/extend-email/hostnames \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 20d7e67 \u2502 packages/cve-2021-44228.git \u2502 False \u2502 N/A \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Enabling and disabling Zeek Scripts can be done simply by adding an --enable or --disable flag. dynamite zeek config site scripts --ids 0fd1658 20d7e67 2878963 99e00b3 --disable","title":"Zeek"},{"location":"configuration/agent/03_scripts_and_rules/#suricata","text":"Suricata's rule interface functions almost exactly as Zeek's. To list all available Suricata rules use the below command. dynamite suricata config main rules Specific --ids can also be selected and enabled or disabled. dynamite suricata config main rules --ids 1aeb6e4 e649d09 0bbab15 bdfac29 --enable \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Id \u2502 Name \u2502 Enabled \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 bdfac29 \u2502 ciarmy.rules \u2502 True \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 0bbab15 \u2502 emerging-scada.rules \u2502 False \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 e649d09 \u2502 tls-events.rules \u2502 False \u2502 N/A \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1aeb6e4 \u2502 emerging-mobile_malware.rules \u2502 False \u2502 N/A \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Advanced users can find the suricata.yaml configuration at /etc/dynamite/suricata/suricata.yaml assuming default options have not been changed.","title":"Suricata"},{"location":"configuration/agent/03_scripts_and_rules/#updating-suricata-rules","text":"We leverage Open Emerging Threat Signatures to identify the latest malicious attacks. To update your ruleset simply run: sudo dynamite suricata update By default, DynamiteNSM will install a root user cronjob that runs twice daily to update these rule-sets.","title":"Updating Suricata Rules"},{"location":"configuration/agent/04_inspection_interfaces/","text":"Inspection Interfaces \u26a0\ufe0f Changes made within these interfaces require that Zeek and/or Suricata be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite agent process restart With so many networks now being defined in software adding and removing inspection interfaces in production, settings is becoming much more commonplace. At this time, the dynamite commandline utility does not provide the ability to directly modify this section of the relevant configurations. Users can however use the agent optimize command to re-balance resources against any added/deleted inspection interfaces. dynamite agent optimize --inspect-interfaces ens33 ens34 ens36 --verbose 2021-06-10 12:33:12 AGENT.THREAD_OPTIMIZE INFO | 8 CPU cores detected. 2021-06-10 12:33:12 AGENT.THREAD_OPTIMIZE INFO | Both Zeek and Suricata are installed. Allocating 60% of resources to Zeek, 30% to Suricata, and 10% to Kernel. Configuration Files Zeek By default DynamiteNSM will install Zeek's node.cfg at /opt/dynamite/zeek/etc/node.cfg . [dynamite-logger] type = logger host = localhost [dynamite-proxy-1] type = proxy host = localhost [dynamite-worker-ens33] type = worker interface = af_packet::ens33 lb_method = custom af_packet_fanout_id = 30983 af_packet_fanout_mode = AF_Packet::FANOUT_HASH lb_procs = 5 pin_cpus = 1,2,3,4,5 host = localhost [dynamite-manager] type = manager host = localhost Suricata By default, Suricata's main config will be installed to /etc/dynamite/suricata/suricata.yaml Suricata provides a threading section for pinning CPUs to thread-families . threading: cpu-affinity: - management-cpu-set: cpu: - 6 - receive-cpu-set: cpu: - 7 - worker-cpu-set: cpu: - 7 mode: exclusive threads: 1 set-cpu-affinity: true","title":"Inspection Interfaces"},{"location":"configuration/agent/04_inspection_interfaces/#inspection-interfaces","text":"\u26a0\ufe0f Changes made within these interfaces require that Zeek and/or Suricata be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite agent process restart With so many networks now being defined in software adding and removing inspection interfaces in production, settings is becoming much more commonplace. At this time, the dynamite commandline utility does not provide the ability to directly modify this section of the relevant configurations. Users can however use the agent optimize command to re-balance resources against any added/deleted inspection interfaces. dynamite agent optimize --inspect-interfaces ens33 ens34 ens36 --verbose 2021-06-10 12:33:12 AGENT.THREAD_OPTIMIZE INFO | 8 CPU cores detected. 2021-06-10 12:33:12 AGENT.THREAD_OPTIMIZE INFO | Both Zeek and Suricata are installed. Allocating 60% of resources to Zeek, 30% to Suricata, and 10% to Kernel.","title":"Inspection Interfaces"},{"location":"configuration/agent/04_inspection_interfaces/#configuration-files","text":"","title":"Configuration Files"},{"location":"configuration/agent/04_inspection_interfaces/#zeek","text":"By default DynamiteNSM will install Zeek's node.cfg at /opt/dynamite/zeek/etc/node.cfg . [dynamite-logger] type = logger host = localhost [dynamite-proxy-1] type = proxy host = localhost [dynamite-worker-ens33] type = worker interface = af_packet::ens33 lb_method = custom af_packet_fanout_id = 30983 af_packet_fanout_mode = AF_Packet::FANOUT_HASH lb_procs = 5 pin_cpus = 1,2,3,4,5 host = localhost [dynamite-manager] type = manager host = localhost","title":"Zeek"},{"location":"configuration/agent/04_inspection_interfaces/#suricata","text":"By default, Suricata's main config will be installed to /etc/dynamite/suricata/suricata.yaml Suricata provides a threading section for pinning CPUs to thread-families . threading: cpu-affinity: - management-cpu-set: cpu: - 6 - receive-cpu-set: cpu: - 7 - worker-cpu-set: cpu: - 7 mode: exclusive threads: 1 set-cpu-affinity: true","title":"Suricata"},{"location":"configuration/monitor/01_elasticsearch/","text":"Elasticsearch \u26a0\ufe0f Changes made within these interfaces require that Elasticsearch be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite elasticsearch process restart DynamiteNSM exposes two configurations specific to Elasticsearch: java and main . The java configuration allows users to automatically adjust the heap_space allocated to Elasticsearch. The main configuration provides limited access into several relevant sections of the elasticsearch.yaml . Java To display current java options. dynamite elasticsearch config java \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 initial_memory \u2502 8g \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 maximum_memory \u2502 8g \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b To update Elasticsearch's allocated heap-space: sudo dynamite elasticsearch config java --initial-memory 12g --maximum-memory 12g Main To display the current main configuration options. sudo dynamite elasticsearch config main \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 node_name \u2502 dyna_dev_es_node \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 cluster_name \u2502 dynamite-nsm-cluster \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 seed_hosts \u2502 ['192.168.194.143'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 initial_master_nodes \u2502 ['jamindev_es_node'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 network_host \u2502 192.168.194.143 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 http_port \u2502 9200 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 path_logs \u2502 /var/log/dynamite/elasticsearch/ \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 search_max_buckets \u2502 10000 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 rest_api_pem_cert_file \u2502 security/auth/admin.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 rest_api_pem_key_file \u2502 security/auth/admin-key.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 rest_api_trusted_cas_file \u2502 security/auth/root-ca.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 transport_pem_cert_file \u2502 security/auth/admin.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 transport_pem_key_file \u2502 security/auth/admin-key.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 transport_trusted_cas_file \u2502 security/auth/root-ca.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 authcz_admin_distinguished_names \u2502 ['C=US,ST=GA,L=Atlanta,O=Dynamite,OU=R&D,CN=dynamite.ai', 'CN=dynamite.ai,OU=R&D,O=Dynamite,L=Atlanta,ST=GA,C=US'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_config_path \u2502 /etc/dynamite/elasticsearch//elasticsearch.yml \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Two update one or more configuration values: sudo dynamite elasticsearch config main --http-port 8080 --search-max-buckets 15000 Update Passwords There are six internal Elasticsearch users available at installation time. The passwords of all of these users can be changed once Elasticsearch has been installed. sudo dynamite elasticsearch config users \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_config_path \u2502 /etc/dynamite/elasticsearch/security/internal_users.yml \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 admin \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana_server \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana_readonly \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 logstash \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 readall \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 snapshot_restore \u2502 Hidden \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b To change a password simply run the below command. sudo dynamite elasticsearch config users --admin \"my$ecureP@ss!\"","title":"Elasticsearch"},{"location":"configuration/monitor/01_elasticsearch/#elasticsearch","text":"\u26a0\ufe0f Changes made within these interfaces require that Elasticsearch be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite elasticsearch process restart DynamiteNSM exposes two configurations specific to Elasticsearch: java and main . The java configuration allows users to automatically adjust the heap_space allocated to Elasticsearch. The main configuration provides limited access into several relevant sections of the elasticsearch.yaml .","title":"Elasticsearch"},{"location":"configuration/monitor/01_elasticsearch/#java","text":"To display current java options. dynamite elasticsearch config java \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 initial_memory \u2502 8g \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 maximum_memory \u2502 8g \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b To update Elasticsearch's allocated heap-space: sudo dynamite elasticsearch config java --initial-memory 12g --maximum-memory 12g","title":"Java"},{"location":"configuration/monitor/01_elasticsearch/#main","text":"To display the current main configuration options. sudo dynamite elasticsearch config main \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 node_name \u2502 dyna_dev_es_node \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 cluster_name \u2502 dynamite-nsm-cluster \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 seed_hosts \u2502 ['192.168.194.143'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 initial_master_nodes \u2502 ['jamindev_es_node'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 network_host \u2502 192.168.194.143 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 http_port \u2502 9200 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 path_logs \u2502 /var/log/dynamite/elasticsearch/ \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 search_max_buckets \u2502 10000 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 rest_api_pem_cert_file \u2502 security/auth/admin.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 rest_api_pem_key_file \u2502 security/auth/admin-key.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 rest_api_trusted_cas_file \u2502 security/auth/root-ca.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 transport_pem_cert_file \u2502 security/auth/admin.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 transport_pem_key_file \u2502 security/auth/admin-key.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 transport_trusted_cas_file \u2502 security/auth/root-ca.pem \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 authcz_admin_distinguished_names \u2502 ['C=US,ST=GA,L=Atlanta,O=Dynamite,OU=R&D,CN=dynamite.ai', 'CN=dynamite.ai,OU=R&D,O=Dynamite,L=Atlanta,ST=GA,C=US'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_config_path \u2502 /etc/dynamite/elasticsearch//elasticsearch.yml \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Two update one or more configuration values: sudo dynamite elasticsearch config main --http-port 8080 --search-max-buckets 15000","title":"Main"},{"location":"configuration/monitor/01_elasticsearch/#update-passwords","text":"There are six internal Elasticsearch users available at installation time. The passwords of all of these users can be changed once Elasticsearch has been installed. sudo dynamite elasticsearch config users \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_config_path \u2502 /etc/dynamite/elasticsearch/security/internal_users.yml \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 admin \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana_server \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana_readonly \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 logstash \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 readall \u2502 Hidden \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 snapshot_restore \u2502 Hidden \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b To change a password simply run the below command. sudo dynamite elasticsearch config users --admin \"my$ecureP@ss!\"","title":"Update Passwords"},{"location":"configuration/monitor/02_kibana/","text":"Kibana \u26a0\ufe0f Changes made within these interfaces require that Kibana be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite kibana process restart DynamiteNSM exposes only one Kibana related configuration: main . The main configuration provides limited access into several relevant sections of the kibana.yaml . Main To display the current main configuration options. dynamite kibana config main \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 host \u2502 192.168.194.143 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 port \u2502 5601 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_targets \u2502 ['https://192.168.194.143:9200'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_username \u2502 kibanaserver \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_password \u2502 kibanaserver \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana_config_path \u2502 /etc/dynamite/kibana/kibana.yml \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b To update one or more configuration values: sudo dynamite kibana config main --elasticsearch-username kibanaserver --elasticsearch-password \"changeme\"","title":"Kibana"},{"location":"configuration/monitor/02_kibana/#kibana","text":"\u26a0\ufe0f Changes made within these interfaces require that Kibana be restarted. Typically, the easiest way to accomplish this is via the command: sudo dynamite kibana process restart DynamiteNSM exposes only one Kibana related configuration: main . The main configuration provides limited access into several relevant sections of the kibana.yaml .","title":"Kibana"},{"location":"configuration/monitor/02_kibana/#main","text":"To display the current main configuration options. dynamite kibana config main \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 host \u2502 192.168.194.143 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 port \u2502 5601 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_targets \u2502 ['https://192.168.194.143:9200'] \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_username \u2502 kibanaserver \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch_password \u2502 kibanaserver \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana_config_path \u2502 /etc/dynamite/kibana/kibana.yml \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b To update one or more configuration values: sudo dynamite kibana config main --elasticsearch-username kibanaserver --elasticsearch-password \"changeme\"","title":"Main"},{"location":"guides/01_quick_start/","text":"Quick Start This document will walk you through getting DynamiteNSM up in running in a small environment. This setup will work with small lab environments, but is not suggested for scenarios where over 300MiBs of sustained throughput is expected. Pre-requisites Two physical or virtual machines running a supported operating system . One machine will be dedicated to packet acquisition and analysis - The Agent ; the other for the storing and presentation of this data - The Monitor . Role RAM CPUs Network Interfaces Monitor 8 4 1 Agent 32 8 2 \u24d8 For the sake of testing your RAM and CPU can be decreased below the above small-network recommendation, however this may result in dropped packets depending on your average throughput. \u24d8 dynamite zeek logs metrics --pretty and dynamite suricata logs metrics --pretty can be used to watch for dropped packets. A physical or virtual switch capable of SPANing or a dedicated TAP device. Python 3.7+ . Setup DynamiteNSM SDKs and utilities. Install DynamiteNSM libraries and command-line utilities. pip install dynamite-nsm Initialize the environment enabling services to be installed and managed. sudo dynamite setup install Install the Monitor The monitor consists of the services which receive network events/alerts from the agent(s), and normalize/visualize them in ways that can be useful for security and operational use-cases. On your first computer, that you will use for monitoring, run the below command. sudo dynamite monitor install Once installed, you may start the monitor. sudo dynamite monitor process start Verify services are running. sudo dynamite monitor process status \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 Running \u2502 Enabled on Startup \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana \u2502 yes \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch \u2502 yes \u2502 yes \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Log into Elasticsearch/Kibana. The default credentials for both are admin/admin . Be sure to select the global tenant when prompted, as we install several default dashboards and visualizations to this space. \u24d8 Note that you it takes time to start these services. You may get connection timeouts initially. You can access elasticsearch and kibana at the below URLs. Monitor Tool URL Elasticsearch https://<management_ip>:9200 Kibana http://<management_ip>:5601 Install the Agent The agent is responsible for acquiring network packets off one or more SPAN/TAP interface and distilling these packets into meaningful events and alerts that can be sent to a Dynamite Monitor or supported collector. On the computer dedicated to packet acquisition determine which network interface you wish to use to monitor traffic. ifconfig and ip addr are useful commands for enumerating the interfaces you have available. Begin the agent installation sudo dynamite agent install --target-type=elasticsearch --targets=https://<monitor-ip-address>:9200 --inspect-interfaces=<mon_iface0> <mon_iface1> Start the agent. sudo dynamite agent process start Confirm the agent is running as expected sudo dynamite agent process status \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 Running \u2502 Enabled on Startup \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 filebeat \u2502 yes \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 zeek \u2502 yes \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 suricata \u2502 yes \u2502 yes \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Confirm that we were able to connect to Elasticsearch sudo dynamite filebeat logs main --pretty \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Time \u2502 Log Level \u2502 Category \u2502 Message \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2021-04-25 17:06:50.780000 \u2502 INFO \u2502 publisher_pipeline_output \u2502 Connection to backoff(elasticsearch(https://192.168.194.143:9200)) established \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Adding Additional Inspection Interfaces Users can easily add new network interfaces for both Zeek and Suricata. dynamite zeek reset node --inspect-interfaces=<inspect-iface-1> <inspect-iface-2> dynamite suricata reset --inspect-interfaces=<inspect-iface-1> <inspect-iface-2> Once your desired configurations are applied to be sure to run the agent optimize command to ensure resources are being balanced between Zeek and Suricata sanely. dynamite agent optimize You must restart the agent for changes to be applied. dynamite agent process restart Manage this Instance Remotely dynamite-nsm now ships with a remote management utility creatively named dynamite-remote . Unlike the dynamite utility dynamite-remote can be run on most *NIX operating systems that have openssh-client installed. First create an authentication package on your remote management server. You can install this utility on the management server simply by installing the latest version of dynamite-nsm via pip3 or a tool like it. user@remote-server:~# dynamite-remote create --name agent1 --host agent1.dev.local --port 22 --description \"agent1 traffic sensor\" Move the authentication package created by the above command over to your agent1 node. scp agent1.tar.gz user@agent1.dev.local:/home/user/ Use the dynamite auth command to install the authentication package you generated. root@agent1.dev.local:~# dynamite auth install --archive /home/user/agent.tar.gz On the remote machine you should now be able to run commands on agent1.dev.local dynamite-remote execute dev-machine \"zeek config site scripts\"","title":"Quick Start"},{"location":"guides/01_quick_start/#quick-start","text":"This document will walk you through getting DynamiteNSM up in running in a small environment. This setup will work with small lab environments, but is not suggested for scenarios where over 300MiBs of sustained throughput is expected.","title":"Quick Start"},{"location":"guides/01_quick_start/#pre-requisites","text":"Two physical or virtual machines running a supported operating system . One machine will be dedicated to packet acquisition and analysis - The Agent ; the other for the storing and presentation of this data - The Monitor . Role RAM CPUs Network Interfaces Monitor 8 4 1 Agent 32 8 2 \u24d8 For the sake of testing your RAM and CPU can be decreased below the above small-network recommendation, however this may result in dropped packets depending on your average throughput. \u24d8 dynamite zeek logs metrics --pretty and dynamite suricata logs metrics --pretty can be used to watch for dropped packets. A physical or virtual switch capable of SPANing or a dedicated TAP device. Python 3.7+ .","title":"Pre-requisites"},{"location":"guides/01_quick_start/#setup-dynamitensm-sdks-and-utilities","text":"Install DynamiteNSM libraries and command-line utilities. pip install dynamite-nsm Initialize the environment enabling services to be installed and managed. sudo dynamite setup install","title":"Setup DynamiteNSM SDKs and utilities."},{"location":"guides/01_quick_start/#install-the-monitor","text":"The monitor consists of the services which receive network events/alerts from the agent(s), and normalize/visualize them in ways that can be useful for security and operational use-cases. On your first computer, that you will use for monitoring, run the below command. sudo dynamite monitor install Once installed, you may start the monitor. sudo dynamite monitor process start Verify services are running. sudo dynamite monitor process status \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 Running \u2502 Enabled on Startup \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 kibana \u2502 yes \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 elasticsearch \u2502 yes \u2502 yes \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Log into Elasticsearch/Kibana. The default credentials for both are admin/admin . Be sure to select the global tenant when prompted, as we install several default dashboards and visualizations to this space. \u24d8 Note that you it takes time to start these services. You may get connection timeouts initially. You can access elasticsearch and kibana at the below URLs. Monitor Tool URL Elasticsearch https://<management_ip>:9200 Kibana http://<management_ip>:5601","title":"Install the Monitor"},{"location":"guides/01_quick_start/#install-the-agent","text":"The agent is responsible for acquiring network packets off one or more SPAN/TAP interface and distilling these packets into meaningful events and alerts that can be sent to a Dynamite Monitor or supported collector. On the computer dedicated to packet acquisition determine which network interface you wish to use to monitor traffic. ifconfig and ip addr are useful commands for enumerating the interfaces you have available. Begin the agent installation sudo dynamite agent install --target-type=elasticsearch --targets=https://<monitor-ip-address>:9200 --inspect-interfaces=<mon_iface0> <mon_iface1> Start the agent. sudo dynamite agent process start Confirm the agent is running as expected sudo dynamite agent process status \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 Running \u2502 Enabled on Startup \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 filebeat \u2502 yes \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 zeek \u2502 yes \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 suricata \u2502 yes \u2502 yes \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Confirm that we were able to connect to Elasticsearch sudo dynamite filebeat logs main --pretty \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Time \u2502 Log Level \u2502 Category \u2502 Message \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2021-04-25 17:06:50.780000 \u2502 INFO \u2502 publisher_pipeline_output \u2502 Connection to backoff(elasticsearch(https://192.168.194.143:9200)) established \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b","title":"Install the Agent"},{"location":"guides/01_quick_start/#adding-additional-inspection-interfaces","text":"Users can easily add new network interfaces for both Zeek and Suricata. dynamite zeek reset node --inspect-interfaces=<inspect-iface-1> <inspect-iface-2> dynamite suricata reset --inspect-interfaces=<inspect-iface-1> <inspect-iface-2> Once your desired configurations are applied to be sure to run the agent optimize command to ensure resources are being balanced between Zeek and Suricata sanely. dynamite agent optimize You must restart the agent for changes to be applied. dynamite agent process restart","title":"Adding Additional Inspection Interfaces"},{"location":"guides/01_quick_start/#manage-this-instance-remotely","text":"dynamite-nsm now ships with a remote management utility creatively named dynamite-remote . Unlike the dynamite utility dynamite-remote can be run on most *NIX operating systems that have openssh-client installed. First create an authentication package on your remote management server. You can install this utility on the management server simply by installing the latest version of dynamite-nsm via pip3 or a tool like it. user@remote-server:~# dynamite-remote create --name agent1 --host agent1.dev.local --port 22 --description \"agent1 traffic sensor\" Move the authentication package created by the above command over to your agent1 node. scp agent1.tar.gz user@agent1.dev.local:/home/user/ Use the dynamite auth command to install the authentication package you generated. root@agent1.dev.local:~# dynamite auth install --archive /home/user/agent.tar.gz On the remote machine you should now be able to run commands on agent1.dev.local dynamite-remote execute dev-machine \"zeek config site scripts\"","title":"Manage this Instance Remotely"},{"location":"guides/02_using_kibana_package/","text":"Working with Kibana Package Manager Kibana package manager can be used to install custom visualization packages useful for viewing your network from many perspectives. There are 5 sub-commands for the package manager. Install - Installs one or many packages from the provided path. Uninstall - Uninstall packages interactively or by providing a package id. List - List installed packages and their related saved objects. List Saved Objects - List all saved objects irrespective of their relationship to packages. List Tenants - List tenants available to be used as installation destinations if multitenancy is enabled. Install a package Basic Usage Installation of a package is straightforward, supply the path to the package to be installed and the credentials required for authenticating to Kibana. $ sudo dynamite kibana package install --path packages/investigator.tar.xz KIBANA.PACKAGE INFO | Checking connection to Kibana. KIBANA.PACKAGE INFO | Installing from TAR archive: packages/investigator.tar.xz. KIBANA.PACKAGE INFO | Dynamite Investigator installation succeeded! OK Duplication avoidance The package manager will detect duplicate packages at install time and ask if you wish to uninstall the existing package unless the --ignore-warnings flag is used. Multitenancy Install The --tenant option can be used to install packages to a tenant specifically, this is useful for organizing your workflows. Remote Kibana Target By default the package manager will look for the local kibana configuration to determine the url where the kibana instance is reachable at, and will fall back to the local primary ip address and the defaut kibana port if configs are not available. Using the --target flag, the user can specify which kibana instance the package manager should operate on. Uninstall a package By default packages are uninstalled interactively unless the --package-id parameter is passed. The number of options in the interactive uninstallation flow can be narrowed down by using the --package-name option, and only packages matching that string will be returned to the uninstaller. If there is only one package installed, or a single result for a provided search query, that package will be uninstalled without asking the user to select it from a list. $ sudo dynamite kibana package uninstall Select a package to uninstall: [1] Protocol Piechart - [private] * Basic piechart displaying traffic by protocol [2] Dynamite Investigator - [global] * Provides basic tools for investigation Select package(s) to uninstall (For example: \"1 2 3 5 8\") List installed packages Get meta information about the installed packages and details about the objects contained within them. By default listing installed packages returns data in JSON format, use the --pretty option to get a human-readable table. ~$ sudo dynamite kibana package list --pretty List installed saved objects Returns information regarding the saved objects installed on the kibana instance whether or not they are part of a package. By default listing installed saved returns data in JSON format, use the --pretty option to get a human-readable table. $ sudo dynamite kibana package list-saved-objects --pretty List Tenants Lists the available OpenSearch Tenants that can be used as installation destinations, by default packages are installed to the global tenant. By default listing tenants returns data in JSON format, use the --pretty option to get a human-readable table. $ sudo dynamite kibana package list-tenants --pretty","title":"Working with Kibana Package Manager"},{"location":"guides/02_using_kibana_package/#working-with-kibana-package-manager","text":"Kibana package manager can be used to install custom visualization packages useful for viewing your network from many perspectives.","title":"Working with Kibana Package Manager"},{"location":"guides/02_using_kibana_package/#there-are-5-sub-commands-for-the-package-manager","text":"Install - Installs one or many packages from the provided path. Uninstall - Uninstall packages interactively or by providing a package id. List - List installed packages and their related saved objects. List Saved Objects - List all saved objects irrespective of their relationship to packages. List Tenants - List tenants available to be used as installation destinations if multitenancy is enabled.","title":"There are 5 sub-commands for the package manager."},{"location":"guides/02_using_kibana_package/#install-a-package","text":"","title":"Install a package"},{"location":"guides/02_using_kibana_package/#basic-usage","text":"Installation of a package is straightforward, supply the path to the package to be installed and the credentials required for authenticating to Kibana. $ sudo dynamite kibana package install --path packages/investigator.tar.xz KIBANA.PACKAGE INFO | Checking connection to Kibana. KIBANA.PACKAGE INFO | Installing from TAR archive: packages/investigator.tar.xz. KIBANA.PACKAGE INFO | Dynamite Investigator installation succeeded! OK","title":"Basic Usage"},{"location":"guides/02_using_kibana_package/#duplication-avoidance","text":"The package manager will detect duplicate packages at install time and ask if you wish to uninstall the existing package unless the --ignore-warnings flag is used.","title":"Duplication avoidance"},{"location":"guides/02_using_kibana_package/#multitenancy-install","text":"The --tenant option can be used to install packages to a tenant specifically, this is useful for organizing your workflows.","title":"Multitenancy Install"},{"location":"guides/02_using_kibana_package/#remote-kibana-target","text":"By default the package manager will look for the local kibana configuration to determine the url where the kibana instance is reachable at, and will fall back to the local primary ip address and the defaut kibana port if configs are not available. Using the --target flag, the user can specify which kibana instance the package manager should operate on.","title":"Remote Kibana Target"},{"location":"guides/02_using_kibana_package/#uninstall-a-package","text":"By default packages are uninstalled interactively unless the --package-id parameter is passed. The number of options in the interactive uninstallation flow can be narrowed down by using the --package-name option, and only packages matching that string will be returned to the uninstaller. If there is only one package installed, or a single result for a provided search query, that package will be uninstalled without asking the user to select it from a list. $ sudo dynamite kibana package uninstall Select a package to uninstall: [1] Protocol Piechart - [private] * Basic piechart displaying traffic by protocol [2] Dynamite Investigator - [global] * Provides basic tools for investigation Select package(s) to uninstall (For example: \"1 2 3 5 8\")","title":"Uninstall a package"},{"location":"guides/02_using_kibana_package/#list-installed-packages","text":"Get meta information about the installed packages and details about the objects contained within them. By default listing installed packages returns data in JSON format, use the --pretty option to get a human-readable table. ~$ sudo dynamite kibana package list --pretty","title":"List installed packages"},{"location":"guides/02_using_kibana_package/#list-installed-saved-objects","text":"Returns information regarding the saved objects installed on the kibana instance whether or not they are part of a package. By default listing installed saved returns data in JSON format, use the --pretty option to get a human-readable table. $ sudo dynamite kibana package list-saved-objects --pretty","title":"List installed saved objects"},{"location":"guides/02_using_kibana_package/#list-tenants","text":"Lists the available OpenSearch Tenants that can be used as installation destinations, by default packages are installed to the global tenant. By default listing tenants returns data in JSON format, use the --pretty option to get a human-readable table. $ sudo dynamite kibana package list-tenants --pretty","title":"List Tenants"},{"location":"guides/03_dynamite_remote/","text":"Dynamite Remote The dynamite-remote utility is a self-contained script that ships with the dynamite-nsm package. The utility allows administrators to create Authentication packages that once installed on remote instances allow remote management. The dynamite-remote script works on most *NIX operating systems with openssh-client installed. Usage The dynamite-remote utility should be run on the machine you wish to use for remote management. Once an authentication package has been created you can install it on a remote node to easily manage it. $ dynamite-remote -h usage: Dynamite Remote [-h] {create,remove,list,execute} ... Remotely manage DynamiteNSM nodes across your network environments. positional arguments: {create,remove,list,execute} create Generate an authentication package that can be installed on a remote node allowing management. Add the remote to list of controllable nodes. remove Remove a remote that was previously created. list List the nodes we can control remotely. execute Run a command against a remote node. optional arguments: -h, --help show this help message and exit Create Remote Authentication Package $ dynamite-remote create -h usage: Dynamite Remote create [-h] --name NAME --host HOST [--port PORT] [--description DESCRIPTION] optional arguments: -h, --help show this help message and exit --name NAME A friendly name for the remote node. --host HOST A host or IP address of the remote node you will be connecting to. --port PORT The corresponding SSH port to use. --description DESCRIPTION A description of this node (E.G web-server environment sensor) Execute on Remote $ dynamite-remote execute -h usage: Dynamite Remote execute [-h] remote command [command ...] positional arguments: remote The name of the node or node group to execute the command against. command The command to run on the remote node (E.G 'elasticsearch process status'). optional arguments: -h, --help show this help message and exit Remove Remote $ dynamite-remote remove -h usage: Dynamite Remote remove [-h] remote positional arguments: remote The name of the node or node group to remove. optional arguments: -h, --help show this help message and exit","title":"Remote Management"},{"location":"guides/03_dynamite_remote/#dynamite-remote","text":"The dynamite-remote utility is a self-contained script that ships with the dynamite-nsm package. The utility allows administrators to create Authentication packages that once installed on remote instances allow remote management. The dynamite-remote script works on most *NIX operating systems with openssh-client installed.","title":"Dynamite Remote"},{"location":"guides/03_dynamite_remote/#usage","text":"The dynamite-remote utility should be run on the machine you wish to use for remote management. Once an authentication package has been created you can install it on a remote node to easily manage it. $ dynamite-remote -h usage: Dynamite Remote [-h] {create,remove,list,execute} ... Remotely manage DynamiteNSM nodes across your network environments. positional arguments: {create,remove,list,execute} create Generate an authentication package that can be installed on a remote node allowing management. Add the remote to list of controllable nodes. remove Remove a remote that was previously created. list List the nodes we can control remotely. execute Run a command against a remote node. optional arguments: -h, --help show this help message and exit","title":"Usage"},{"location":"guides/03_dynamite_remote/#create-remote-authentication-package","text":"$ dynamite-remote create -h usage: Dynamite Remote create [-h] --name NAME --host HOST [--port PORT] [--description DESCRIPTION] optional arguments: -h, --help show this help message and exit --name NAME A friendly name for the remote node. --host HOST A host or IP address of the remote node you will be connecting to. --port PORT The corresponding SSH port to use. --description DESCRIPTION A description of this node (E.G web-server environment sensor)","title":"Create Remote Authentication Package"},{"location":"guides/03_dynamite_remote/#execute-on-remote","text":"$ dynamite-remote execute -h usage: Dynamite Remote execute [-h] remote command [command ...] positional arguments: remote The name of the node or node group to execute the command against. command The command to run on the remote node (E.G 'elasticsearch process status'). optional arguments: -h, --help show this help message and exit","title":"Execute on Remote"},{"location":"guides/03_dynamite_remote/#remove-remote","text":"$ dynamite-remote remove -h usage: Dynamite Remote remove [-h] remote positional arguments: remote The name of the node or node group to remove. optional arguments: -h, --help show this help message and exit","title":"Remove Remote"},{"location":"guides/base_views/01_overview/","text":"Overview By default , DynamiteNSM ships with a powerful analytics package containing dashboards and visualizations, purpose built for a variety of operational and detection use-cases. Modules Within the BaseViews Kibana package, modules designate a collection of views built around a specific sub-set of filters. These filters slice Dynamite's ECS based data into four categories: alerts, events, hosts, and protocols. Each module provides a unique perspective into one of these categories. To view a specific module, click the module name in the left pane of the view. Module views are structured in a standard way where you'll find links to additional views within the module at the very top of the page, directly below the view title. Below that you'll find histograms and other visualizations that illuminate porportions of different categorical values in the data set. Visualizations are interactive, and by clicking on a value you can quickly include or exclude the value from the filter in use by the view. After you've explored the view's visualizations, use the data table at the bottom for more detailed analysis and to access pivots to other relevant views. Alerts Alerts are typically indicative of suspicious or malicious behavior. The Alerts module provides different perspectives that make investigating this suspicious traffic, simple and efficient. \u24d8 You can always adjust the kinds of alerts that get triggered through the dynamite commandline utility. Conversations Conversations represent bi-directional communications between hosts. The Conversations module provides high-level summaries of top-talkers, ports, and application protocols and can serve as a starting point for more focused threat hunts. Hosts The Hosts module contains views that provide metrics from the perspective of internal and external hosts. This is a host-centric take on analyzing network traffic and allows you to quickly identify specific hosts that can serve as the focal point of your analysis efforts. Protocols The Protocols module primarily focuses on common application layer protocols. Views in this module function as a launch point into other protocol specific views. \u24d8 You can always adjust the kinds of protocols that get analyzed through the dynamite commandline utility.","title":"Overview"},{"location":"guides/base_views/01_overview/#overview","text":"By default , DynamiteNSM ships with a powerful analytics package containing dashboards and visualizations, purpose built for a variety of operational and detection use-cases.","title":"Overview"},{"location":"guides/base_views/01_overview/#modules","text":"Within the BaseViews Kibana package, modules designate a collection of views built around a specific sub-set of filters. These filters slice Dynamite's ECS based data into four categories: alerts, events, hosts, and protocols. Each module provides a unique perspective into one of these categories. To view a specific module, click the module name in the left pane of the view. Module views are structured in a standard way where you'll find links to additional views within the module at the very top of the page, directly below the view title. Below that you'll find histograms and other visualizations that illuminate porportions of different categorical values in the data set. Visualizations are interactive, and by clicking on a value you can quickly include or exclude the value from the filter in use by the view. After you've explored the view's visualizations, use the data table at the bottom for more detailed analysis and to access pivots to other relevant views.","title":"Modules"},{"location":"guides/base_views/01_overview/#alerts","text":"Alerts are typically indicative of suspicious or malicious behavior. The Alerts module provides different perspectives that make investigating this suspicious traffic, simple and efficient. \u24d8 You can always adjust the kinds of alerts that get triggered through the dynamite commandline utility.","title":"Alerts"},{"location":"guides/base_views/01_overview/#conversations","text":"Conversations represent bi-directional communications between hosts. The Conversations module provides high-level summaries of top-talkers, ports, and application protocols and can serve as a starting point for more focused threat hunts.","title":"Conversations"},{"location":"guides/base_views/01_overview/#hosts","text":"The Hosts module contains views that provide metrics from the perspective of internal and external hosts. This is a host-centric take on analyzing network traffic and allows you to quickly identify specific hosts that can serve as the focal point of your analysis efforts.","title":"Hosts"},{"location":"guides/base_views/01_overview/#protocols","text":"The Protocols module primarily focuses on common application layer protocols. Views in this module function as a launch point into other protocol specific views. \u24d8 You can always adjust the kinds of protocols that get analyzed through the dynamite commandline utility.","title":"Protocols"},{"location":"guides/base_views/02_alerts/","text":"Alerts Module The Alerts Module is tailored to the analysis of Suricata alerts. The primary view contains a histogram that provides a breakdown of alert volume by category. Below you'll find categorical visualizations that provide different vantage points into notable alert attributes. Click on a value in the legend to add/remove specific values from the view filter. Alert Map The Alert Map view provides a world map showing the volume of alerts (represented as a color) generated by countries around the world. Alerts shown in this view originated externally, that is outside of the monitored network environment. While exploring the world map, click on a particular country to add it to the view filter. The list of alerts shown in the data table will be updated allowing you to quickly triage the alerts source from the selected country. Discovery View The Discovery View in the Alerts module provides a list of raw alerts in a customized data table. This view provides an easy-to-digest list of alerts that can be explored using the search bar and filters. Most commonly, the discovery view is used when you want to search for alerts of a particular type or to/from a particular host. This view is also great for refining queries and filters and turning them into custom visualizations. Initial Access The Initial Access view shows a pre-filtered list of alerts that align to MITRE ATT&CK's Initial Access tactic category. This primarily includes alerts related to exploitation attempts or attempted authentication using default or guessed credentials. Reconnaissance The Reconnaissance view shows a pre-filtered list of alerts that align to MITRE ATT&CK's Reconnaissance tactic category. This primarily includes alerts related to active scanning or probing of accessible hosts and services.","title":"Alert"},{"location":"guides/base_views/02_alerts/#alerts-module","text":"The Alerts Module is tailored to the analysis of Suricata alerts. The primary view contains a histogram that provides a breakdown of alert volume by category. Below you'll find categorical visualizations that provide different vantage points into notable alert attributes. Click on a value in the legend to add/remove specific values from the view filter.","title":"Alerts Module"},{"location":"guides/base_views/02_alerts/#alert-map","text":"The Alert Map view provides a world map showing the volume of alerts (represented as a color) generated by countries around the world. Alerts shown in this view originated externally, that is outside of the monitored network environment. While exploring the world map, click on a particular country to add it to the view filter. The list of alerts shown in the data table will be updated allowing you to quickly triage the alerts source from the selected country.","title":"Alert Map"},{"location":"guides/base_views/02_alerts/#discovery-view","text":"The Discovery View in the Alerts module provides a list of raw alerts in a customized data table. This view provides an easy-to-digest list of alerts that can be explored using the search bar and filters. Most commonly, the discovery view is used when you want to search for alerts of a particular type or to/from a particular host. This view is also great for refining queries and filters and turning them into custom visualizations.","title":"Discovery View"},{"location":"guides/base_views/02_alerts/#initial-access","text":"The Initial Access view shows a pre-filtered list of alerts that align to MITRE ATT&CK's Initial Access tactic category. This primarily includes alerts related to exploitation attempts or attempted authentication using default or guessed credentials.","title":"Initial Access"},{"location":"guides/base_views/02_alerts/#reconnaissance","text":"The Reconnaissance view shows a pre-filtered list of alerts that align to MITRE ATT&CK's Reconnaissance tactic category. This primarily includes alerts related to active scanning or probing of accessible hosts and services.","title":"Reconnaissance"},{"location":"guides/base_views/03_conversations/","text":"Conversations The Conversations module contains views built for the analysis of host-to-host communications that are not necessarily associated with a suspicious traffic alert. The primary view contains a histogram depicting the volume of connections over time as well as high-level metrics describing the nature of the traffic that occurred during the time frame. Additionally you'll find lists of top-talkers, top ports, and application protocols that can serve as a starting point for more focused threat hunts. Conversations Map The Conversations Map view includes a world map with individual countries colorized to depict the volume of connections that originated from within. Click on a specific country to filter the conversations for traffic that it was involved in. Discovery View The conversations Discovery View is a customized data table built for exploring conversation records. Use the search bar and filters to refine the conversations included in the data table. Expand the details to learn more about the nature of the conversation and to access pivots to other related views.","title":"Conversations"},{"location":"guides/base_views/03_conversations/#conversations","text":"The Conversations module contains views built for the analysis of host-to-host communications that are not necessarily associated with a suspicious traffic alert. The primary view contains a histogram depicting the volume of connections over time as well as high-level metrics describing the nature of the traffic that occurred during the time frame. Additionally you'll find lists of top-talkers, top ports, and application protocols that can serve as a starting point for more focused threat hunts.","title":"Conversations"},{"location":"guides/base_views/03_conversations/#conversations-map","text":"The Conversations Map view includes a world map with individual countries colorized to depict the volume of connections that originated from within. Click on a specific country to filter the conversations for traffic that it was involved in.","title":"Conversations Map"},{"location":"guides/base_views/03_conversations/#discovery-view","text":"The conversations Discovery View is a customized data table built for exploring conversation records. Use the search bar and filters to refine the conversations included in the data table. Expand the details to learn more about the nature of the conversation and to access pivots to other related views.","title":"Discovery View"},{"location":"guides/base_views/04_hosts/","text":"Hosts Module The Hosts module contains views that provide metrics from the perspective of internal and external hosts. The primary view includes several metrics that describe the number of internal vs external hosts observed as well as the traffic volume sent and received from each. Internal Hosts The Internal Hosts view includes a summary of conversations from the perspective of internal, or protected, hosts. At the top of the page you'll find a summary of data transfer in both the inbound and outbound directions. It also includes a list of alerts generated by internal hosts as well as a breakdown of transport protocols in use. Further down the page, you'll find visualizations and tables that identify notable internal hosts based on the average duration and bytes sent/received during relevant conversations. External Hosts The External Hosts view presents a summary of conversations and alerts from the perspective of external, or unknown hosts. It includes summary metrics of data transfer, a list of alerts generated by external hosts as well as a breakdown of transport protocols in use. Further down the page, you'll find visualizations and tables that identify notable external hosts based on the average duration and bytes sent/received during relevant conversations.","title":"Hosts"},{"location":"guides/base_views/04_hosts/#hosts-module","text":"The Hosts module contains views that provide metrics from the perspective of internal and external hosts. The primary view includes several metrics that describe the number of internal vs external hosts observed as well as the traffic volume sent and received from each.","title":"Hosts Module"},{"location":"guides/base_views/04_hosts/#internal-hosts","text":"The Internal Hosts view includes a summary of conversations from the perspective of internal, or protected, hosts. At the top of the page you'll find a summary of data transfer in both the inbound and outbound directions. It also includes a list of alerts generated by internal hosts as well as a breakdown of transport protocols in use. Further down the page, you'll find visualizations and tables that identify notable internal hosts based on the average duration and bytes sent/received during relevant conversations.","title":"Internal Hosts"},{"location":"guides/base_views/04_hosts/#external-hosts","text":"The External Hosts view presents a summary of conversations and alerts from the perspective of external, or unknown hosts. It includes summary metrics of data transfer, a list of alerts generated by external hosts as well as a breakdown of transport protocols in use. Further down the page, you'll find visualizations and tables that identify notable external hosts based on the average duration and bytes sent/received during relevant conversations.","title":"External Hosts"},{"location":"guides/base_views/05_protocols/","text":"Protocols Module The Protocols module provides views geared toward the analysis of common application layer protocol usage. Views in this module function are meant to seed threat hunting efforts and drive investigative workflows that are not dependant on IDS alerts. The primary view includes a histogram showing protocol activity over time as well as a donut visualization representing the volume of connections by protocol. Each of these visualizations is interactive and can be used to adjust the time frame and protocol field filters respectively. Using the view navigation menu (highlighted below) you can quickly access pre-filtered discovery views for exploring communications for a specific protocol.","title":"Protocols"},{"location":"guides/base_views/05_protocols/#protocols-module","text":"The Protocols module provides views geared toward the analysis of common application layer protocol usage. Views in this module function are meant to seed threat hunting efforts and drive investigative workflows that are not dependant on IDS alerts. The primary view includes a histogram showing protocol activity over time as well as a donut visualization representing the volume of connections by protocol. Each of these visualizations is interactive and can be used to adjust the time frame and protocol field filters respectively. Using the view navigation menu (highlighted below) you can quickly access pre-filtered discovery views for exploring communications for a specific protocol.","title":"Protocols Module"},{"location":"guides/base_views/06_pivot_fields/","text":"Pivot Fields Pivot fields are special fields that can be used to dynamically apply specific views and filters. Types of Pivot Fields Community Id Pivots - These pivots rely on a common id that exist within every event and alert generated by the agent. They typically return between 1 and 3. IP Address Pivots - These pivots work on the source.ip and destination.ip fields. There is no upper limit to the number of events that can be returned by this pivot.","title":"Pivot Fields"},{"location":"guides/base_views/06_pivot_fields/#pivot-fields","text":"Pivot fields are special fields that can be used to dynamically apply specific views and filters.","title":"Pivot Fields"},{"location":"guides/base_views/06_pivot_fields/#types-of-pivot-fields","text":"Community Id Pivots - These pivots rely on a common id that exist within every event and alert generated by the agent. They typically return between 1 and 3. IP Address Pivots - These pivots work on the source.ip and destination.ip fields. There is no upper limit to the number of events that can be returned by this pivot.","title":"Types of Pivot Fields"},{"location":"guides/developers/01_overview/","text":"Welcome to the Software Developer Documentation. We're glad you're here. This documentation is targeted a developers who wish to contribute directly to the DynamiteNSM modules. Below you can find documentation to get you started down a development track. Before getting started please checkout our contributing guidelines here . img { transition:transform 0.25s ease; position: relative; } img:hover { -webkit-transform:scale(1.5); /* or some other value */ transform:scale(2); z-index: 9000; } Project Preview Description Link Build a Commandline Utility Create a new service and ship it with minimal code. Guide \u00bb Extend Configuration Manager Extend a service configuration wrapper to handle additional configuration files and options. Coming Soon Create a Kibana Package Create and group Kibana visualizations, dashboards, searches, and more into a self-contained analysis package. Guide \u00bb Explore the SDK Documentation Curious what else you can do? check out the SDK documentation. Docs \u00bb","title":"Overview"},{"location":"guides/developers/01_overview/#welcome-to-the-software-developer-documentation","text":"We're glad you're here. This documentation is targeted a developers who wish to contribute directly to the DynamiteNSM modules. Below you can find documentation to get you started down a development track. Before getting started please checkout our contributing guidelines here . img { transition:transform 0.25s ease; position: relative; } img:hover { -webkit-transform:scale(1.5); /* or some other value */ transform:scale(2); z-index: 9000; } Project Preview Description Link Build a Commandline Utility Create a new service and ship it with minimal code. Guide \u00bb Extend Configuration Manager Extend a service configuration wrapper to handle additional configuration files and options. Coming Soon Create a Kibana Package Create and group Kibana visualizations, dashboards, searches, and more into a self-contained analysis package. Guide \u00bb Explore the SDK Documentation Curious what else you can do? check out the SDK documentation. Docs \u00bb","title":"Welcome to the Software Developer Documentation."},{"location":"guides/developers/02_build_a_commandline_utility/","text":"Build a Commandline Utility DynamiteNSM services SDK provides programmatic access to Zeek, Suricata, Filebeat, Elasticsearch, Logstash, Kibana, and several other utilities. DynamiteNSM's cmd SDK provides the ability to dynamically generate commandline interfaces for service classes. Service Architecture Services have three primary design principles: Must provide a consistent layer of abstraction above a set of common actions . In doing so we maintain least-surprise in keeping the development experience relatively consistent across all services. Must be self-contained. Each service is a dedicated module with individual action sub-modules. Must expose classes that fit into one of the following paradigms: single-responsibility or multi-responsibility. Single Responsibility Class cmd.service_interfaces.SingleResponsibilityInterface \ud83d\udd17 A class that when instantiated has one primary method that will be called to make things happen. This pattern works well with many service InstallManager . Example Class class InstallManager: def __init__(self, install_directory: str): \"\"\" Prepare install manager, perhaps profiling the system and determining constraints \"\"\" def setup(self, cpus: int = 4): \"\"\" Perform the actual operations of installing service dependencies, and performing some initial configuration. \"\"\" # Invoking the class installer = InstallManager('/opt/dynamite/stuff') installer.setup() Multiple Responsibility Class cmd.service_interfaces.MultipleResponsibilityInterface \ud83d\udd17 A class that when instantiated has multiple methods that can be invoked to perform various actions. The ProcessManager class commonly falls into this pattern. Example Class class ProcessManager: def __init__(self, stdout: bool = True): \"\"\" Load up process information \"\"\" def start(self): \"\"\" start the loaded process \"\"\" def stop(self): \"\"\" stop the loaded process \"\"\" def restart(self): \"\"\" restart the loaded process \"\"\" # Invoking the class proc_man = ProcessManager() proc_man.start() proc_man.stop() proc_man.restart() Building a Simple Service We will use the updates service to demonstrate how to quickly prototype a dynamite cmd utility. The updates service has one primary task. To grab the latest configurations and mirrors for external components. The updates Service The updates service has one action module install.py . This class does nothing too complicated, and provides a single method upon which an interface can be derived. class InstallManager(install.BaseInstallManager): def __init__(self, stdout: Optional[bool] = False, verbose: Optional[bool] = False): super().__init__('updates.install', verbose, stdout) self.stdout = stdout self.verbose = verbose @staticmethod def update_default_configurations(): \"\"\" Retrieves the latest skeleton configurations for setting up ElasticSearch, LogStash, Kibana, Zeek, Suricata, and Filebeat \"\"\" ... @staticmethod def update_mirrors(): \"\"\" Retrieves the latest mirrors which contain the download locations for all components \"\"\" ... def setup(self): \"\"\" Download updates and setup them up. \"\"\" self.logger.info( 'Attempting to download the latest mirrors and default configurations for installable components.') self.update_mirrors() self.update_default_configurations() self.logger.info( 'Updates have been applied. The next time you install: elasticsearch, logstash, kibana, zeek, suricata, ' 'or filebeat these updates will be applied to that component.') Converting to a Commandline Utility You should now be able to expose a commandline interface by wrapping the class created above inside a SingleResponsibilityInterface with the following code. from dynamite_nsm.services.updates import install from dynamite_nsm.cmd.service_interfaces import SingleResponsibilityInterface interface = \\ SingleResponsibilityInterface(cls=install.InstallManager, interface_name='Update DynamiteNSM Default Configs', interface_description='Update mirrors and default configurations', entry_method_name='setup', defaults=dict(stdout=True) ) if __name__ == '__main__': # Get an instance of argparse.ArgumentParser parser = interface.get_parser() # Parse out the arguments that have been passed in args = parser.parse_args() # Call the execute method against the parsed args interface.execute(args) Calling this utility from the commandline will result in the following: Update DynamiteNSM Default Configs - Update mirrors and default configurations optional arguments: -h, --help show this help message and exit --stdout Print output to console --verbose Include detailed debug messages","title":"Create a Commandline Utility"},{"location":"guides/developers/02_build_a_commandline_utility/#build-a-commandline-utility","text":"DynamiteNSM services SDK provides programmatic access to Zeek, Suricata, Filebeat, Elasticsearch, Logstash, Kibana, and several other utilities. DynamiteNSM's cmd SDK provides the ability to dynamically generate commandline interfaces for service classes.","title":"Build a Commandline Utility"},{"location":"guides/developers/02_build_a_commandline_utility/#service-architecture","text":"Services have three primary design principles: Must provide a consistent layer of abstraction above a set of common actions . In doing so we maintain least-surprise in keeping the development experience relatively consistent across all services. Must be self-contained. Each service is a dedicated module with individual action sub-modules. Must expose classes that fit into one of the following paradigms: single-responsibility or multi-responsibility.","title":"Service Architecture"},{"location":"guides/developers/02_build_a_commandline_utility/#single-responsibility-class","text":"cmd.service_interfaces.SingleResponsibilityInterface \ud83d\udd17 A class that when instantiated has one primary method that will be called to make things happen. This pattern works well with many service InstallManager .","title":"Single Responsibility Class"},{"location":"guides/developers/02_build_a_commandline_utility/#example-class","text":"class InstallManager: def __init__(self, install_directory: str): \"\"\" Prepare install manager, perhaps profiling the system and determining constraints \"\"\" def setup(self, cpus: int = 4): \"\"\" Perform the actual operations of installing service dependencies, and performing some initial configuration. \"\"\" # Invoking the class installer = InstallManager('/opt/dynamite/stuff') installer.setup()","title":"Example Class"},{"location":"guides/developers/02_build_a_commandline_utility/#multiple-responsibility-class","text":"cmd.service_interfaces.MultipleResponsibilityInterface \ud83d\udd17 A class that when instantiated has multiple methods that can be invoked to perform various actions. The ProcessManager class commonly falls into this pattern.","title":"Multiple Responsibility Class"},{"location":"guides/developers/02_build_a_commandline_utility/#example-class_1","text":"class ProcessManager: def __init__(self, stdout: bool = True): \"\"\" Load up process information \"\"\" def start(self): \"\"\" start the loaded process \"\"\" def stop(self): \"\"\" stop the loaded process \"\"\" def restart(self): \"\"\" restart the loaded process \"\"\" # Invoking the class proc_man = ProcessManager() proc_man.start() proc_man.stop() proc_man.restart()","title":"Example Class"},{"location":"guides/developers/02_build_a_commandline_utility/#building-a-simple-service","text":"We will use the updates service to demonstrate how to quickly prototype a dynamite cmd utility. The updates service has one primary task. To grab the latest configurations and mirrors for external components.","title":"Building a Simple Service"},{"location":"guides/developers/02_build_a_commandline_utility/#the-updates-service","text":"The updates service has one action module install.py . This class does nothing too complicated, and provides a single method upon which an interface can be derived. class InstallManager(install.BaseInstallManager): def __init__(self, stdout: Optional[bool] = False, verbose: Optional[bool] = False): super().__init__('updates.install', verbose, stdout) self.stdout = stdout self.verbose = verbose @staticmethod def update_default_configurations(): \"\"\" Retrieves the latest skeleton configurations for setting up ElasticSearch, LogStash, Kibana, Zeek, Suricata, and Filebeat \"\"\" ... @staticmethod def update_mirrors(): \"\"\" Retrieves the latest mirrors which contain the download locations for all components \"\"\" ... def setup(self): \"\"\" Download updates and setup them up. \"\"\" self.logger.info( 'Attempting to download the latest mirrors and default configurations for installable components.') self.update_mirrors() self.update_default_configurations() self.logger.info( 'Updates have been applied. The next time you install: elasticsearch, logstash, kibana, zeek, suricata, ' 'or filebeat these updates will be applied to that component.')","title":"The updates Service"},{"location":"guides/developers/02_build_a_commandline_utility/#converting-to-a-commandline-utility","text":"You should now be able to expose a commandline interface by wrapping the class created above inside a SingleResponsibilityInterface with the following code. from dynamite_nsm.services.updates import install from dynamite_nsm.cmd.service_interfaces import SingleResponsibilityInterface interface = \\ SingleResponsibilityInterface(cls=install.InstallManager, interface_name='Update DynamiteNSM Default Configs', interface_description='Update mirrors and default configurations', entry_method_name='setup', defaults=dict(stdout=True) ) if __name__ == '__main__': # Get an instance of argparse.ArgumentParser parser = interface.get_parser() # Parse out the arguments that have been passed in args = parser.parse_args() # Call the execute method against the parsed args interface.execute(args) Calling this utility from the commandline will result in the following: Update DynamiteNSM Default Configs - Update mirrors and default configurations optional arguments: -h, --help show this help message and exit --stdout Print output to console --verbose Include detailed debug messages","title":"Converting to a Commandline Utility"},{"location":"guides/developers/02_create_a_kibana_package/","text":"Create a Kibana Package DynamiteNSM ships with a simple package manager for installing and uninstalling groups Kibana objects. Packages typically contain searches, visualizations and dashboards combined to facilitate one or more investigatory workflows. By default, DynamiteNSM will install the dynamite-investigator package, which provides a unique blend of host centric and event/alert centric views. \u24d8 dynamite kibana package is still in the early stages of development, and thus likely to change in future releases. Checkout Existing Packages git clone https://github.com/DynamiteAI/kibana_packages.git Package Format Guidelines We've developed a few internal guidelines that must be followed for those wishing to submit their own package to the Dynamite package repository. They are available here . Setting up a Working Environment Before you can create a new Kibana package you will need to setup a working monitor and agent instance . Once the agent starts sending events Kibana's discovery view will quickly fill up, and you can begin creating new visualisations and dashboards. The dynamite-investigator package provides some out-of-the-box saved searches, useful for filtering and differentiating between event types. These searches can serve as a basis for creating your own visualizations and dashboards. Creating a Visualization Kibana provides a fairly exhaustive set of visualizations for representing both simple and complex relationships in your data. You can create a new visualization by double-clicking the Vizualize tab in the left-hand sidebar. From there simply select the Create visualization button to enter into the New Vizualization interface. Adding a Visualization to a Dashboard Dashboards serve as space to present a variety of visualizations that typically share some common theme. Kibana dashboards provide the ability to enforce certain global constraints against all visualizations within that dashboard. For example, the time-range filter and any term filters or KQL searches can be applied consistently across all visualizations within a dashboard. To create a new Dashboard double-click the Dashboard tab in left-hand sidebar. You may then add any visualization or saved_search you have created. Exporting Saved Objects To export saved objects simply navigate to Stack Management in the left-hand sidebar. From there select Saved Objects link. Within this UI you can export all the objects or just those of a certain type. We suggest that objects are exported for each type and without including related objects. By doing so other developers can easily build upon the parts of your package most useful to them. Creating the Package Every kibana package consists of one or more saved_object.ndjson files and a manifest.json file. The .ndjson files are the output of a Kibana export operation as outlined above. A manifest.json simply contains some additional metadata as well as a list of files to be installed via Kibana's saved_object's API. manifest.json { \"name\": \"Baselines\", \"author\": \"John Doe\", \"author_email\": \"jdoe@example.com\", \"description\": \"Includes several base-lining techniques useful for identifying anomalies on small networks\", \"package_type\": \"saved_objects\", \"file_list\": [\"config.ndjson\", \"index_patterns.ndjson\", \"searches.ndjson\", \"visualizations.ndjson\", \"dashboards.ndjson\"] } Important! : The order of appearance in the file_list is important, dependencies should precede their dependants. In the example above searches.ndjson relies on or references the data from index_patterns.ndjson to be available at installation time, otherwise errors and unexpected behavior may arise. Create an Archive tar -cvf baselines.tar.gz baselines/*","title":"Create a Kibana Package"},{"location":"guides/developers/02_create_a_kibana_package/#create-a-kibana-package","text":"DynamiteNSM ships with a simple package manager for installing and uninstalling groups Kibana objects. Packages typically contain searches, visualizations and dashboards combined to facilitate one or more investigatory workflows. By default, DynamiteNSM will install the dynamite-investigator package, which provides a unique blend of host centric and event/alert centric views. \u24d8 dynamite kibana package is still in the early stages of development, and thus likely to change in future releases.","title":"Create a Kibana Package"},{"location":"guides/developers/02_create_a_kibana_package/#checkout-existing-packages","text":"git clone https://github.com/DynamiteAI/kibana_packages.git","title":"Checkout Existing Packages"},{"location":"guides/developers/02_create_a_kibana_package/#package-format-guidelines","text":"We've developed a few internal guidelines that must be followed for those wishing to submit their own package to the Dynamite package repository. They are available here .","title":"Package Format Guidelines"},{"location":"guides/developers/02_create_a_kibana_package/#setting-up-a-working-environment","text":"Before you can create a new Kibana package you will need to setup a working monitor and agent instance . Once the agent starts sending events Kibana's discovery view will quickly fill up, and you can begin creating new visualisations and dashboards. The dynamite-investigator package provides some out-of-the-box saved searches, useful for filtering and differentiating between event types. These searches can serve as a basis for creating your own visualizations and dashboards.","title":"Setting up a Working Environment"},{"location":"guides/developers/02_create_a_kibana_package/#creating-a-visualization","text":"Kibana provides a fairly exhaustive set of visualizations for representing both simple and complex relationships in your data. You can create a new visualization by double-clicking the Vizualize tab in the left-hand sidebar. From there simply select the Create visualization button to enter into the New Vizualization interface.","title":"Creating a Visualization"},{"location":"guides/developers/02_create_a_kibana_package/#adding-a-visualization-to-a-dashboard","text":"Dashboards serve as space to present a variety of visualizations that typically share some common theme. Kibana dashboards provide the ability to enforce certain global constraints against all visualizations within that dashboard. For example, the time-range filter and any term filters or KQL searches can be applied consistently across all visualizations within a dashboard. To create a new Dashboard double-click the Dashboard tab in left-hand sidebar. You may then add any visualization or saved_search you have created.","title":"Adding a Visualization to a Dashboard"},{"location":"guides/developers/02_create_a_kibana_package/#exporting-saved-objects","text":"To export saved objects simply navigate to Stack Management in the left-hand sidebar. From there select Saved Objects link. Within this UI you can export all the objects or just those of a certain type. We suggest that objects are exported for each type and without including related objects. By doing so other developers can easily build upon the parts of your package most useful to them.","title":"Exporting Saved Objects"},{"location":"guides/developers/02_create_a_kibana_package/#creating-the-package","text":"Every kibana package consists of one or more saved_object.ndjson files and a manifest.json file. The .ndjson files are the output of a Kibana export operation as outlined above. A manifest.json simply contains some additional metadata as well as a list of files to be installed via Kibana's saved_object's API.","title":"Creating the Package"},{"location":"guides/developers/02_create_a_kibana_package/#manifestjson","text":"{ \"name\": \"Baselines\", \"author\": \"John Doe\", \"author_email\": \"jdoe@example.com\", \"description\": \"Includes several base-lining techniques useful for identifying anomalies on small networks\", \"package_type\": \"saved_objects\", \"file_list\": [\"config.ndjson\", \"index_patterns.ndjson\", \"searches.ndjson\", \"visualizations.ndjson\", \"dashboards.ndjson\"] } Important! : The order of appearance in the file_list is important, dependencies should precede their dependants. In the example above searches.ndjson relies on or references the data from index_patterns.ndjson to be available at installation time, otherwise errors and unexpected behavior may arise.","title":"manifest.json"},{"location":"guides/developers/02_create_a_kibana_package/#create-an-archive","text":"tar -cvf baselines.tar.gz baselines/*","title":"Create an Archive"},{"location":"guides/developers/SDK/01_overview/","text":"SDK Overview Highlevel Design Concepts dynamite-nsm provides several entry points for developers to build their own utilities or automate a deployment process. The dynamite-nsm package is divided into two major modules: services and cmd . The services module provides a common set of wrappers around various utilities. The cmd module provides a set of functions and classes for converting services into fully functioning commandline utilities. Services Module ( services ) The services module is essentially a collection of submodules for managing the installation , configuration , process management , and monitoring of utilities currently supported within our stack. All services inherit from interfaces found within the service.base submodule. One of our driving design principles for this module was to use similar patterns of abstraction across all supported services. For example, the underlying mechanics of enabling a Suricata Rule-set verses a Zeek script are essentially identical, allowing us to present very similar configuration managers for each. [+] \u251c\u2500 dynamite_nsm/ [-] \u251c\u2500 cmd/ [+] \u251c\u2500 services/ [+] \u251c\u2500 zeek/ \u251c\u2500 install.py \u251c\u2500 logs.py \u251c\u2500 uninstall.py \u251c\u2500 config.py \u251c\u2500 process.py \u251c\u2500 profile.py [-] \u251c\u2500 suricata/ [-] \u251c\u2500 filebeat/ [-] \u251c\u2500 elasticsearch/ [-] \u251c\u2500 logstash/ [-] \u251c\u2500 kibana/ [-] \u251c\u2500 base/ \u251c\u2500 const.py \u251c\u2500 exceptions.py \u251c\u2500 logger.py \u251c\u2500 package_manager.py \u251c\u2500 utilities.py Submodule Module Description Corresponding Base Classes install An interface to manage the installation of a service. BaseInstallManager uninstall An interface to manage the uninstallation of a service. BaseUninstallManager config An interface for interacting with various configurations available to the service. GenericConfigManager YamlConfigManager logs An interface for searching through various logs. LogFile process An interface for managing process state ( start stop status restart ) BaseProcessManager profile An interface the provides a set of checks against a service to ensure that it is installed and configured properly. BaseProcessProfiler Commandline Builder Module ( cmd ) The cmd module comes with a set of functions for converting service.config , service.install , service.process , and service.logs classes into commandline utilities that are invokable under the /usr/local/bin/dynamite utility. \u24d8 If you are interested in building your own service and commandline utility check out this guide . [+] \u251c\u2500 dynamite_nsm/ [+] \u251c\u2500 cmd/ [-] \u251c\u2500 zeek/ [-] \u251c\u2500 config/ [+] \u251c\u2500 install/ \u251c\u2500 __main__.py \u251c\u2500 __init__.py [-] \u251c\u2500 logs/ [-] \u251c\u2500 process/ [-] \u251c\u2500 uninstall/ [-] \u251c\u2500 suricata/ [-] \u251c\u2500 filebeat/ [-] \u251c\u2500 elasticsearch/ [-] \u251c\u2500 kibana/ [-] \u251c\u2500 logstash/ [-] \u251c\u2500 updates/ \u251c\u2500 base_interface.py \u251c\u2500 config_object_interfaces.py \u251c\u2500 service_interfaces.py \u251c\u2500 inspection_helpers.py \u251c\u2500 interface_operations.py CMD SDK Module Description base_interface Abstract base interface implemented by all interface modules service_interfaces Provides commandline wrappers for many service.base action classes. config_object_interfaces Provides commandline wrappers for complex config objects. inspection_helpers Provides a set of utility functions and classes for building commandline parsers dynamically. interface_operations Provides a set of utility functions for combining commandline interfaces.","title":"Overview"},{"location":"guides/developers/SDK/01_overview/#sdk-overview","text":"","title":"SDK Overview"},{"location":"guides/developers/SDK/01_overview/#highlevel-design-concepts","text":"dynamite-nsm provides several entry points for developers to build their own utilities or automate a deployment process. The dynamite-nsm package is divided into two major modules: services and cmd . The services module provides a common set of wrappers around various utilities. The cmd module provides a set of functions and classes for converting services into fully functioning commandline utilities.","title":"Highlevel Design Concepts"},{"location":"guides/developers/SDK/01_overview/#services-module-services","text":"The services module is essentially a collection of submodules for managing the installation , configuration , process management , and monitoring of utilities currently supported within our stack. All services inherit from interfaces found within the service.base submodule. One of our driving design principles for this module was to use similar patterns of abstraction across all supported services. For example, the underlying mechanics of enabling a Suricata Rule-set verses a Zeek script are essentially identical, allowing us to present very similar configuration managers for each. [+] \u251c\u2500 dynamite_nsm/ [-] \u251c\u2500 cmd/ [+] \u251c\u2500 services/ [+] \u251c\u2500 zeek/ \u251c\u2500 install.py \u251c\u2500 logs.py \u251c\u2500 uninstall.py \u251c\u2500 config.py \u251c\u2500 process.py \u251c\u2500 profile.py [-] \u251c\u2500 suricata/ [-] \u251c\u2500 filebeat/ [-] \u251c\u2500 elasticsearch/ [-] \u251c\u2500 logstash/ [-] \u251c\u2500 kibana/ [-] \u251c\u2500 base/ \u251c\u2500 const.py \u251c\u2500 exceptions.py \u251c\u2500 logger.py \u251c\u2500 package_manager.py \u251c\u2500 utilities.py Submodule Module Description Corresponding Base Classes install An interface to manage the installation of a service. BaseInstallManager uninstall An interface to manage the uninstallation of a service. BaseUninstallManager config An interface for interacting with various configurations available to the service. GenericConfigManager YamlConfigManager logs An interface for searching through various logs. LogFile process An interface for managing process state ( start stop status restart ) BaseProcessManager profile An interface the provides a set of checks against a service to ensure that it is installed and configured properly. BaseProcessProfiler","title":"Services Module (services)"},{"location":"guides/developers/SDK/01_overview/#commandline-builder-module-cmd","text":"The cmd module comes with a set of functions for converting service.config , service.install , service.process , and service.logs classes into commandline utilities that are invokable under the /usr/local/bin/dynamite utility. \u24d8 If you are interested in building your own service and commandline utility check out this guide . [+] \u251c\u2500 dynamite_nsm/ [+] \u251c\u2500 cmd/ [-] \u251c\u2500 zeek/ [-] \u251c\u2500 config/ [+] \u251c\u2500 install/ \u251c\u2500 __main__.py \u251c\u2500 __init__.py [-] \u251c\u2500 logs/ [-] \u251c\u2500 process/ [-] \u251c\u2500 uninstall/ [-] \u251c\u2500 suricata/ [-] \u251c\u2500 filebeat/ [-] \u251c\u2500 elasticsearch/ [-] \u251c\u2500 kibana/ [-] \u251c\u2500 logstash/ [-] \u251c\u2500 updates/ \u251c\u2500 base_interface.py \u251c\u2500 config_object_interfaces.py \u251c\u2500 service_interfaces.py \u251c\u2500 inspection_helpers.py \u251c\u2500 interface_operations.py CMD SDK Module Description base_interface Abstract base interface implemented by all interface modules service_interfaces Provides commandline wrappers for many service.base action classes. config_object_interfaces Provides commandline wrappers for complex config objects. inspection_helpers Provides a set of utility functions and classes for building commandline parsers dynamically. interface_operations Provides a set of utility functions for combining commandline interfaces.","title":"Commandline Builder Module (cmd)"},{"location":"guides/developers/SDK/logger.py/","text":"A simple logging interface that can be imported by various services. To import... from dynamite_nsm import logger get_logger ( component_name , level = 20 , stdout = True , stdout_only = False ) Get a pre-configured logging instance Parameters: Name Type Description Default component_name The name of the service doing the logging. required level The minimum logging level 20 stdout If True, prints to console True stdout_only If True, we only print to the console (overrides stdout) False Returns: A logger instance Source code in dynamite_nsm/logger.py def get_logger ( component_name , level = logging . INFO , stdout = True , stdout_only = False ) -> logging . Logger : \"\"\"Get a pre-configured logging instance Args: component_name: The name of the service doing the logging. level: The minimum logging level stdout: If True, prints to console stdout_only: If True, we only print to the console (overrides stdout) Returns: A logger instance \"\"\" coloredlogs . DEFAULT_FIELD_STYLES = { 'asctime' : { 'color' : 'green' }, 'hostname' : { 'color' : 'magenta' }, 'levelname' : { 'bold' : True , 'color' : 'black' }, 'name' : { 'color' : 'cyan' , 'bold' : True }, 'programname' : { 'color' : 'blue' }, 'username' : { 'color' : 'yellow' }} logger = logging . getLogger ( component_name . upper ()) logger . setLevel ( level ) if not len ( logger . handlers ): if not stdout_only : log_out_path = os . path . join ( const . LOG_PATH , 'dynamite- {} .log' . format ( TODAY_FORMATTED_DATE )) fh = logging . FileHandler ( log_out_path ) fformatter = logging . Formatter ( ' %(asctime)s | %(name)20s | %(module)20s | %(funcName)45s | %(lineno)4s | %(levelname)8s | %(message)s ' ) fh . setFormatter ( fformatter ) logger . addHandler ( fh ) if utilities . is_root (): utilities . set_ownership_of_file ( log_out_path ) utilities . set_permissions_of_file ( log_out_path , 660 ) else : stdout = True if stdout : coloredlogs . install ( level = level , logger = logger , fmt = ' %(asctime)s %(name)-25s %(levelname)-10s | %(message)s ' ) logger . propagate = False return logger","title":"logger.py"},{"location":"guides/developers/SDK/logger.py/#dynamite_nsm.logger.get_logger","text":"Get a pre-configured logging instance Parameters: Name Type Description Default component_name The name of the service doing the logging. required level The minimum logging level 20 stdout If True, prints to console True stdout_only If True, we only print to the console (overrides stdout) False Returns: A logger instance Source code in dynamite_nsm/logger.py def get_logger ( component_name , level = logging . INFO , stdout = True , stdout_only = False ) -> logging . Logger : \"\"\"Get a pre-configured logging instance Args: component_name: The name of the service doing the logging. level: The minimum logging level stdout: If True, prints to console stdout_only: If True, we only print to the console (overrides stdout) Returns: A logger instance \"\"\" coloredlogs . DEFAULT_FIELD_STYLES = { 'asctime' : { 'color' : 'green' }, 'hostname' : { 'color' : 'magenta' }, 'levelname' : { 'bold' : True , 'color' : 'black' }, 'name' : { 'color' : 'cyan' , 'bold' : True }, 'programname' : { 'color' : 'blue' }, 'username' : { 'color' : 'yellow' }} logger = logging . getLogger ( component_name . upper ()) logger . setLevel ( level ) if not len ( logger . handlers ): if not stdout_only : log_out_path = os . path . join ( const . LOG_PATH , 'dynamite- {} .log' . format ( TODAY_FORMATTED_DATE )) fh = logging . FileHandler ( log_out_path ) fformatter = logging . Formatter ( ' %(asctime)s | %(name)20s | %(module)20s | %(funcName)45s | %(lineno)4s | %(levelname)8s | %(message)s ' ) fh . setFormatter ( fformatter ) logger . addHandler ( fh ) if utilities . is_root (): utilities . set_ownership_of_file ( log_out_path ) utilities . set_permissions_of_file ( log_out_path , 660 ) else : stdout = True if stdout : coloredlogs . install ( level = level , logger = logger , fmt = ' %(asctime)s %(name)-25s %(levelname)-10s | %(message)s ' ) logger . propagate = False return logger","title":"get_logger()"},{"location":"guides/developers/SDK/package_manager/","text":"Install packages via apt-get or yum. To import... from dynamite_nsm import package_manager OSPackageManager Interface for interacting with the operating systems package manager system Currently supports YUM/apt-get __init__ ( self , stdout = True , verbose = False ) special Parameters: Name Type Description Default stdout Optional[bool] Print the output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/package_manager.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False ): \"\"\" Args: stdout: Print the output to console verbose: Include detailed debug messages \"\"\" self . package_manager = self . detect_package_manager ( verbose = verbose ) self . verbose = verbose log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( 'package.manager' , level = log_level , stdout = stdout ) detect_package_manager ( verbose = False ) staticmethod Detect the POSIX package manager currently being used Parameters: Name Type Description Default verbose Optional[bool] Include detailed debug messages False Returns: Type Description str The package manager command (either apt-get or yum) Source code in dynamite_nsm/package_manager.py @staticmethod def detect_package_manager ( verbose : Optional [ bool ] = False ) -> str : \"\"\"Detect the POSIX package manager currently being used Args: verbose: Include detailed debug messages Returns: The package manager command (either apt-get or yum) \"\"\" if verbose : apt_get_p = subprocess . Popen ( 'apt-get -h &> /dev/null' , shell = True ) else : apt_get_p = subprocess . Popen ( 'apt-get -h &> /dev/null' , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) apt_get_p . communicate () if verbose : yum_p = subprocess . Popen ( 'yum -h &> /dev/null' , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : yum_p = subprocess . Popen ( 'yum -h &> /dev/null' , shell = True ) yum_p . communicate () if apt_get_p . returncode == 0 : return 'apt-get' elif yum_p . returncode == 0 : return 'yum' else : raise OsPackageManagerNotDetectedError () install_packages ( self , packages ) Given a set of packages, installs the packages Parameters: Name Type Description Default packages List[str] Name of binary packages to install required Returns: Type Description None None Source code in dynamite_nsm/package_manager.py def install_packages ( self , packages : List [ str ]) -> None : \"\"\"Given a set of packages, installs the packages Args: packages: Name of binary packages to install Returns: None \"\"\" flags = '-y' failed_packages = [] if not self . package_manager : return None for package in packages : self . logger . info ( 'Installing {} ' . format ( package )) if self . verbose : p = subprocess . Popen ( ' {} {} install {} ' . format ( self . package_manager , flags , package ), shell = True ) else : p = subprocess . Popen ( ' {} {} install {} ' . format ( self . package_manager , flags , package ), shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . communicate () if p . returncode not in [ 0 , 100 ]: # Interestingly enough apt-get can return 100s if https isn't forced # https://stackoverflow.com/questions/38002543/apt-get-update-returned-a-non-zero-code-100 self . logger . warning ( ' {} failed to install.' . format ( package )) failed_packages . append ( package ) if failed_packages : self . logger . warning ( f 'One or more packages failed to install you may need to install the following packages ' f 'manually: { failed_packages } .' ) refresh_package_indexes ( self ) Refresh the package cache Returns: Type Description None None Source code in dynamite_nsm/package_manager.py def refresh_package_indexes ( self ) -> None : \"\"\"Refresh the package cache Args: Returns: None \"\"\" params = None if self . package_manager == 'apt-get' : params = 'update' elif self . package_manager == 'yum' : params = 'check-update' if not self . package_manager : return if self . verbose : p = subprocess . Popen ( f ' { self . package_manager } { params } &> /dev/null' , shell = True ) else : p = subprocess . Popen ( f ' { self . package_manager } { params } &> /dev/null' , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . communicate () if p . returncode not in [ 0 , 100 ]: self . logger . warning ( f 'Could not refresh package index via { self . package_manager } ' )","title":"package_manager.py"},{"location":"guides/developers/SDK/package_manager/#dynamite_nsm.package_manager.OSPackageManager","text":"Interface for interacting with the operating systems package manager system Currently supports YUM/apt-get","title":"OSPackageManager"},{"location":"guides/developers/SDK/package_manager/#dynamite_nsm.package_manager.OSPackageManager.__init__","text":"Parameters: Name Type Description Default stdout Optional[bool] Print the output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/package_manager.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False ): \"\"\" Args: stdout: Print the output to console verbose: Include detailed debug messages \"\"\" self . package_manager = self . detect_package_manager ( verbose = verbose ) self . verbose = verbose log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( 'package.manager' , level = log_level , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/package_manager/#dynamite_nsm.package_manager.OSPackageManager.detect_package_manager","text":"Detect the POSIX package manager currently being used Parameters: Name Type Description Default verbose Optional[bool] Include detailed debug messages False Returns: Type Description str The package manager command (either apt-get or yum) Source code in dynamite_nsm/package_manager.py @staticmethod def detect_package_manager ( verbose : Optional [ bool ] = False ) -> str : \"\"\"Detect the POSIX package manager currently being used Args: verbose: Include detailed debug messages Returns: The package manager command (either apt-get or yum) \"\"\" if verbose : apt_get_p = subprocess . Popen ( 'apt-get -h &> /dev/null' , shell = True ) else : apt_get_p = subprocess . Popen ( 'apt-get -h &> /dev/null' , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) apt_get_p . communicate () if verbose : yum_p = subprocess . Popen ( 'yum -h &> /dev/null' , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : yum_p = subprocess . Popen ( 'yum -h &> /dev/null' , shell = True ) yum_p . communicate () if apt_get_p . returncode == 0 : return 'apt-get' elif yum_p . returncode == 0 : return 'yum' else : raise OsPackageManagerNotDetectedError ()","title":"detect_package_manager()"},{"location":"guides/developers/SDK/package_manager/#dynamite_nsm.package_manager.OSPackageManager.install_packages","text":"Given a set of packages, installs the packages Parameters: Name Type Description Default packages List[str] Name of binary packages to install required Returns: Type Description None None Source code in dynamite_nsm/package_manager.py def install_packages ( self , packages : List [ str ]) -> None : \"\"\"Given a set of packages, installs the packages Args: packages: Name of binary packages to install Returns: None \"\"\" flags = '-y' failed_packages = [] if not self . package_manager : return None for package in packages : self . logger . info ( 'Installing {} ' . format ( package )) if self . verbose : p = subprocess . Popen ( ' {} {} install {} ' . format ( self . package_manager , flags , package ), shell = True ) else : p = subprocess . Popen ( ' {} {} install {} ' . format ( self . package_manager , flags , package ), shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . communicate () if p . returncode not in [ 0 , 100 ]: # Interestingly enough apt-get can return 100s if https isn't forced # https://stackoverflow.com/questions/38002543/apt-get-update-returned-a-non-zero-code-100 self . logger . warning ( ' {} failed to install.' . format ( package )) failed_packages . append ( package ) if failed_packages : self . logger . warning ( f 'One or more packages failed to install you may need to install the following packages ' f 'manually: { failed_packages } .' )","title":"install_packages()"},{"location":"guides/developers/SDK/package_manager/#dynamite_nsm.package_manager.OSPackageManager.refresh_package_indexes","text":"Refresh the package cache Returns: Type Description None None Source code in dynamite_nsm/package_manager.py def refresh_package_indexes ( self ) -> None : \"\"\"Refresh the package cache Args: Returns: None \"\"\" params = None if self . package_manager == 'apt-get' : params = 'update' elif self . package_manager == 'yum' : params = 'check-update' if not self . package_manager : return if self . verbose : p = subprocess . Popen ( f ' { self . package_manager } { params } &> /dev/null' , shell = True ) else : p = subprocess . Popen ( f ' { self . package_manager } { params } &> /dev/null' , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . communicate () if p . returncode not in [ 0 , 100 ]: self . logger . warning ( f 'Could not refresh package index via { self . package_manager } ' )","title":"refresh_package_indexes()"},{"location":"guides/developers/SDK/utilities/","text":"General purpose utility methods used by a variety of services. To import... from dynamite_nsm import utilities backup_configuration_file ( source_file , configuration_backup_directory , destination_file_prefix ) Backup a configuration file Parameters: Name Type Description Default source_file str The configuration file you wish to backup required configuration_backup_directory str The destination configuration directory required destination_file_prefix str The prefix of the file; timestamp is automatically appended in filename required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def backup_configuration_file ( source_file : str , configuration_backup_directory : str , destination_file_prefix : str ) -> None : \"\"\"Backup a configuration file Args: source_file: The configuration file you wish to backup configuration_backup_directory: The destination configuration directory destination_file_prefix: The prefix of the file; timestamp is automatically appended in filename Returns: None \"\"\" timestamp = int ( time . time ()) destination_backup_config_file = os . path . join ( configuration_backup_directory , ' {} . {} ' . format ( destination_file_prefix , timestamp )) try : makedirs ( configuration_backup_directory , exist_ok = True ) set_ownership_of_file ( configuration_backup_directory ) except Exception as e : raise exceptions . WriteConfigError ( \"General error while attempting to create backup directory at {} ; {} \" . format ( configuration_backup_directory , e )) try : shutil . copy ( source_file , destination_backup_config_file ) except Exception as e : raise exceptions . WriteConfigError ( \"General error while attempting to copy {} to {} \" . format ( source_file , destination_backup_config_file , e )) check_pid ( pid ) :return: True, if the process is running Returns: Type Description bool None Source code in dynamite_nsm/utilities.py def check_pid ( pid : int ) -> bool : \"\"\":return: True, if the process is running Args: The process id Returns: None \"\"\" if not pid : return False if pid == - 1 : return False try : os . kill ( pid , 0 ) except OSError : return False else : return True check_socket ( host , port ) Check if a host is listening on a given port Parameters: Name Type Description Default host str The host the service is listening on required port int The port the service is listening on required Returns: Type Description bool True, if a service is listening on a given HOST:PORT Source code in dynamite_nsm/utilities.py def check_socket ( host : str , port : int ) -> bool : \"\"\"Check if a host is listening on a given port Args: host: The host the service is listening on port: The port the service is listening on Returns: True, if a service is listening on a given HOST:PORT \"\"\" if isinstance ( port , str ): port = int ( port ) with closing ( socket . socket ( socket . AF_INET , socket . SOCK_STREAM )) as sock : if sock . connect_ex (( host , port )) == 0 : return True else : return False check_user_exists ( username ) Check of a UNIX user exists Parameters: Name Type Description Default username str The username of the user to check required Returns: Type Description bool : True if the user exists Source code in dynamite_nsm/utilities.py def check_user_exists ( username : str ) -> bool : \"\"\"Check of a UNIX user exists Args: username: The username of the user to check Returns: : True if the user exists \"\"\" try : pwd . getpwnam ( username ) return True except KeyError : return False copytree ( src , dst , symlinks = False , ignore = None ) Copy a src file or directory to a destination file or directory Parameters: Name Type Description Default src str The source directory required dst str The destination directory required symlinks Optional[bool] If True, symlinks will be followed (Default value = False) False ignore Optional[bool] If True, errors will be ignored None Returns: Type Description None None Source code in dynamite_nsm/utilities.py def copytree ( src : str , dst : str , symlinks : Optional [ bool ] = False , ignore : Optional [ bool ] = None ) -> None : \"\"\"Copy a src file or directory to a destination file or directory Args: src: The source directory dst: The destination directory symlinks: If True, symlinks will be followed (Default value = False) ignore: If True, errors will be ignored Returns: None \"\"\" for item in os . listdir ( src ): s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if os . path . isdir ( s ): try : shutil . copytree ( s , d , symlinks , ignore ) except Exception : # File exists or handle locked pass else : shutil . copy2 ( s , d ) create_dynamite_environment_file () Creates the dynamite environment file accessible only to the root user. Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_dynamite_environment_file () -> None : \"\"\"Creates the dynamite environment file accessible only to the root user. Args: Returns: None \"\"\" makedirs ( const . CONFIG_PATH , exist_ok = True ) env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) env_file_f = open ( env_file , 'a' ) env_file_f . write ( '' ) env_file_f . close () set_ownership_of_file ( env_file , user = 'dynamite' , group = 'dynamite' ) set_permissions_of_file ( env_file , 770 ) create_dynamite_remote_user () Create the dynamite-remote user and group Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_dynamite_remote_user () -> None : \"\"\"Create the dynamite-remote user and group Returns: None \"\"\" password = salt = str ( random . randint ( 10 , 99 )) pass_encry = crypt . crypt ( password , salt ) subprocess . call ( 'useradd -r -p \" {} \" -s /bin/bash dynamite-remote' . format ( pass_encry ), shell = True ) create_dynamite_user () Create the dynamite user and group Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_dynamite_user () -> None : \"\"\"Create the dynamite user and group Returns: None \"\"\" password = salt = str ( random . randint ( 10 , 99 )) pass_encry = crypt . crypt ( password , salt ) subprocess . call ( 'useradd -r -p \" {} \" -s /bin/bash dynamite' . format ( pass_encry ), shell = True ) create_jupyter_user ( password ) Create the jupyter user w/ home Parameters: Name Type Description Default password str The password for the user required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_jupyter_user ( password : str ) -> None : \"\"\"Create the jupyter user w/ home Args: password: The password for the user Returns: None \"\"\" pass_encry = crypt . crypt ( password , str ( random . randint ( 10 , 99 ))) subprocess . call ( 'useradd -r -m -p \" {} \" -s /bin/bash jupyter' . format ( pass_encry ), shell = True ) delete_dynamite_remote_user () Remove the dynamite-remote user Returns: Type Description None None Source code in dynamite_nsm/utilities.py def delete_dynamite_remote_user () -> None : \"\"\" Remove the dynamite-remote user Returns: None \"\"\" subprocess . run ([ 'userdel' , 'dynamite-remote' ]) download_file ( url , filename , stdout = False ) Given a URL and destination file name, download the file to local install_cache Parameters: Name Type Description Default url str The url to the file to download required filename str The name of the file to write to disk required stdout Optional[bool] Print the output to the console False Returns: True, if successfully downloaded. Source code in dynamite_nsm/utilities.py def download_file ( url : str , filename : str , stdout : Optional [ bool ] = False ) -> bool : \"\"\" Given a URL and destination file name, download the file to local install_cache Args: url: The url to the file to download filename: The name of the file to write to disk stdout: Print the output to the console Returns: True, if successfully downloaded. \"\"\" CHUNK = 16 * 1024 makedirs ( const . INSTALL_CACHE , exist_ok = True ) response = urlopen ( url ) try : response_size_bytes = int ( response . headers [ 'Content-Length' ]) except ( KeyError , TypeError , ValueError ): response_size_bytes = None widgets = [ ' \\033 [92m' , ' {} ' . format ( datetime . strftime ( datetime . utcnow (), '%Y-%m- %d %H:%M:%S' )), ' \\033 [0m' , ' \\033 [0;36m' 'DOWNLOAD_MANAGER ' , ' \\033 [0m' , ' | ' , progressbar . FileTransferSpeed (), ' ' , progressbar . Bar (), ' ' , '( {} )' . format ( filename ), ' ' , progressbar . ETA (), ] pb = progressbar . ProgressBar ( widgets = widgets , maxval = int ( response_size_bytes )) if stdout : try : pb . start () except Exception : # Something broke, disable stdout going forward stdout = False try : with open ( os . path . join ( const . INSTALL_CACHE , filename ), 'wb' ) as f : chunk_num = 0 while True : chunk = response . read ( CHUNK ) if not chunk : break f . write ( chunk ) if stdout : try : pb . update ( CHUNK * chunk_num ) except ValueError : pass chunk_num += 1 if stdout : pb . finish () except URLError : return False return True extract_archive ( archive_path , destination_path ) Extract a tar.gz archive to a given destination path. Parameters: Name Type Description Default archive_path str The full path to the tar.gz archive file required destination_path str The path where the archive will be extracted required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def extract_archive ( archive_path : str , destination_path : str ) -> None : \"\"\"Extract a tar.gz archive to a given destination path. Args: archive_path: The full path to the tar.gz archive file destination_path: The path where the archive will be extracted Returns: None \"\"\" try : tf = tarfile . open ( archive_path ) tf . extractall ( path = destination_path ) except IOError : pass generate_random_password ( length = 30 ) Generate a random password containing alphanumeric and symbolic characters Parameters: Name Type Description Default length int The length of the password 30 Returns: Type Description str The string representation of the password Source code in dynamite_nsm/utilities.py def generate_random_password ( length : int = 30 ) -> str : \"\"\"Generate a random password containing alphanumeric and symbolic characters Args: length: The length of the password Returns: The string representation of the password \"\"\" tokens = string . ascii_lowercase + string . ascii_uppercase + '0123456789' + '!@#$%^&*()_+' return '' . join ( random . choice ( tokens ) for i in range ( length )) get_cpu_core_count () Get the number of availble CPU cores Returns: Type Description int The count of CPU cores available on the system Source code in dynamite_nsm/utilities.py def get_cpu_core_count () -> int : \"\"\"Get the number of availble CPU cores Args: Returns: The count of CPU cores available on the system \"\"\" return multiprocessing . cpu_count () get_default_agent_tag () Get the agent tag Returns: Type Description str The agent tag Source code in dynamite_nsm/utilities.py def get_default_agent_tag () -> str : \"\"\"Get the agent tag Args: Returns: The agent tag \"\"\" return '' . join ([ c . lower () for c in str ( socket . gethostname ()) if c . isalnum ()][ 0 : 25 ]) + '_agt' get_default_es_node_name () :return: The node name Returns: Type Description str The node name. Source code in dynamite_nsm/utilities.py def get_default_es_node_name () -> str : \"\"\":return: The node name Args: Returns: The node name. \"\"\" return '' . join ([ c . lower () for c in str ( socket . gethostname ()) if c . isalnum ()][ 0 : 25 ]) + '_es_node' get_environment_file_dict () Get the contents of the dynamite environment file as a dictionary. Returns: Type Description Dict The contents of the /etc/dynamite/environment file as a dictionary Source code in dynamite_nsm/utilities.py def get_environment_file_dict () -> Dict : \"\"\"Get the contents of the dynamite environment file as a dictionary. Args: Returns: The contents of the /etc/dynamite/environment file as a dictionary \"\"\" export_dict = {} try : for line in open ( os . path . join ( const . CONFIG_PATH , 'environment' )) . readlines (): if '=' in line : key , value = line . strip () . split ( '=' ) export_dict [ key ] = value except PermissionError : return {} except FileNotFoundError : return {} return export_dict get_environment_file_str () Get the contents of the dynamite environment file as a string. Returns: Type Description str The contents of the /etc/dynamite/environment file as a giant export string Source code in dynamite_nsm/utilities.py def get_environment_file_str () -> str : \"\"\"Get the contents of the dynamite environment file as a string. Args: Returns: The contents of the /etc/dynamite/environment file as a giant export string \"\"\" export_str = '' with open ( os . path . join ( const . CONFIG_PATH , 'environment' )) as env_f : for line in env_f . readlines (): if '=' in line : key , value = line . strip () . split ( '=' ) export_str += 'export {} = \\' {} \\' && ' . format ( key , value ) return export_str get_epoch_time_seconds () Get the number of seconds since 01/01/1970 Returns: An integer representing the number of seconds between 01/01/1970 and now. Source code in dynamite_nsm/utilities.py def get_epoch_time_seconds () -> int : \"\"\"Get the number of seconds since 01/01/1970 Returns: An integer representing the number of seconds between 01/01/1970 and now. \"\"\" return int ( time . time ()) get_file_md5_hash ( fh ) Given a file-like object return the md5 hash of that object Parameters: Name Type Description Default fh Union[BinaryIO, TextIO] file handle (file like object) required Returns: Type Description str the md5 hash of the file Source code in dynamite_nsm/utilities.py def get_file_md5_hash ( fh : Union [ BinaryIO , TextIO ]) -> str : \"\"\"Given a file-like object return the md5 hash of that object Args: fh: file handle (file like object) Returns: the md5 hash of the file \"\"\" block_size = 65536 md5_hasher = md5 () buf = fh . read ( block_size ) while len ( buf ) > 0 : md5_hasher . update ( buf ) buf = fh . read ( block_size ) return md5_hasher . hexdigest () get_filepath_md5_hash ( file_path ) Given a file-path to return the md5 hash of that file Parameters: Name Type Description Default file_path str path to the file being hashed required Returns: Type Description str the md5 hash of a file Source code in dynamite_nsm/utilities.py def get_filepath_md5_hash ( file_path : str ) -> str : \"\"\"Given a file-path to return the md5 hash of that file Args: file_path: path to the file being hashed Returns: the md5 hash of a file \"\"\" with open ( file_path , 'rb' ) as afile : return get_file_md5_hash ( afile ) get_memory_available_bytes () Get the amount of RAM (in bytes) of the current system Returns: Type Description int The number of bytes available in memory Source code in dynamite_nsm/utilities.py def get_memory_available_bytes () -> int : \"\"\"Get the amount of RAM (in bytes) of the current system Args: Returns: The number of bytes available in memory \"\"\" return os . sysconf ( 'SC_PAGE_SIZE' ) * os . sysconf ( 'SC_PHYS_PAGES' ) get_network_addresses () Returns a list of valid IP addresses for the host Returns: Type Description Tuple A tuple containing an internal, and external IP address Source code in dynamite_nsm/utilities.py def get_network_addresses () -> Tuple : \"\"\"Returns a list of valid IP addresses for the host Args: Returns: A tuple containing an internal, and external IP address \"\"\" valid_addresses = [] internal_address , external_address = None , None try : site = str ( urlopen ( \"http://checkip.dyndns.org/\" , timeout = 2 ) . read ()) grab = re . findall ( r '([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)' , site ) external_address = grab [ 0 ] except ( URLError , IndexError , HTTPError ): pass internal_address = socket . gethostbyname ( socket . gethostname ()) if internal_address : valid_addresses . append ( internal_address ) if external_address : valid_addresses . append ( external_address ) return tuple ( valid_addresses ) get_network_interface_configurations () Returns a list of network interfaces available on the system Returns: Type Description List[Dict] A list of network interfaces Source code in dynamite_nsm/utilities.py def get_network_interface_configurations () -> List [ Dict ]: \"\"\"Returns a list of network interfaces available on the system Args: Returns: A list of network interfaces \"\"\" addresses = psutil . net_if_addrs () stats = psutil . net_if_stats () available_networks = [] for intface , addr_list in addresses . items (): if intface . startswith ( 'lo' ): continue elif intface . startswith ( 'docker' ): continue elif intface . startswith ( 'veth' ): continue elif intface . startswith ( 'br-' ): continue elif intface not in stats : continue name = intface speed = stats [ intface ] . speed duplex = str ( stats [ intface ] . duplex ) mtu = stats [ intface ] . mtu available_networks . append ({ 'name' : name , 'speed' : speed , 'duplex' : duplex , 'mtu' : mtu }) return available_networks get_network_interface_names () Returns a list of network interfaces available on the system Returns: Type Description List[str] A list of network interfaces Source code in dynamite_nsm/utilities.py def get_network_interface_names () -> List [ str ]: \"\"\"Returns a list of network interfaces available on the system Args: Returns: A list of network interfaces \"\"\" addresses = psutil . net_if_addrs () stats = psutil . net_if_stats () available_networks = [] for intface , addr_list in addresses . items (): if intface . startswith ( 'lo' ): continue elif intface . startswith ( 'docker' ): continue elif intface . startswith ( 'veth' ): continue elif intface . startswith ( 'br-' ): continue elif intface not in stats : continue available_networks . append ( intface ) return available_networks get_primary_ip_address () Get the IP address for the default route out. Returns: Type Description str The IP address of the primary (default route) interface Source code in dynamite_nsm/utilities.py def get_primary_ip_address () -> str : \"\"\"Get the IP address for the default route out. Args: Returns: The IP address of the primary (default route) interface \"\"\" s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) try : # doesn't even have to be reachable s . connect (( '10.255.255.255' , 1 )) IP = s . getsockname ()[ 0 ] except Exception : IP = '127.0.0.1' finally : s . close () return IP get_sshd_directory_path () Gets the path of the Include directory in the sshd_config Returns: Type Description The path to the sshd_config.d/ Source code in dynamite_nsm/utilities.py def get_sshd_directory_path (): \"\"\"Gets the path of the Include directory in the sshd_config Returns: The path to the sshd_config.d/ \"\"\" include_directory = None with open ( const . SSH_CONF_FILE , 'r' ) as sudoers_in : for i , line in enumerate ( sudoers_in . readlines ()): line = line . strip () if line . startswith ( 'Include' ): include_directory = ' ' . join ( line . split ( ' ' )[ 1 :]) break include_directory = include_directory . replace ( '*.conf' , '' ) return include_directory get_sudoers_directory_path () Get the path to the #includedir directory Returns: Type Description The path to sudoers.d/ Source code in dynamite_nsm/utilities.py def get_sudoers_directory_path (): \"\"\"Get the path to the #includedir directory Returns: The path to sudoers.d/ \"\"\" include_directory = None with open ( const . SUDOERS_FILE , 'r' ) as sudoers_in : for i , line in enumerate ( sudoers_in . readlines ()): line = line . strip () if line . startswith ( '#includedir' ) or line . startswith ( '@includedir' ): include_directory = ' ' . join ( line . split ( ' ' )[ 1 :]) break return include_directory get_terminal_size () Returns the width and height of the current terminal Returns: Type Description Optional[Tuple[int, int]] (width, height) of the current terminal Source code in dynamite_nsm/utilities.py def get_terminal_size () -> Optional [ Tuple [ int , int ]]: \"\"\"Returns the width and height of the current terminal Args: Returns: (width, height) of the current terminal \"\"\" try : h , w , hp , wp = struct . unpack ( 'HHHH' , fcntl . ioctl ( 0 , termios . TIOCGWINSZ , struct . pack ( 'HHHH' , 0 , 0 , 0 , 0 ))) except Exception : return None return w , h is_dynamite_member ( user ) Check if a user is a member of the dynamite group Parameters: Name Type Description Default user str A username required Returns: Type Description bool True, if the user is a member of the dynamite group Source code in dynamite_nsm/utilities.py def is_dynamite_member ( user : str ) -> bool : \"\"\" Check if a user is a member of the dynamite group Args: user: A username Returns: True, if the user is a member of the dynamite group \"\"\" group = grp . getgrnam ( 'dynamite' ) return user in group [ 3 ] is_root () Determine whether or not the current user is root Returns: Type Description bool True, if the user is root Source code in dynamite_nsm/utilities.py def is_root () -> bool : \"\"\"Determine whether or not the current user is root Args: Returns: True, if the user is root \"\"\" return os . getuid () == 0 is_setup () Check if DynamiteNSM has required directories created. Returns: Type Description bool True if setup properly Source code in dynamite_nsm/utilities.py def is_setup () -> bool : \"\"\"Check if DynamiteNSM has required directories created. Returns: True if setup properly \"\"\" if not os . path . exists ( const . CONFIG_PATH ): return False elif not os . path . exists ( const . INSTALL_PATH ): return False elif not os . path . exists ( const . LOG_PATH ): return False return True list_backup_configurations ( configuration_backup_directory ) List available backup files in the configuration_backup_directory Parameters: Name Type Description Default configuration_backup_directory str The destination configuration directory backup directory required Returns: Type Description List[Dict] A list of dictionaries, where each dictionary contains a filename representing the name of the backup and a UNIX timestamp. Source code in dynamite_nsm/utilities.py def list_backup_configurations ( configuration_backup_directory : str ) -> List [ Dict ]: \"\"\"List available backup files in the configuration_backup_directory Args: configuration_backup_directory: The destination configuration directory backup directory Returns: A list of dictionaries, where each dictionary contains a filename representing the name of the backup and a UNIX timestamp. \"\"\" backups = [] digits_only_re = re . compile ( \"^([\\s\\d]+)$\" ) try : for conf in os . listdir ( configuration_backup_directory ): timestampstr = conf . split ( '.' )[ - 1 ] if not digits_only_re . match ( timestampstr ): confpath = os . path . join ( configuration_backup_directory , conf ) if os . path . isdir ( confpath ): for subconf in os . listdir ( confpath ): timestampstr = subconf . split ( '.' )[ - 1 ] if digits_only_re . match ( timestampstr ): backups . append ( { 'filename' : subconf , 'filepath' : os . path . join ( confpath , subconf ), 'time' : float ( timestampstr ) } ) else : # file is not a dir, and does not match expected format with timestamp. skip it. continue else : backups . append ( { 'filename' : conf , 'filepath' : os . path . join ( configuration_backup_directory , conf ), 'time' : float ( timestampstr ) } ) except FileNotFoundError : return backups backups . sort ( key = lambda item : item [ 'time' ], reverse = True ) return backups makedirs ( path , exist_ok = True ) Create directory(ies) at a given path Parameters: Name Type Description Default path str The path to the directories required exist_ok Optional[bool] If it exists, create anyway (Default value = True) True Returns: Type Description None None Source code in dynamite_nsm/utilities.py def makedirs ( path : str , exist_ok : Optional [ bool ] = True ) -> None : \"\"\"Create directory(ies) at a given path Args: path: The path to the directories exist_ok: If it exists, create anyway (Default value = True) Returns: None \"\"\" if exist_ok : os . makedirs ( path , exist_ok = True ) else : os . makedirs ( path ) print_coffee_art () Print the dynamite logo! Returns: Type Description None None Source code in dynamite_nsm/utilities.py def print_coffee_art () -> None : \"\"\"Print the dynamite logo! Args: Returns: None \"\"\" try : coffee_icon = \\ \"\"\" \u256d\u256f\u256d\u256f\u256d\u256f \u2588\u2593\u2593\u2593\u2593\u2593\u2588\u2550\u256e \u2588\u2593\u2593\u2593\u2593\u2593\u2588\u258f\ufe31 \u2588\u2593\u2593\u2593\u2593\u2593\u2588\u2550\u256f \u25e5\u2588\u2588\u2588\u2588\u2588\u25e4 ~~~ \"Have a cup of coffee while you wait.\" ~~~ \"\"\" print ( coffee_icon ) print ( ' \\n ' ) except ( SyntaxError , UnicodeEncodeError ): # Your operating system is super lame :( pass print_dynamite_lab_art () Print the dynamite lab ascii art Returns: Type Description None None Source code in dynamite_nsm/utilities.py def print_dynamite_lab_art () -> None : \"\"\"Print the dynamite lab ascii art Args: Returns: None \"\"\" try : lab_art = \\ \"\"\" \\033[0;36m _ | | DynamiteLab | / \\ is an experimental | / \\ feature. | (_____) Happy Hacking! | ~The Dynamite Team~ \\033[0m \"\"\" print ( lab_art ) except ( SyntaxError , UnicodeEncodeError ): print () print_dynamite_logo ( version ) Print the dynamite logo! Returns: Type Description None None Source code in dynamite_nsm/utilities.py def print_dynamite_logo ( version : str ) -> None : \"\"\"Print the dynamite logo! Args: Returns: None \"\"\" try : dynamite_logo = \\ \"\"\" \\033[0;36m ,,,,, ,\u2584\u2584\u2584\u2584\u2553 .\u2584\u2593\u2580\u2580\u2580\u2591\u2580\u2580\u2580\u2593\u2593\u2553 \u2554\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2580\u2593 #\u2229\u2553 \u2580\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2584\u2580\u2593\u2584 \u2551\u258c\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2569\u2593 \u2580\u2593\"\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2229\u2593\u2584 ,,\u2584\u2584\u2584\u2593\u2593\u2593\u258c\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2566\u2593 \u2590\u2593\u2559\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2590\u2593\u2580\u2580\u2580^\u2559\u2514\"^^\u2580\u2593\u2580\u2593\u2593\u2593\u2593\u2580\u2592\u2593` \u2590\u2593]\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2590\u2593 \u2580\u2580\u2580\u2580\u2580\u2580^ \u2584\u2593.\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u00dc\u2593\u2580 \u2559\u2559\u2580\u2588\u2592\u2584\u2584,, '#\u03b5\u2559\u2559\u2584\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2580\u2584\u2593\u258c `\u2559\u2580\u2580\u2593\u2593\u2593\u2593\u2584\u2584\u2553, ,, \"\u2588\u2593\u2584\u2584\u2593\u2593\u2593\u2591\u2584\u2593\u2593\u2580 \u2559\u2557, '\"\u2580\u2580\u2588\u2593\u2593\u2593\u2593\u2593\u2584#\u2563\u2593\u2593\u2593\u2593 \u2551\u2580\"\u2580\u2580\u2514, \u2580\u2593\u2584 ^\u2580\u2580\u2580\u258c\u2593\u2593\u2593\u2593\u2593\u255b \u2554\u2593 \u2593 \u2580\u2593\u2584,\u2553\u2553, \u2559\u2580\u2580\u2580\" ]\u2593\u258c \u2559\u258c '\u2593\u2593\u2593\u2593\u2593\u2593\u2310 \u2553\u2584\u2584\u2593\u2593\u2591 \u2593\u258c \u256b\u2593\u2593\u2593\u2593\u2593> \u2553\u2593\u2580\u2593\u2593\u2593\u2593\u2593\u2580\u2593 \u2559\u2593\u258c \u2559\u2559\u2580\u2559 \u2554\u2593\u2592\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2591\u2593 \u2551\u2593\u2555 \u255a\u258c\u2551\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2569\u2593 \u2593\u2593 \u2580\u2593\u2580\u2593\u2593\u2593\u2593\u2580\u2560\u2593\u2518 \u255a\u2593\u2593 \u2580\u2580\u2588\u2588\u2580\u2580\u2559 \u2593\u2593\u2593\u2553 \u256b\u2593\u2593\u2593\u2593\u2593\u03b5 \\033[0m http://dynamite.ai Version: {} \"\"\" . format ( version ) print ( dynamite_logo ) print ( ' \\n ' ) except ( SyntaxError , UnicodeEncodeError ): print ( 'http://dynamite.ai \\n\\n Version: {} \\n ' . format ( version )) prompt_input ( message , valid_responses = None ) Taking in input Parameters: Name Type Description Default message str The message appearing next to the input prompt. required valid_responses Optional[List] A list of expected responses None Returns: Type Description str The inputted text Source code in dynamite_nsm/utilities.py def prompt_input ( message : str , valid_responses : Optional [ List ] = None ) -> str : \"\"\"Taking in input Args: message: The message appearing next to the input prompt. valid_responses: A list of expected responses Returns: The inputted text \"\"\" res = input ( message ) if valid_responses : while str ( res ) . strip () not in [ str ( r ) for r in valid_responses ]: print ( f 'Please enter a valid value: { valid_responses } ' ) res = input ( message ) return res prompt_password ( prompt = '[?] Enter a secure password: ' , confirm_prompt = '[?] Confirm Password: ' ) Prompt user for password, and confirm Parameters: Name Type Description Default prompt The first password prompt '[?] Enter a secure password: ' confirm_prompt The confirmation prompt '[?] Confirm Password: ' Returns: Type Description str The password entered Source code in dynamite_nsm/utilities.py def prompt_password ( prompt = '[?] Enter a secure password: ' , confirm_prompt = '[?] Confirm Password: ' ) -> str : \"\"\"Prompt user for password, and confirm Args: prompt: The first password prompt confirm_prompt: The confirmation prompt Returns: The password entered \"\"\" password = '0' confirm_password = '1' first_attempt = True valid = False while password != confirm_password or len ( password ) < 6 or not valid : if not first_attempt : sys . stderr . write ( '[-] Passwords either did not match or were less than 6 characters. Please try again. \\n\\n ' ) sys . stderr . flush () elif '\"' in password or \"'\" in password : sys . stderr . write ( '[-] Passwords cannot contain quote characters. Please try again. \\n\\n ' ) sys . stderr . flush () else : valid = True password = getpass . getpass ( prompt ) confirm_password = getpass . getpass ( confirm_prompt ) first_attempt = False return password restore_backup_configuration ( configuration_backup_filepath , config_filepath ) Restore a backup file to a configuration folder of choice Parameters: Name Type Description Default configuration_backup_filepath str The full path to the backup file required config_filepath str The full path to the to-be-restored configuration file required Returns: Type Description bool True, if successful Source code in dynamite_nsm/utilities.py def restore_backup_configuration ( configuration_backup_filepath : str , config_filepath : str ) -> bool : \"\"\"Restore a backup file to a configuration folder of choice Args: configuration_backup_filepath: The full path to the backup file config_filepath: The full path to the to-be-restored configuration file Returns: True, if successful \"\"\" try : shutil . move ( configuration_backup_filepath , config_filepath ) return True except shutil . Error : return False except FileNotFoundError : return False run_subprocess_with_status ( process , expected_lines = None ) Run a subprocess inside a wrapper, that hides the output, and replaces with a progressbar Parameters: Name Type Description Default process Popen The subprocess.Popen instance required expected_lines Optional[int] The number of stdout lines to expect None Returns: Type Description int The exit code. Source code in dynamite_nsm/utilities.py def run_subprocess_with_status ( process : subprocess . Popen , expected_lines : Optional [ int ] = None ) -> int : \"\"\"Run a subprocess inside a wrapper, that hides the output, and replaces with a progressbar Args: process: The subprocess.Popen instance expected_lines: The number of stdout lines to expect Returns: The exit code. \"\"\" i = 0 widgets = [ ' \\033 [92m' , ' {} ' . format ( datetime . strftime ( datetime . utcnow (), '%Y-%m- %d %H:%M:%S' )), ' \\033 [0m' , ' \\033 [0;36m' , 'PROCESS_TRACKER ' , ' \\033 [0m' , ' | ' , progressbar . Percentage (), ' ' , progressbar . Bar (), ' ' , progressbar . FormatLabel ( '' ), ' ' , progressbar . ETA () ] over_max_value = False pb = progressbar . ProgressBar ( widgets = widgets , maxval = expected_lines ) pb . start () while True : output = process . stdout . readline () . decode () if output == '' and process . poll () is not None : break if output : i += 1 try : if not over_max_value : widgets [ 11 ] = '< {0} ...>' . format ( str ( output ) . replace ( ' \\n ' , '' ) . replace ( ' \\t ' , '' )[ 0 : 40 ]) pb . update ( i ) except ValueError : if not over_max_value : pb . finish () over_max_value = True # print(i, process.poll(), output) if not over_max_value : pb . finish () return process . poll () safely_remove_file ( path ) Remove a file if it exists at the given path Parameters: Name Type Description Default path str The path of the file to remove required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def safely_remove_file ( path : str ) -> None : \"\"\"Remove a file if it exists at the given path Args: path: The path of the file to remove Returns: None \"\"\" if os . path . exists ( path ): os . remove ( path ) set_ownership_of_file ( path , user = 'dynamite' , group = 'dynamite' ) Set the ownership of a file given a user/group and a path Parameters: Name Type Description Default path str The path to the file required user Optional[str] The name of the user 'dynamite' group Optional[str] The group of the user 'dynamite' Returns: Type Description None None Source code in dynamite_nsm/utilities.py def set_ownership_of_file ( path : str , user : Optional [ str ] = 'dynamite' , group : Optional [ str ] = 'dynamite' ) -> None : \"\"\"Set the ownership of a file given a user/group and a path Args: path: The path to the file user: The name of the user group: The group of the user Returns: None \"\"\" uid = pwd . getpwnam ( user ) . pw_uid group = grp . getgrnam ( group ) . gr_gid os . chown ( path , uid , group ) for root , dirs , files in os . walk ( path ): for momo in dirs : os . chown ( os . path . join ( root , momo ), uid , group ) for momo in files : if momo == 'environment' : continue os . chown ( os . path . join ( root , momo ), uid , group ) set_permissions_of_file ( file_path , unix_permissions_integer ) Set the permissions of a file to unix_permissions_integer Parameters: Name Type Description Default file_path str The path to the file required unix_permissions_integer Union[str, int] The numeric representation of user/group/everyone permissions on a file required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def set_permissions_of_file ( file_path : str , unix_permissions_integer : Union [ str , int ]) -> None : \"\"\"Set the permissions of a file to unix_permissions_integer Args: file_path: The path to the file unix_permissions_integer: The numeric representation of user/group/everyone permissions on a file Returns: None \"\"\" subprocess . call ( 'chmod -R {} {} ' . format ( unix_permissions_integer , file_path ), shell = True ) test_bpf_filter ( expr , include_message = False ) Given a BPF expression determine if it is valid, and optionally return a message if not Parameters: Name Type Description Default expr str A valid Berkeley Packet Filter required include_message bool If True, Include an error message if expression is not valid. False Returns: Type Description Union[bool, Tuple[bool, str]] The result and optional result message Source code in dynamite_nsm/utilities.py def test_bpf_filter ( expr : str , include_message : bool = False ) -> Union [ bool , Tuple [ bool , str ]]: \"\"\"Given a BPF expression determine if it is valid, and optionally return a message if not Args: expr: A valid Berkeley Packet Filter include_message: If True, Include an error message if expression is not valid. Returns: The result and optional result message \"\"\" bin_path = pkg_resources . resource_filename ( 'dynamite_nsm' , 'bin/bpf_validate' ) set_permissions_of_file ( bin_path , '+x' ) p = subprocess . Popen ([ bin_path ] + expr . split ( ' ' ), stdout = subprocess . PIPE ) output , _ = p . communicate () serialized_values = json . loads ( output ) if not include_message : return serialized_values [ 'success' ] else : return serialized_values [ 'success' ], serialized_values [ 'msg' ] update_sysctl ( verbose = False ) Updates the vm.max_map_count and fs.file-max count Parameters: Name Type Description Default verbose Optional[bool] Include output from system utilities False Returns: Type Description None None Source code in dynamite_nsm/utilities.py def update_sysctl ( verbose : Optional [ bool ] = False ) -> None : \"\"\"Updates the vm.max_map_count and fs.file-max count Args: verbose: Include output from system utilities Returns: None \"\"\" new_output = '' vm_found = False fs_found = False for line in open ( '/etc/sysctl.conf' ) . readlines (): if not line . startswith ( '#' ) and 'vm.max_map_count' in line : new_output += 'vm.max_map_count=262144 \\n ' vm_found = True elif not line . startswith ( '#' ) and 'fs.file-max' in line : new_output += 'fs.file-max=65535 \\n ' fs_found = True else : new_output += line . strip () + ' \\n ' if not vm_found : new_output += 'vm.max_map_count=262144 \\n ' if not fs_found : new_output += 'fs.file-max=65535 \\n ' with open ( '/etc/sysctl.conf' , 'w' ) as f : f . write ( new_output ) if verbose : subprocess . call ( 'sysctl -w vm.max_map_count=262144' , shell = True ) subprocess . call ( 'sysctl -w fs.file-max=65535' , shell = True ) subprocess . call ( 'sysctl -p' , shell = True ) else : subprocess . call ( 'sysctl -w vm.max_map_count=262144' , shell = True , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) subprocess . call ( 'sysctl -w fs.file-max=65535' , shell = True , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) subprocess . call ( 'sysctl -p' , shell = True , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) update_user_file_handle_limits () Updates the max number of file handles the dynamite user can have open Returns: Type Description None None Source code in dynamite_nsm/utilities.py def update_user_file_handle_limits () -> None : \"\"\"Updates the max number of file handles the dynamite user can have open Args: Returns: None \"\"\" new_output = '' limit_found = False for line in open ( '/etc/security/limits.conf' ) . readlines (): if line . startswith ( 'dynamite' ): new_output += 'dynamite - nofile 65535' limit_found = True else : new_output += line . strip () new_output += ' \\n ' if not limit_found : new_output += ' \\n dynamite - nofile 65535 \\n ' with open ( '/etc/security/limits.conf' , 'w' ) as f : f . write ( new_output ) wrap_text ( s ) Given a string adds newlines based on the current size of the terminal window (if one is found) Parameters: Name Type Description Default s str A string required Returns: Type Description str A new line delaminated string Source code in dynamite_nsm/utilities.py def wrap_text ( s : str ) -> str : \"\"\"Given a string adds newlines based on the current size of the terminal window (if one is found) Args: s: A string Returns: A new line delaminated string \"\"\" if not s : return \"\" term_dim = get_terminal_size () if not term_dim : w , h = 150 , 90 else : w , h = term_dim wrapped_s = ' \\n ' . join ( textwrap . wrap ( s , w - 40 , fix_sentence_endings = True )) return wrapped_s","title":"utilities.py"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.backup_configuration_file","text":"Backup a configuration file Parameters: Name Type Description Default source_file str The configuration file you wish to backup required configuration_backup_directory str The destination configuration directory required destination_file_prefix str The prefix of the file; timestamp is automatically appended in filename required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def backup_configuration_file ( source_file : str , configuration_backup_directory : str , destination_file_prefix : str ) -> None : \"\"\"Backup a configuration file Args: source_file: The configuration file you wish to backup configuration_backup_directory: The destination configuration directory destination_file_prefix: The prefix of the file; timestamp is automatically appended in filename Returns: None \"\"\" timestamp = int ( time . time ()) destination_backup_config_file = os . path . join ( configuration_backup_directory , ' {} . {} ' . format ( destination_file_prefix , timestamp )) try : makedirs ( configuration_backup_directory , exist_ok = True ) set_ownership_of_file ( configuration_backup_directory ) except Exception as e : raise exceptions . WriteConfigError ( \"General error while attempting to create backup directory at {} ; {} \" . format ( configuration_backup_directory , e )) try : shutil . copy ( source_file , destination_backup_config_file ) except Exception as e : raise exceptions . WriteConfigError ( \"General error while attempting to copy {} to {} \" . format ( source_file , destination_backup_config_file , e ))","title":"backup_configuration_file()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.check_pid","text":":return: True, if the process is running Returns: Type Description bool None Source code in dynamite_nsm/utilities.py def check_pid ( pid : int ) -> bool : \"\"\":return: True, if the process is running Args: The process id Returns: None \"\"\" if not pid : return False if pid == - 1 : return False try : os . kill ( pid , 0 ) except OSError : return False else : return True","title":"check_pid()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.check_socket","text":"Check if a host is listening on a given port Parameters: Name Type Description Default host str The host the service is listening on required port int The port the service is listening on required Returns: Type Description bool True, if a service is listening on a given HOST:PORT Source code in dynamite_nsm/utilities.py def check_socket ( host : str , port : int ) -> bool : \"\"\"Check if a host is listening on a given port Args: host: The host the service is listening on port: The port the service is listening on Returns: True, if a service is listening on a given HOST:PORT \"\"\" if isinstance ( port , str ): port = int ( port ) with closing ( socket . socket ( socket . AF_INET , socket . SOCK_STREAM )) as sock : if sock . connect_ex (( host , port )) == 0 : return True else : return False","title":"check_socket()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.check_user_exists","text":"Check of a UNIX user exists Parameters: Name Type Description Default username str The username of the user to check required Returns: Type Description bool : True if the user exists Source code in dynamite_nsm/utilities.py def check_user_exists ( username : str ) -> bool : \"\"\"Check of a UNIX user exists Args: username: The username of the user to check Returns: : True if the user exists \"\"\" try : pwd . getpwnam ( username ) return True except KeyError : return False","title":"check_user_exists()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.copytree","text":"Copy a src file or directory to a destination file or directory Parameters: Name Type Description Default src str The source directory required dst str The destination directory required symlinks Optional[bool] If True, symlinks will be followed (Default value = False) False ignore Optional[bool] If True, errors will be ignored None Returns: Type Description None None Source code in dynamite_nsm/utilities.py def copytree ( src : str , dst : str , symlinks : Optional [ bool ] = False , ignore : Optional [ bool ] = None ) -> None : \"\"\"Copy a src file or directory to a destination file or directory Args: src: The source directory dst: The destination directory symlinks: If True, symlinks will be followed (Default value = False) ignore: If True, errors will be ignored Returns: None \"\"\" for item in os . listdir ( src ): s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if os . path . isdir ( s ): try : shutil . copytree ( s , d , symlinks , ignore ) except Exception : # File exists or handle locked pass else : shutil . copy2 ( s , d )","title":"copytree()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.create_dynamite_environment_file","text":"Creates the dynamite environment file accessible only to the root user. Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_dynamite_environment_file () -> None : \"\"\"Creates the dynamite environment file accessible only to the root user. Args: Returns: None \"\"\" makedirs ( const . CONFIG_PATH , exist_ok = True ) env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) env_file_f = open ( env_file , 'a' ) env_file_f . write ( '' ) env_file_f . close () set_ownership_of_file ( env_file , user = 'dynamite' , group = 'dynamite' ) set_permissions_of_file ( env_file , 770 )","title":"create_dynamite_environment_file()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.create_dynamite_remote_user","text":"Create the dynamite-remote user and group Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_dynamite_remote_user () -> None : \"\"\"Create the dynamite-remote user and group Returns: None \"\"\" password = salt = str ( random . randint ( 10 , 99 )) pass_encry = crypt . crypt ( password , salt ) subprocess . call ( 'useradd -r -p \" {} \" -s /bin/bash dynamite-remote' . format ( pass_encry ), shell = True )","title":"create_dynamite_remote_user()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.create_dynamite_user","text":"Create the dynamite user and group Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_dynamite_user () -> None : \"\"\"Create the dynamite user and group Returns: None \"\"\" password = salt = str ( random . randint ( 10 , 99 )) pass_encry = crypt . crypt ( password , salt ) subprocess . call ( 'useradd -r -p \" {} \" -s /bin/bash dynamite' . format ( pass_encry ), shell = True )","title":"create_dynamite_user()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.create_jupyter_user","text":"Create the jupyter user w/ home Parameters: Name Type Description Default password str The password for the user required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def create_jupyter_user ( password : str ) -> None : \"\"\"Create the jupyter user w/ home Args: password: The password for the user Returns: None \"\"\" pass_encry = crypt . crypt ( password , str ( random . randint ( 10 , 99 ))) subprocess . call ( 'useradd -r -m -p \" {} \" -s /bin/bash jupyter' . format ( pass_encry ), shell = True )","title":"create_jupyter_user()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.delete_dynamite_remote_user","text":"Remove the dynamite-remote user Returns: Type Description None None Source code in dynamite_nsm/utilities.py def delete_dynamite_remote_user () -> None : \"\"\" Remove the dynamite-remote user Returns: None \"\"\" subprocess . run ([ 'userdel' , 'dynamite-remote' ])","title":"delete_dynamite_remote_user()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.download_file","text":"Given a URL and destination file name, download the file to local install_cache Parameters: Name Type Description Default url str The url to the file to download required filename str The name of the file to write to disk required stdout Optional[bool] Print the output to the console False Returns: True, if successfully downloaded. Source code in dynamite_nsm/utilities.py def download_file ( url : str , filename : str , stdout : Optional [ bool ] = False ) -> bool : \"\"\" Given a URL and destination file name, download the file to local install_cache Args: url: The url to the file to download filename: The name of the file to write to disk stdout: Print the output to the console Returns: True, if successfully downloaded. \"\"\" CHUNK = 16 * 1024 makedirs ( const . INSTALL_CACHE , exist_ok = True ) response = urlopen ( url ) try : response_size_bytes = int ( response . headers [ 'Content-Length' ]) except ( KeyError , TypeError , ValueError ): response_size_bytes = None widgets = [ ' \\033 [92m' , ' {} ' . format ( datetime . strftime ( datetime . utcnow (), '%Y-%m- %d %H:%M:%S' )), ' \\033 [0m' , ' \\033 [0;36m' 'DOWNLOAD_MANAGER ' , ' \\033 [0m' , ' | ' , progressbar . FileTransferSpeed (), ' ' , progressbar . Bar (), ' ' , '( {} )' . format ( filename ), ' ' , progressbar . ETA (), ] pb = progressbar . ProgressBar ( widgets = widgets , maxval = int ( response_size_bytes )) if stdout : try : pb . start () except Exception : # Something broke, disable stdout going forward stdout = False try : with open ( os . path . join ( const . INSTALL_CACHE , filename ), 'wb' ) as f : chunk_num = 0 while True : chunk = response . read ( CHUNK ) if not chunk : break f . write ( chunk ) if stdout : try : pb . update ( CHUNK * chunk_num ) except ValueError : pass chunk_num += 1 if stdout : pb . finish () except URLError : return False return True","title":"download_file()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.extract_archive","text":"Extract a tar.gz archive to a given destination path. Parameters: Name Type Description Default archive_path str The full path to the tar.gz archive file required destination_path str The path where the archive will be extracted required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def extract_archive ( archive_path : str , destination_path : str ) -> None : \"\"\"Extract a tar.gz archive to a given destination path. Args: archive_path: The full path to the tar.gz archive file destination_path: The path where the archive will be extracted Returns: None \"\"\" try : tf = tarfile . open ( archive_path ) tf . extractall ( path = destination_path ) except IOError : pass","title":"extract_archive()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.generate_random_password","text":"Generate a random password containing alphanumeric and symbolic characters Parameters: Name Type Description Default length int The length of the password 30 Returns: Type Description str The string representation of the password Source code in dynamite_nsm/utilities.py def generate_random_password ( length : int = 30 ) -> str : \"\"\"Generate a random password containing alphanumeric and symbolic characters Args: length: The length of the password Returns: The string representation of the password \"\"\" tokens = string . ascii_lowercase + string . ascii_uppercase + '0123456789' + '!@#$%^&*()_+' return '' . join ( random . choice ( tokens ) for i in range ( length ))","title":"generate_random_password()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_cpu_core_count","text":"Get the number of availble CPU cores Returns: Type Description int The count of CPU cores available on the system Source code in dynamite_nsm/utilities.py def get_cpu_core_count () -> int : \"\"\"Get the number of availble CPU cores Args: Returns: The count of CPU cores available on the system \"\"\" return multiprocessing . cpu_count ()","title":"get_cpu_core_count()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_default_agent_tag","text":"Get the agent tag Returns: Type Description str The agent tag Source code in dynamite_nsm/utilities.py def get_default_agent_tag () -> str : \"\"\"Get the agent tag Args: Returns: The agent tag \"\"\" return '' . join ([ c . lower () for c in str ( socket . gethostname ()) if c . isalnum ()][ 0 : 25 ]) + '_agt'","title":"get_default_agent_tag()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_default_es_node_name","text":":return: The node name Returns: Type Description str The node name. Source code in dynamite_nsm/utilities.py def get_default_es_node_name () -> str : \"\"\":return: The node name Args: Returns: The node name. \"\"\" return '' . join ([ c . lower () for c in str ( socket . gethostname ()) if c . isalnum ()][ 0 : 25 ]) + '_es_node'","title":"get_default_es_node_name()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_environment_file_dict","text":"Get the contents of the dynamite environment file as a dictionary. Returns: Type Description Dict The contents of the /etc/dynamite/environment file as a dictionary Source code in dynamite_nsm/utilities.py def get_environment_file_dict () -> Dict : \"\"\"Get the contents of the dynamite environment file as a dictionary. Args: Returns: The contents of the /etc/dynamite/environment file as a dictionary \"\"\" export_dict = {} try : for line in open ( os . path . join ( const . CONFIG_PATH , 'environment' )) . readlines (): if '=' in line : key , value = line . strip () . split ( '=' ) export_dict [ key ] = value except PermissionError : return {} except FileNotFoundError : return {} return export_dict","title":"get_environment_file_dict()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_environment_file_str","text":"Get the contents of the dynamite environment file as a string. Returns: Type Description str The contents of the /etc/dynamite/environment file as a giant export string Source code in dynamite_nsm/utilities.py def get_environment_file_str () -> str : \"\"\"Get the contents of the dynamite environment file as a string. Args: Returns: The contents of the /etc/dynamite/environment file as a giant export string \"\"\" export_str = '' with open ( os . path . join ( const . CONFIG_PATH , 'environment' )) as env_f : for line in env_f . readlines (): if '=' in line : key , value = line . strip () . split ( '=' ) export_str += 'export {} = \\' {} \\' && ' . format ( key , value ) return export_str","title":"get_environment_file_str()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_epoch_time_seconds","text":"Get the number of seconds since 01/01/1970 Returns: An integer representing the number of seconds between 01/01/1970 and now. Source code in dynamite_nsm/utilities.py def get_epoch_time_seconds () -> int : \"\"\"Get the number of seconds since 01/01/1970 Returns: An integer representing the number of seconds between 01/01/1970 and now. \"\"\" return int ( time . time ())","title":"get_epoch_time_seconds()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_file_md5_hash","text":"Given a file-like object return the md5 hash of that object Parameters: Name Type Description Default fh Union[BinaryIO, TextIO] file handle (file like object) required Returns: Type Description str the md5 hash of the file Source code in dynamite_nsm/utilities.py def get_file_md5_hash ( fh : Union [ BinaryIO , TextIO ]) -> str : \"\"\"Given a file-like object return the md5 hash of that object Args: fh: file handle (file like object) Returns: the md5 hash of the file \"\"\" block_size = 65536 md5_hasher = md5 () buf = fh . read ( block_size ) while len ( buf ) > 0 : md5_hasher . update ( buf ) buf = fh . read ( block_size ) return md5_hasher . hexdigest ()","title":"get_file_md5_hash()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_filepath_md5_hash","text":"Given a file-path to return the md5 hash of that file Parameters: Name Type Description Default file_path str path to the file being hashed required Returns: Type Description str the md5 hash of a file Source code in dynamite_nsm/utilities.py def get_filepath_md5_hash ( file_path : str ) -> str : \"\"\"Given a file-path to return the md5 hash of that file Args: file_path: path to the file being hashed Returns: the md5 hash of a file \"\"\" with open ( file_path , 'rb' ) as afile : return get_file_md5_hash ( afile )","title":"get_filepath_md5_hash()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_memory_available_bytes","text":"Get the amount of RAM (in bytes) of the current system Returns: Type Description int The number of bytes available in memory Source code in dynamite_nsm/utilities.py def get_memory_available_bytes () -> int : \"\"\"Get the amount of RAM (in bytes) of the current system Args: Returns: The number of bytes available in memory \"\"\" return os . sysconf ( 'SC_PAGE_SIZE' ) * os . sysconf ( 'SC_PHYS_PAGES' )","title":"get_memory_available_bytes()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_network_addresses","text":"Returns a list of valid IP addresses for the host Returns: Type Description Tuple A tuple containing an internal, and external IP address Source code in dynamite_nsm/utilities.py def get_network_addresses () -> Tuple : \"\"\"Returns a list of valid IP addresses for the host Args: Returns: A tuple containing an internal, and external IP address \"\"\" valid_addresses = [] internal_address , external_address = None , None try : site = str ( urlopen ( \"http://checkip.dyndns.org/\" , timeout = 2 ) . read ()) grab = re . findall ( r '([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)' , site ) external_address = grab [ 0 ] except ( URLError , IndexError , HTTPError ): pass internal_address = socket . gethostbyname ( socket . gethostname ()) if internal_address : valid_addresses . append ( internal_address ) if external_address : valid_addresses . append ( external_address ) return tuple ( valid_addresses )","title":"get_network_addresses()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_network_interface_configurations","text":"Returns a list of network interfaces available on the system Returns: Type Description List[Dict] A list of network interfaces Source code in dynamite_nsm/utilities.py def get_network_interface_configurations () -> List [ Dict ]: \"\"\"Returns a list of network interfaces available on the system Args: Returns: A list of network interfaces \"\"\" addresses = psutil . net_if_addrs () stats = psutil . net_if_stats () available_networks = [] for intface , addr_list in addresses . items (): if intface . startswith ( 'lo' ): continue elif intface . startswith ( 'docker' ): continue elif intface . startswith ( 'veth' ): continue elif intface . startswith ( 'br-' ): continue elif intface not in stats : continue name = intface speed = stats [ intface ] . speed duplex = str ( stats [ intface ] . duplex ) mtu = stats [ intface ] . mtu available_networks . append ({ 'name' : name , 'speed' : speed , 'duplex' : duplex , 'mtu' : mtu }) return available_networks","title":"get_network_interface_configurations()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_network_interface_names","text":"Returns a list of network interfaces available on the system Returns: Type Description List[str] A list of network interfaces Source code in dynamite_nsm/utilities.py def get_network_interface_names () -> List [ str ]: \"\"\"Returns a list of network interfaces available on the system Args: Returns: A list of network interfaces \"\"\" addresses = psutil . net_if_addrs () stats = psutil . net_if_stats () available_networks = [] for intface , addr_list in addresses . items (): if intface . startswith ( 'lo' ): continue elif intface . startswith ( 'docker' ): continue elif intface . startswith ( 'veth' ): continue elif intface . startswith ( 'br-' ): continue elif intface not in stats : continue available_networks . append ( intface ) return available_networks","title":"get_network_interface_names()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_primary_ip_address","text":"Get the IP address for the default route out. Returns: Type Description str The IP address of the primary (default route) interface Source code in dynamite_nsm/utilities.py def get_primary_ip_address () -> str : \"\"\"Get the IP address for the default route out. Args: Returns: The IP address of the primary (default route) interface \"\"\" s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) try : # doesn't even have to be reachable s . connect (( '10.255.255.255' , 1 )) IP = s . getsockname ()[ 0 ] except Exception : IP = '127.0.0.1' finally : s . close () return IP","title":"get_primary_ip_address()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_sshd_directory_path","text":"Gets the path of the Include directory in the sshd_config Returns: Type Description The path to the sshd_config.d/ Source code in dynamite_nsm/utilities.py def get_sshd_directory_path (): \"\"\"Gets the path of the Include directory in the sshd_config Returns: The path to the sshd_config.d/ \"\"\" include_directory = None with open ( const . SSH_CONF_FILE , 'r' ) as sudoers_in : for i , line in enumerate ( sudoers_in . readlines ()): line = line . strip () if line . startswith ( 'Include' ): include_directory = ' ' . join ( line . split ( ' ' )[ 1 :]) break include_directory = include_directory . replace ( '*.conf' , '' ) return include_directory","title":"get_sshd_directory_path()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_sudoers_directory_path","text":"Get the path to the #includedir directory Returns: Type Description The path to sudoers.d/ Source code in dynamite_nsm/utilities.py def get_sudoers_directory_path (): \"\"\"Get the path to the #includedir directory Returns: The path to sudoers.d/ \"\"\" include_directory = None with open ( const . SUDOERS_FILE , 'r' ) as sudoers_in : for i , line in enumerate ( sudoers_in . readlines ()): line = line . strip () if line . startswith ( '#includedir' ) or line . startswith ( '@includedir' ): include_directory = ' ' . join ( line . split ( ' ' )[ 1 :]) break return include_directory","title":"get_sudoers_directory_path()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.get_terminal_size","text":"Returns the width and height of the current terminal Returns: Type Description Optional[Tuple[int, int]] (width, height) of the current terminal Source code in dynamite_nsm/utilities.py def get_terminal_size () -> Optional [ Tuple [ int , int ]]: \"\"\"Returns the width and height of the current terminal Args: Returns: (width, height) of the current terminal \"\"\" try : h , w , hp , wp = struct . unpack ( 'HHHH' , fcntl . ioctl ( 0 , termios . TIOCGWINSZ , struct . pack ( 'HHHH' , 0 , 0 , 0 , 0 ))) except Exception : return None return w , h","title":"get_terminal_size()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.is_dynamite_member","text":"Check if a user is a member of the dynamite group Parameters: Name Type Description Default user str A username required Returns: Type Description bool True, if the user is a member of the dynamite group Source code in dynamite_nsm/utilities.py def is_dynamite_member ( user : str ) -> bool : \"\"\" Check if a user is a member of the dynamite group Args: user: A username Returns: True, if the user is a member of the dynamite group \"\"\" group = grp . getgrnam ( 'dynamite' ) return user in group [ 3 ]","title":"is_dynamite_member()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.is_root","text":"Determine whether or not the current user is root Returns: Type Description bool True, if the user is root Source code in dynamite_nsm/utilities.py def is_root () -> bool : \"\"\"Determine whether or not the current user is root Args: Returns: True, if the user is root \"\"\" return os . getuid () == 0","title":"is_root()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.is_setup","text":"Check if DynamiteNSM has required directories created. Returns: Type Description bool True if setup properly Source code in dynamite_nsm/utilities.py def is_setup () -> bool : \"\"\"Check if DynamiteNSM has required directories created. Returns: True if setup properly \"\"\" if not os . path . exists ( const . CONFIG_PATH ): return False elif not os . path . exists ( const . INSTALL_PATH ): return False elif not os . path . exists ( const . LOG_PATH ): return False return True","title":"is_setup()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.list_backup_configurations","text":"List available backup files in the configuration_backup_directory Parameters: Name Type Description Default configuration_backup_directory str The destination configuration directory backup directory required Returns: Type Description List[Dict] A list of dictionaries, where each dictionary contains a filename representing the name of the backup and a UNIX timestamp. Source code in dynamite_nsm/utilities.py def list_backup_configurations ( configuration_backup_directory : str ) -> List [ Dict ]: \"\"\"List available backup files in the configuration_backup_directory Args: configuration_backup_directory: The destination configuration directory backup directory Returns: A list of dictionaries, where each dictionary contains a filename representing the name of the backup and a UNIX timestamp. \"\"\" backups = [] digits_only_re = re . compile ( \"^([\\s\\d]+)$\" ) try : for conf in os . listdir ( configuration_backup_directory ): timestampstr = conf . split ( '.' )[ - 1 ] if not digits_only_re . match ( timestampstr ): confpath = os . path . join ( configuration_backup_directory , conf ) if os . path . isdir ( confpath ): for subconf in os . listdir ( confpath ): timestampstr = subconf . split ( '.' )[ - 1 ] if digits_only_re . match ( timestampstr ): backups . append ( { 'filename' : subconf , 'filepath' : os . path . join ( confpath , subconf ), 'time' : float ( timestampstr ) } ) else : # file is not a dir, and does not match expected format with timestamp. skip it. continue else : backups . append ( { 'filename' : conf , 'filepath' : os . path . join ( configuration_backup_directory , conf ), 'time' : float ( timestampstr ) } ) except FileNotFoundError : return backups backups . sort ( key = lambda item : item [ 'time' ], reverse = True ) return backups","title":"list_backup_configurations()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.makedirs","text":"Create directory(ies) at a given path Parameters: Name Type Description Default path str The path to the directories required exist_ok Optional[bool] If it exists, create anyway (Default value = True) True Returns: Type Description None None Source code in dynamite_nsm/utilities.py def makedirs ( path : str , exist_ok : Optional [ bool ] = True ) -> None : \"\"\"Create directory(ies) at a given path Args: path: The path to the directories exist_ok: If it exists, create anyway (Default value = True) Returns: None \"\"\" if exist_ok : os . makedirs ( path , exist_ok = True ) else : os . makedirs ( path )","title":"makedirs()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.print_coffee_art","text":"Print the dynamite logo! Returns: Type Description None None Source code in dynamite_nsm/utilities.py def print_coffee_art () -> None : \"\"\"Print the dynamite logo! Args: Returns: None \"\"\" try : coffee_icon = \\ \"\"\" \u256d\u256f\u256d\u256f\u256d\u256f \u2588\u2593\u2593\u2593\u2593\u2593\u2588\u2550\u256e \u2588\u2593\u2593\u2593\u2593\u2593\u2588\u258f\ufe31 \u2588\u2593\u2593\u2593\u2593\u2593\u2588\u2550\u256f \u25e5\u2588\u2588\u2588\u2588\u2588\u25e4 ~~~ \"Have a cup of coffee while you wait.\" ~~~ \"\"\" print ( coffee_icon ) print ( ' \\n ' ) except ( SyntaxError , UnicodeEncodeError ): # Your operating system is super lame :( pass","title":"print_coffee_art()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.print_dynamite_lab_art","text":"Print the dynamite lab ascii art Returns: Type Description None None Source code in dynamite_nsm/utilities.py def print_dynamite_lab_art () -> None : \"\"\"Print the dynamite lab ascii art Args: Returns: None \"\"\" try : lab_art = \\ \"\"\" \\033[0;36m _ | | DynamiteLab | / \\ is an experimental | / \\ feature. | (_____) Happy Hacking! | ~The Dynamite Team~ \\033[0m \"\"\" print ( lab_art ) except ( SyntaxError , UnicodeEncodeError ): print ()","title":"print_dynamite_lab_art()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.print_dynamite_logo","text":"Print the dynamite logo! Returns: Type Description None None Source code in dynamite_nsm/utilities.py def print_dynamite_logo ( version : str ) -> None : \"\"\"Print the dynamite logo! Args: Returns: None \"\"\" try : dynamite_logo = \\ \"\"\" \\033[0;36m ,,,,, ,\u2584\u2584\u2584\u2584\u2553 .\u2584\u2593\u2580\u2580\u2580\u2591\u2580\u2580\u2580\u2593\u2593\u2553 \u2554\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2580\u2593 #\u2229\u2553 \u2580\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2584\u2580\u2593\u2584 \u2551\u258c\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2569\u2593 \u2580\u2593\"\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2229\u2593\u2584 ,,\u2584\u2584\u2584\u2593\u2593\u2593\u258c\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2566\u2593 \u2590\u2593\u2559\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2590\u2593\u2580\u2580\u2580^\u2559\u2514\"^^\u2580\u2593\u2580\u2593\u2593\u2593\u2593\u2580\u2592\u2593` \u2590\u2593]\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2590\u2593 \u2580\u2580\u2580\u2580\u2580\u2580^ \u2584\u2593.\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u00dc\u2593\u2580 \u2559\u2559\u2580\u2588\u2592\u2584\u2584,, '#\u03b5\u2559\u2559\u2584\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2580\u2584\u2593\u258c `\u2559\u2580\u2580\u2593\u2593\u2593\u2593\u2584\u2584\u2553, ,, \"\u2588\u2593\u2584\u2584\u2593\u2593\u2593\u2591\u2584\u2593\u2593\u2580 \u2559\u2557, '\"\u2580\u2580\u2588\u2593\u2593\u2593\u2593\u2593\u2584#\u2563\u2593\u2593\u2593\u2593 \u2551\u2580\"\u2580\u2580\u2514, \u2580\u2593\u2584 ^\u2580\u2580\u2580\u258c\u2593\u2593\u2593\u2593\u2593\u255b \u2554\u2593 \u2593 \u2580\u2593\u2584,\u2553\u2553, \u2559\u2580\u2580\u2580\" ]\u2593\u258c \u2559\u258c '\u2593\u2593\u2593\u2593\u2593\u2593\u2310 \u2553\u2584\u2584\u2593\u2593\u2591 \u2593\u258c \u256b\u2593\u2593\u2593\u2593\u2593> \u2553\u2593\u2580\u2593\u2593\u2593\u2593\u2593\u2580\u2593 \u2559\u2593\u258c \u2559\u2559\u2580\u2559 \u2554\u2593\u2592\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2591\u2593 \u2551\u2593\u2555 \u255a\u258c\u2551\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2569\u2593 \u2593\u2593 \u2580\u2593\u2580\u2593\u2593\u2593\u2593\u2580\u2560\u2593\u2518 \u255a\u2593\u2593 \u2580\u2580\u2588\u2588\u2580\u2580\u2559 \u2593\u2593\u2593\u2553 \u256b\u2593\u2593\u2593\u2593\u2593\u03b5 \\033[0m http://dynamite.ai Version: {} \"\"\" . format ( version ) print ( dynamite_logo ) print ( ' \\n ' ) except ( SyntaxError , UnicodeEncodeError ): print ( 'http://dynamite.ai \\n\\n Version: {} \\n ' . format ( version ))","title":"print_dynamite_logo()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.prompt_input","text":"Taking in input Parameters: Name Type Description Default message str The message appearing next to the input prompt. required valid_responses Optional[List] A list of expected responses None Returns: Type Description str The inputted text Source code in dynamite_nsm/utilities.py def prompt_input ( message : str , valid_responses : Optional [ List ] = None ) -> str : \"\"\"Taking in input Args: message: The message appearing next to the input prompt. valid_responses: A list of expected responses Returns: The inputted text \"\"\" res = input ( message ) if valid_responses : while str ( res ) . strip () not in [ str ( r ) for r in valid_responses ]: print ( f 'Please enter a valid value: { valid_responses } ' ) res = input ( message ) return res","title":"prompt_input()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.prompt_password","text":"Prompt user for password, and confirm Parameters: Name Type Description Default prompt The first password prompt '[?] Enter a secure password: ' confirm_prompt The confirmation prompt '[?] Confirm Password: ' Returns: Type Description str The password entered Source code in dynamite_nsm/utilities.py def prompt_password ( prompt = '[?] Enter a secure password: ' , confirm_prompt = '[?] Confirm Password: ' ) -> str : \"\"\"Prompt user for password, and confirm Args: prompt: The first password prompt confirm_prompt: The confirmation prompt Returns: The password entered \"\"\" password = '0' confirm_password = '1' first_attempt = True valid = False while password != confirm_password or len ( password ) < 6 or not valid : if not first_attempt : sys . stderr . write ( '[-] Passwords either did not match or were less than 6 characters. Please try again. \\n\\n ' ) sys . stderr . flush () elif '\"' in password or \"'\" in password : sys . stderr . write ( '[-] Passwords cannot contain quote characters. Please try again. \\n\\n ' ) sys . stderr . flush () else : valid = True password = getpass . getpass ( prompt ) confirm_password = getpass . getpass ( confirm_prompt ) first_attempt = False return password","title":"prompt_password()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.restore_backup_configuration","text":"Restore a backup file to a configuration folder of choice Parameters: Name Type Description Default configuration_backup_filepath str The full path to the backup file required config_filepath str The full path to the to-be-restored configuration file required Returns: Type Description bool True, if successful Source code in dynamite_nsm/utilities.py def restore_backup_configuration ( configuration_backup_filepath : str , config_filepath : str ) -> bool : \"\"\"Restore a backup file to a configuration folder of choice Args: configuration_backup_filepath: The full path to the backup file config_filepath: The full path to the to-be-restored configuration file Returns: True, if successful \"\"\" try : shutil . move ( configuration_backup_filepath , config_filepath ) return True except shutil . Error : return False except FileNotFoundError : return False","title":"restore_backup_configuration()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.run_subprocess_with_status","text":"Run a subprocess inside a wrapper, that hides the output, and replaces with a progressbar Parameters: Name Type Description Default process Popen The subprocess.Popen instance required expected_lines Optional[int] The number of stdout lines to expect None Returns: Type Description int The exit code. Source code in dynamite_nsm/utilities.py def run_subprocess_with_status ( process : subprocess . Popen , expected_lines : Optional [ int ] = None ) -> int : \"\"\"Run a subprocess inside a wrapper, that hides the output, and replaces with a progressbar Args: process: The subprocess.Popen instance expected_lines: The number of stdout lines to expect Returns: The exit code. \"\"\" i = 0 widgets = [ ' \\033 [92m' , ' {} ' . format ( datetime . strftime ( datetime . utcnow (), '%Y-%m- %d %H:%M:%S' )), ' \\033 [0m' , ' \\033 [0;36m' , 'PROCESS_TRACKER ' , ' \\033 [0m' , ' | ' , progressbar . Percentage (), ' ' , progressbar . Bar (), ' ' , progressbar . FormatLabel ( '' ), ' ' , progressbar . ETA () ] over_max_value = False pb = progressbar . ProgressBar ( widgets = widgets , maxval = expected_lines ) pb . start () while True : output = process . stdout . readline () . decode () if output == '' and process . poll () is not None : break if output : i += 1 try : if not over_max_value : widgets [ 11 ] = '< {0} ...>' . format ( str ( output ) . replace ( ' \\n ' , '' ) . replace ( ' \\t ' , '' )[ 0 : 40 ]) pb . update ( i ) except ValueError : if not over_max_value : pb . finish () over_max_value = True # print(i, process.poll(), output) if not over_max_value : pb . finish () return process . poll ()","title":"run_subprocess_with_status()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.safely_remove_file","text":"Remove a file if it exists at the given path Parameters: Name Type Description Default path str The path of the file to remove required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def safely_remove_file ( path : str ) -> None : \"\"\"Remove a file if it exists at the given path Args: path: The path of the file to remove Returns: None \"\"\" if os . path . exists ( path ): os . remove ( path )","title":"safely_remove_file()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.set_ownership_of_file","text":"Set the ownership of a file given a user/group and a path Parameters: Name Type Description Default path str The path to the file required user Optional[str] The name of the user 'dynamite' group Optional[str] The group of the user 'dynamite' Returns: Type Description None None Source code in dynamite_nsm/utilities.py def set_ownership_of_file ( path : str , user : Optional [ str ] = 'dynamite' , group : Optional [ str ] = 'dynamite' ) -> None : \"\"\"Set the ownership of a file given a user/group and a path Args: path: The path to the file user: The name of the user group: The group of the user Returns: None \"\"\" uid = pwd . getpwnam ( user ) . pw_uid group = grp . getgrnam ( group ) . gr_gid os . chown ( path , uid , group ) for root , dirs , files in os . walk ( path ): for momo in dirs : os . chown ( os . path . join ( root , momo ), uid , group ) for momo in files : if momo == 'environment' : continue os . chown ( os . path . join ( root , momo ), uid , group )","title":"set_ownership_of_file()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.set_permissions_of_file","text":"Set the permissions of a file to unix_permissions_integer Parameters: Name Type Description Default file_path str The path to the file required unix_permissions_integer Union[str, int] The numeric representation of user/group/everyone permissions on a file required Returns: Type Description None None Source code in dynamite_nsm/utilities.py def set_permissions_of_file ( file_path : str , unix_permissions_integer : Union [ str , int ]) -> None : \"\"\"Set the permissions of a file to unix_permissions_integer Args: file_path: The path to the file unix_permissions_integer: The numeric representation of user/group/everyone permissions on a file Returns: None \"\"\" subprocess . call ( 'chmod -R {} {} ' . format ( unix_permissions_integer , file_path ), shell = True )","title":"set_permissions_of_file()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.test_bpf_filter","text":"Given a BPF expression determine if it is valid, and optionally return a message if not Parameters: Name Type Description Default expr str A valid Berkeley Packet Filter required include_message bool If True, Include an error message if expression is not valid. False Returns: Type Description Union[bool, Tuple[bool, str]] The result and optional result message Source code in dynamite_nsm/utilities.py def test_bpf_filter ( expr : str , include_message : bool = False ) -> Union [ bool , Tuple [ bool , str ]]: \"\"\"Given a BPF expression determine if it is valid, and optionally return a message if not Args: expr: A valid Berkeley Packet Filter include_message: If True, Include an error message if expression is not valid. Returns: The result and optional result message \"\"\" bin_path = pkg_resources . resource_filename ( 'dynamite_nsm' , 'bin/bpf_validate' ) set_permissions_of_file ( bin_path , '+x' ) p = subprocess . Popen ([ bin_path ] + expr . split ( ' ' ), stdout = subprocess . PIPE ) output , _ = p . communicate () serialized_values = json . loads ( output ) if not include_message : return serialized_values [ 'success' ] else : return serialized_values [ 'success' ], serialized_values [ 'msg' ]","title":"test_bpf_filter()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.update_sysctl","text":"Updates the vm.max_map_count and fs.file-max count Parameters: Name Type Description Default verbose Optional[bool] Include output from system utilities False Returns: Type Description None None Source code in dynamite_nsm/utilities.py def update_sysctl ( verbose : Optional [ bool ] = False ) -> None : \"\"\"Updates the vm.max_map_count and fs.file-max count Args: verbose: Include output from system utilities Returns: None \"\"\" new_output = '' vm_found = False fs_found = False for line in open ( '/etc/sysctl.conf' ) . readlines (): if not line . startswith ( '#' ) and 'vm.max_map_count' in line : new_output += 'vm.max_map_count=262144 \\n ' vm_found = True elif not line . startswith ( '#' ) and 'fs.file-max' in line : new_output += 'fs.file-max=65535 \\n ' fs_found = True else : new_output += line . strip () + ' \\n ' if not vm_found : new_output += 'vm.max_map_count=262144 \\n ' if not fs_found : new_output += 'fs.file-max=65535 \\n ' with open ( '/etc/sysctl.conf' , 'w' ) as f : f . write ( new_output ) if verbose : subprocess . call ( 'sysctl -w vm.max_map_count=262144' , shell = True ) subprocess . call ( 'sysctl -w fs.file-max=65535' , shell = True ) subprocess . call ( 'sysctl -p' , shell = True ) else : subprocess . call ( 'sysctl -w vm.max_map_count=262144' , shell = True , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) subprocess . call ( 'sysctl -w fs.file-max=65535' , shell = True , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) subprocess . call ( 'sysctl -p' , shell = True , stderr = subprocess . PIPE , stdout = subprocess . PIPE )","title":"update_sysctl()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.update_user_file_handle_limits","text":"Updates the max number of file handles the dynamite user can have open Returns: Type Description None None Source code in dynamite_nsm/utilities.py def update_user_file_handle_limits () -> None : \"\"\"Updates the max number of file handles the dynamite user can have open Args: Returns: None \"\"\" new_output = '' limit_found = False for line in open ( '/etc/security/limits.conf' ) . readlines (): if line . startswith ( 'dynamite' ): new_output += 'dynamite - nofile 65535' limit_found = True else : new_output += line . strip () new_output += ' \\n ' if not limit_found : new_output += ' \\n dynamite - nofile 65535 \\n ' with open ( '/etc/security/limits.conf' , 'w' ) as f : f . write ( new_output )","title":"update_user_file_handle_limits()"},{"location":"guides/developers/SDK/utilities/#dynamite_nsm.utilities.wrap_text","text":"Given a string adds newlines based on the current size of the terminal window (if one is found) Parameters: Name Type Description Default s str A string required Returns: Type Description str A new line delaminated string Source code in dynamite_nsm/utilities.py def wrap_text ( s : str ) -> str : \"\"\"Given a string adds newlines based on the current size of the terminal window (if one is found) Args: s: A string Returns: A new line delaminated string \"\"\" if not s : return \"\" term_dim = get_terminal_size () if not term_dim : w , h = 150 , 90 else : w , h = term_dim wrapped_s = ' \\n ' . join ( textwrap . wrap ( s , w - 40 , fix_sentence_endings = True )) return wrapped_s","title":"wrap_text()"},{"location":"guides/developers/SDK/cmd/base_interface/","text":"Contains abstract base interface classes and helpers. To import... from dynamite_nsm.cmd import base_interface BaseInterface An abstract interface used primarily in instance checks __init__ ( self , interface_name = None , interface_description = None , defaults = None ) special Setup the interface Parameters: Name Type Description Default interface_name Optional[str] A descriptive name of the interface None interface_description Optional[str] A description of what the utility does None defaults Optional[Dict] Any arguments and their value you wish to default (E.G stdout=True) None Source code in dynamite_nsm/cmd/base_interface.py def __init__ ( self , interface_name : Optional [ str ] = None , interface_description : Optional [ str ] = None , defaults : Optional [ Dict ] = None ): \"\"\" Setup the interface Args: interface_name: A descriptive name of the interface interface_description: A description of what the utility does defaults: Any arguments and their value you wish to default (E.G stdout=True) \"\"\" self . interface_name = interface_name self . interface_description = interface_description self . defaults = defaults if not self . defaults : self . defaults = dict () execute ( self , args ) Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/base_interface.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the parsed arguments and execute using the proper `service.action` class; can return any value. Args: args: `argparse.Namespace` created by a method such as `argparse.ArgumentParser().parse_args()` Returns: Can return any value; completely depends on the `service.action` being invoked \"\"\" raise NotImplementedError () get_parser ( self ) Returns: an ArgumentParser instance Source code in dynamite_nsm/cmd/base_interface.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\" Returns: an `ArgumentParser` instance \"\"\" raise NotImplementedError ()","title":"base_interface"},{"location":"guides/developers/SDK/cmd/base_interface/#dynamite_nsm.cmd.base_interface.BaseInterface","text":"An abstract interface used primarily in instance checks","title":"BaseInterface"},{"location":"guides/developers/SDK/cmd/base_interface/#dynamite_nsm.cmd.base_interface.BaseInterface.__init__","text":"Setup the interface Parameters: Name Type Description Default interface_name Optional[str] A descriptive name of the interface None interface_description Optional[str] A description of what the utility does None defaults Optional[Dict] Any arguments and their value you wish to default (E.G stdout=True) None Source code in dynamite_nsm/cmd/base_interface.py def __init__ ( self , interface_name : Optional [ str ] = None , interface_description : Optional [ str ] = None , defaults : Optional [ Dict ] = None ): \"\"\" Setup the interface Args: interface_name: A descriptive name of the interface interface_description: A description of what the utility does defaults: Any arguments and their value you wish to default (E.G stdout=True) \"\"\" self . interface_name = interface_name self . interface_description = interface_description self . defaults = defaults if not self . defaults : self . defaults = dict ()","title":"__init__()"},{"location":"guides/developers/SDK/cmd/base_interface/#dynamite_nsm.cmd.base_interface.BaseInterface.execute","text":"Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/base_interface.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the parsed arguments and execute using the proper `service.action` class; can return any value. Args: args: `argparse.Namespace` created by a method such as `argparse.ArgumentParser().parse_args()` Returns: Can return any value; completely depends on the `service.action` being invoked \"\"\" raise NotImplementedError ()","title":"execute()"},{"location":"guides/developers/SDK/cmd/base_interface/#dynamite_nsm.cmd.base_interface.BaseInterface.get_parser","text":"Returns: an ArgumentParser instance Source code in dynamite_nsm/cmd/base_interface.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\" Returns: an `ArgumentParser` instance \"\"\" raise NotImplementedError ()","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/","text":"Contains several classes capable of converting configuration objects found within ConfigManager instances into commandline utilities. To import... from dynamite_nsm.cmd import config_object_interfaces AnalyzersInterface Convert any generic.Analyzers derived class into a commandline utility (E.G Zeek scripts, signatures, and redefs as well as Suricata rule-sets __init__ ( self , config_obj ) special Setup the interface Parameters: Name Type Description Default config_obj generic.Analyzers A complex config object that contains one or more Analyzers required Source code in dynamite_nsm/cmd/config_object_interfaces.py def __init__ ( self , config_obj : generic . Analyzers ): \"\"\" Setup the interface Args: config_obj: A complex config object that contains one or more `Analyzers` \"\"\" super () . __init__ () self . config_obj = config_obj self . changed_rows = [] build_parser ( interface , parser ) staticmethod Build a parser from any AnalysisInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface AnalyzersInterface The AnalyzerInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance combined with the instantiated AnalyzersInterface derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : AnalyzersInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\" Build a parser from any `AnalysisInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `AnalyzerInterface` instance you wish to append Returns: An argument parser instance combined with the instantiated `AnalyzersInterface` derived class \"\"\" choices = [] for analyzer in interface . config_obj . analyzers : choices . append ( analyzer . id ) parser . add_argument ( '--ids' , dest = 'analyzer_ids' , nargs = '+' , type = str , default = [], help = 'Specify one or more ids for the config object you want to work with.' ) parser . add_argument ( '--enable' , dest = 'enable' , action = 'store_true' , help = f 'Enable selected object.' ) parser . add_argument ( '--disable' , dest = 'disable' , action = 'store_true' , help = f 'Disable selected object' ) if getattr ( interface . config_obj . analyzers [ 0 ], 'value' , None ): parser . add_argument ( '--value' , dest = 'value' , type = str , help = 'The value associated with the selected object' ) return parser execute ( self , args ) Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Type Description Any Any value; completely depends on the parent interface's ConfigManager.commit method Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: `argparse.Namespace` created by a method such as `argparse.ArgumentParser().parse_args()` Returns: Any value; completely depends on the parent interface's `ConfigManager.commit` method \"\"\" self . changed_rows = [] headers = [ 'Id' , 'Name' , 'Enabled' , 'Value' ] table = [ headers ] selected_items = [] for analyzer in self . config_obj . analyzers : analyzer_value = 'N/A' if getattr ( analyzer , 'value' , None ): analyzer_value = analyzer . value row = [ analyzer . id , analyzer . name , analyzer . enabled , analyzer_value ] table . append ( row ) if analyzer . id in list ( args . analyzer_ids ): selected_analyzer = analyzer if selected_analyzer : if args . disable : selected_analyzer . enabled = False elif args . enable : selected_analyzer . enabled = True if getattr ( args , 'value' , None ): if not str ( args . value ) . endswith ( ';' ): args . value = args . value + ';' selected_analyzer . value = args . value selected_items . append ([ selected_analyzer . id , selected_analyzer . name , selected_analyzer . enabled , selected_analyzer . value if selected_analyzer . value else 'N/A' ]) if not args . analyzer_ids : all_analyzers = [] for analyzer in self . config_obj . analyzers : all_analyzers . append ( [ analyzer . id , analyzer . name , analyzer . enabled , analyzer . value if analyzer . value else 'N/A' ]) return tabulate ( headers = headers , tabular_data = all_analyzers , tablefmt = 'fancy_grid' ) else : self . changed_rows = selected_items return self . config_obj get_parser ( self ) Get the current interface as an argparse.ArgumentParser instance Returns: Type Description argparse.ArgumentParser An argparse.ArgumentParser instance for the instantiated AnalyzerInterface derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Get the current interface as an `argparse.ArgumentParser` instance Returns: An argparse.ArgumentParser instance for the instantiated `AnalyzerInterface` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser ) FilebeatTargetsInterface Convert any Filebeat BaseTargets derived class into a commandline utility. __init__ ( self , config_obj , defaults = None ) special Setup the interface Parameters: Name Type Description Default config_obj targets.BaseTargets A Filebeat specific complex config object that specifies a downstream target config - where required defaults Optional[Dict] Any default commandline arguments you wish to define ahead of time dict(arg_name=arg_value) None Source code in dynamite_nsm/cmd/config_object_interfaces.py def __init__ ( self , config_obj : targets . BaseTargets , defaults : Optional [ Dict ] = None ): \"\"\" Setup the interface Args: config_obj: A Filebeat specific complex config object that specifies a downstream target config - where to send logs. defaults: Any default commandline arguments you wish to define ahead of time `dict(arg_name=arg_value)` \"\"\" super () . __init__ ( defaults = defaults ) self . config_obj = config_obj self . changed_rows = [] build_parser ( interface , parser ) staticmethod Build a parser from any BaseTargets and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface FilebeatTargetsInterface The FilebeatTargetsInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : FilebeatTargetsInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `BaseTargets` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `FilebeatTargetsInterface` instance you wish to append Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" target_options = parser . add_argument_group ( 'target options' ) for var in vars ( interface . config_obj ): args = ArgparseParameters . create_from_typing_annotation ( var , type ( getattr ( interface . config_obj , var )), required = False ) if var == 'enabled' : continue arg_description = interface . _get_description_for_instance_var ( var ) . replace ( ' \\n ' , ' ' ) args . add_description ( arg_description ) try : target_options . add_argument ( * args . flags , ** args . kwargs ) except argparse . ArgumentError : continue target_options . add_argument ( '--enable' , dest = 'enable' , action = 'store_true' , help = f 'Enable selected target.' ) target_options . add_argument ( '--disable' , dest = 'disable' , action = 'store_true' , help = f 'Disable selected target' ) return parser execute ( self , args ) Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Type Description Any Any value; completely depends on the parent interface's ConfigManager.commit method Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: `argparse.Namespace` created by a method such as `argparse.ArgumentParser().parse_args()` Returns: Any value; completely depends on the parent interface's `ConfigManager.commit` method \"\"\" self . changed_rows = [] headers = [ 'Config Option' , 'Value' ] table = [] for option , value in args . __dict__ . items (): if option in [ 'enable' , 'disable' ]: continue if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if not value : config_value = ( option . replace ( '_' , '-' ), getattr ( self . config_obj , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' table . append ( config_value ) else : self . changed_rows . append ([ option . replace ( '_' , '-' ), value ]) setattr ( self . config_obj , option , value ) if args . enable : self . changed_rows . append ([ 'enabled' , True ]) self . config_obj . enabled = True elif args . disable : self . changed_rows . append ([ 'enabled' , False ]) self . config_obj . enabled = False if not self . changed_rows : table . append ([ 'enabled' , self . config_obj . enabled ]) return tabulate ( table , tablefmt = 'fancy_grid' , headers = headers ) return self . config_obj get_parser ( self ) For the given interface return an argparse.ArgumentParser object for a Filebeat BaseTargets object Returns: Type Description An argument parser instance for the instantiated FilebeatTargetsInterface derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Filebeat `BaseTargets` object Returns: An argument parser instance for the instantiated `FilebeatTargetsInterface` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser ) SuricataInterfaceConfigObjectsInterface build_parser ( interface , parser ) staticmethod Build a parser from a node.AfPacketInterfaces and argparse.ArgumentParser derived class Parameters: Name Type Description Default interface SuricataInterfaceConfigObjectsInterface The SuricataInterfaceConfigObjectsInterface instance you wish to append required parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required Returns: Type Description An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : SuricataInterfaceConfigObjectsInterface , parser : argparse . ArgumentParser ): \"\"\"Build a parser from a `node.AfPacketInterfaces` and `argparse.ArgumentParser` derived class Args: interface: The `SuricataInterfaceConfigObjectsInterface` instance you wish to append parser: The `argparse.ArgumentParser` instance that you want to add a new parser too Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" component_select = parser . add_argument_group ( 'component editor' ) component_select . add_argument ( '--select' , dest = 'select' , help = 'The interface you want to modify.' ) component_select . add_argument ( '--interface' , dest = 'inspect_interface' , required = False , help = 'A new inspection interface to monitor.' ) component_select . add_argument ( '--bpf-filter' , dest = 'bpf_filter' , required = False , help = 'A filter that can be used to drop packets before they are analyzed.' ) return parser execute ( self , args ) Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : headers = [ 'Interface Name' , 'BPF Filter' ] changed_values_table = [] changed_value_headers = [ 'Interface Name' , 'Config Option' , 'Value' ] component_summaries = [( component . interface , component . bpf_filter ) for component in self . config_obj . interfaces ] if not args . select : return tabulate ( headers = headers , tabular_data = component_summaries , tablefmt = 'fancy_grid' ) else : modified_component = self . config_obj . get ( args . select ) if not modified_component : return None for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if option not in [ 'bpf_filter' , 'inspect_interface' ]: continue if option == 'inspect_interface' : option = 'interface' if not value : config_value = ( args . select , option . replace ( '_' , '-' ), getattr ( modified_component , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' changed_values_table . append ( config_value ) else : self . changed_rows . append ([ args . select , option . replace ( '_' , '-' ), value ]) setattr ( modified_component , option , value ) if not self . changed_rows : return tabulate ( changed_values_table , headers = changed_value_headers , tablefmt = 'fancy_grid' ) return self . config_obj get_parser ( self ) For the given interface return an argparse.ArgumentParser object for a Zeek node.BaseComponents object Returns: Type Description An argument parser instance for the instantiated node.BaseComponents derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Zeek `node.BaseComponents` object Returns: An argument parser instance for the instantiated `node.BaseComponents` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser ) ZeekNodeConfigObjectInterface Convert any Zeek node BaseComponent derived class into a commandline utility. build_parser ( interface , parser ) staticmethod Build a parser from any node.BaseComponent and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface ZeekNodeConfigObjectInterface The ZeekNodeConfigObjectInterface instance you wish to append required Returns: Type Description An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : ZeekNodeConfigObjectInterface , parser : argparse . ArgumentParser ): \"\"\"Build a parser from any `node.BaseComponent` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `ZeekNodeConfigObjectInterface` instance you wish to append Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" node_component_options = parser . add_argument_group ( 'component options' ) for var in vars ( interface . config_obj ): args = ArgparseParameters . create_from_typing_annotation ( var , type ( getattr ( interface . config_obj , var )), required = False ) if var == 'type' : continue arg_description = interface . _get_description_for_instance_var ( var ) . replace ( ' \\n ' , ' ' ) args . add_description ( arg_description ) try : node_component_options . add_argument ( * args . flags , ** args . kwargs ) except argparse . ArgumentError : continue return parser execute ( self , args ) Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : self . changed_rows = [] headers = [ 'Config Option' , 'Value' ] table = [] for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if not value : config_value = ( option . replace ( '_' , '-' ), getattr ( self . config_obj , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' table . append ( config_value ) else : self . changed_rows . append ([ option . replace ( '_' , '-' ), value ]) setattr ( self . config_obj , option , value ) if not self . changed_rows : return tabulate ( table , tablefmt = 'fancy_grid' , headers = headers ) return self . config_obj get_parser ( self ) For the given interface return an argparse.ArgumentParser object for a Zeek node.BaseComponent object Returns: Type Description An argument parser instance for the instantiated node.BaseComponent derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Zeek `node.BaseComponent` object Returns: An argument parser instance for the instantiated `node.BaseComponent` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser ) ZeekNodeConfigObjectsInterface build_parser ( interface , parser ) staticmethod Build a parser from any node.BaseComponents and argparse.ArgumentParser derived class Parameters: Name Type Description Default interface ZeekNodeConfigObjectsInterface The ZeekNodeConfigObjectsInterface instance you wish to append required parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required Returns: Type Description An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : ZeekNodeConfigObjectsInterface , parser : argparse . ArgumentParser ): \"\"\"Build a parser from any `node.BaseComponents` and `argparse.ArgumentParser` derived class Args: interface: The `ZeekNodeConfigObjectsInterface` instance you wish to append parser: The `argparse.ArgumentParser` instance that you want to add a new parser too Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" component_select = parser . add_argument_group ( 'component editor' ) component_select . add_argument ( '--select' , dest = 'select' , help = 'The name of the component you want to modify.' ) component_select . add_argument ( '--name' , dest = 'name' , required = False , help = 'A new name for the component.' ) component_select . add_argument ( '--host' , dest = 'host' , required = False , help = 'A new host or ip address for this component.' ) # Dirty hack to add worker specific arguments if 'workers' in sys . argv : component_select . add_argument ( '--interface' , dest = 'inspect_interface' , required = False , help = 'A new inspection interface for the selected worker.' ) return parser execute ( self , args ) Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : headers = [ 'Component Name' , 'Host' ] changed_values_table = [] changed_value_headers = [ 'Component Name' , 'Config Option' , 'Value' ] component_summaries = [( component . name , component . host ) for component in self . config_obj . components ] if isinstance ( self . config_obj , zeek_node_config . Workers ): headers = [ 'Component Name' , 'Host' , 'Inspection Interface' ] component_summaries = [( component . name , component . host , component . interface ) for component in self . config_obj . components ] if not args . select : return tabulate ( headers = headers , tabular_data = component_summaries , tablefmt = 'fancy_grid' ) else : modified_component = self . config_obj . get ( args . select ) if not modified_component : return None for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if option == 'select' : continue if option == 'inspect_interface' : option = 'interface' if not value : config_value = ( args . select , option . replace ( '_' , '-' ), getattr ( modified_component , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' changed_values_table . append ( config_value ) else : self . changed_rows . append ([ args . select , option . replace ( '_' , '-' ), value ]) setattr ( modified_component , option , value ) if not self . changed_rows : return tabulate ( changed_values_table , headers = changed_value_headers , tablefmt = 'fancy_grid' ) return self . config_obj get_parser ( self ) For the given interface return an argparse.ArgumentParser object for a Zeek node.BaseComponents object Returns: Type Description An argument parser instance for the instantiated node.BaseComponents derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Zeek `node.BaseComponents` object Returns: An argument parser instance for the instantiated `node.BaseComponents` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser ) append_config_object_analyzer_interface_to_parser ( parser , interface ) Append an AnalyzersInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface AnalyzersInterface The AnalyzerInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_analyzer_interface_to_parser ( parser : argparse . ArgumentParser , interface : AnalyzersInterface ) -> argparse . ArgumentParser : \"\"\"Append an `AnalyzersInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `AnalyzerInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser ) append_config_object_filebeat_targets_to_parser ( parser , interface ) Append an FilebeatTargetsInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface FilebeatTargetsInterface The FilebeatTargetsInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_filebeat_targets_to_parser ( parser : argparse . ArgumentParser , interface : FilebeatTargetsInterface ) -> argparse . ArgumentParser : \"\"\"Append an `FilebeatTargetsInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `FilebeatTargetsInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser ) append_config_object_suricata_interface_obj_to_parser ( parser , interface ) Append an SuricataInterfaceConfigObjectsInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface SuricataInterfaceConfigObjectsInterface The SuricataInterfaceConfigObjectsInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_suricata_interface_obj_to_parser ( parser : argparse . ArgumentParser , interface : SuricataInterfaceConfigObjectsInterface ) -> \\ argparse . ArgumentParser : \"\"\"Append an `SuricataInterfaceConfigObjectsInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `SuricataInterfaceConfigObjectsInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser ) append_config_object_zeek_node_obj_to_parser ( parser , interface ) Append an ZeekNodeConfigObjectInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface ZeekNodeConfigObjectInterface The ZeekNodeConfigObjectInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_zeek_node_obj_to_parser ( parser : argparse . ArgumentParser , interface : ZeekNodeConfigObjectInterface ) -> argparse . ArgumentParser : \"\"\"Append an `ZeekNodeConfigObjectInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `ZeekNodeConfigObjectInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser ) append_config_object_zeek_node_objs_to_parser ( parser , interface ) Append an ZeekNodeConfigObjectsInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface ZeekNodeConfigObjectsInterface The ZeekNodeConfigObjectInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_zeek_node_objs_to_parser ( parser : argparse . ArgumentParser , interface : ZeekNodeConfigObjectsInterface ) -> argparse . ArgumentParser : \"\"\"Append an `ZeekNodeConfigObjectsInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `ZeekNodeConfigObjectInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser )","title":"config_object_interfaces"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.AnalyzersInterface","text":"Convert any generic.Analyzers derived class into a commandline utility (E.G Zeek scripts, signatures, and redefs as well as Suricata rule-sets","title":"AnalyzersInterface"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.AnalyzersInterface.__init__","text":"Setup the interface Parameters: Name Type Description Default config_obj generic.Analyzers A complex config object that contains one or more Analyzers required Source code in dynamite_nsm/cmd/config_object_interfaces.py def __init__ ( self , config_obj : generic . Analyzers ): \"\"\" Setup the interface Args: config_obj: A complex config object that contains one or more `Analyzers` \"\"\" super () . __init__ () self . config_obj = config_obj self . changed_rows = []","title":"__init__()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.AnalyzersInterface.build_parser","text":"Build a parser from any AnalysisInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface AnalyzersInterface The AnalyzerInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance combined with the instantiated AnalyzersInterface derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : AnalyzersInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\" Build a parser from any `AnalysisInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `AnalyzerInterface` instance you wish to append Returns: An argument parser instance combined with the instantiated `AnalyzersInterface` derived class \"\"\" choices = [] for analyzer in interface . config_obj . analyzers : choices . append ( analyzer . id ) parser . add_argument ( '--ids' , dest = 'analyzer_ids' , nargs = '+' , type = str , default = [], help = 'Specify one or more ids for the config object you want to work with.' ) parser . add_argument ( '--enable' , dest = 'enable' , action = 'store_true' , help = f 'Enable selected object.' ) parser . add_argument ( '--disable' , dest = 'disable' , action = 'store_true' , help = f 'Disable selected object' ) if getattr ( interface . config_obj . analyzers [ 0 ], 'value' , None ): parser . add_argument ( '--value' , dest = 'value' , type = str , help = 'The value associated with the selected object' ) return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.AnalyzersInterface.execute","text":"Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Type Description Any Any value; completely depends on the parent interface's ConfigManager.commit method Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: `argparse.Namespace` created by a method such as `argparse.ArgumentParser().parse_args()` Returns: Any value; completely depends on the parent interface's `ConfigManager.commit` method \"\"\" self . changed_rows = [] headers = [ 'Id' , 'Name' , 'Enabled' , 'Value' ] table = [ headers ] selected_items = [] for analyzer in self . config_obj . analyzers : analyzer_value = 'N/A' if getattr ( analyzer , 'value' , None ): analyzer_value = analyzer . value row = [ analyzer . id , analyzer . name , analyzer . enabled , analyzer_value ] table . append ( row ) if analyzer . id in list ( args . analyzer_ids ): selected_analyzer = analyzer if selected_analyzer : if args . disable : selected_analyzer . enabled = False elif args . enable : selected_analyzer . enabled = True if getattr ( args , 'value' , None ): if not str ( args . value ) . endswith ( ';' ): args . value = args . value + ';' selected_analyzer . value = args . value selected_items . append ([ selected_analyzer . id , selected_analyzer . name , selected_analyzer . enabled , selected_analyzer . value if selected_analyzer . value else 'N/A' ]) if not args . analyzer_ids : all_analyzers = [] for analyzer in self . config_obj . analyzers : all_analyzers . append ( [ analyzer . id , analyzer . name , analyzer . enabled , analyzer . value if analyzer . value else 'N/A' ]) return tabulate ( headers = headers , tabular_data = all_analyzers , tablefmt = 'fancy_grid' ) else : self . changed_rows = selected_items return self . config_obj","title":"execute()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.AnalyzersInterface.get_parser","text":"Get the current interface as an argparse.ArgumentParser instance Returns: Type Description argparse.ArgumentParser An argparse.ArgumentParser instance for the instantiated AnalyzerInterface derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Get the current interface as an `argparse.ArgumentParser` instance Returns: An argparse.ArgumentParser instance for the instantiated `AnalyzerInterface` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.FilebeatTargetsInterface","text":"Convert any Filebeat BaseTargets derived class into a commandline utility.","title":"FilebeatTargetsInterface"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.FilebeatTargetsInterface.__init__","text":"Setup the interface Parameters: Name Type Description Default config_obj targets.BaseTargets A Filebeat specific complex config object that specifies a downstream target config - where required defaults Optional[Dict] Any default commandline arguments you wish to define ahead of time dict(arg_name=arg_value) None Source code in dynamite_nsm/cmd/config_object_interfaces.py def __init__ ( self , config_obj : targets . BaseTargets , defaults : Optional [ Dict ] = None ): \"\"\" Setup the interface Args: config_obj: A Filebeat specific complex config object that specifies a downstream target config - where to send logs. defaults: Any default commandline arguments you wish to define ahead of time `dict(arg_name=arg_value)` \"\"\" super () . __init__ ( defaults = defaults ) self . config_obj = config_obj self . changed_rows = []","title":"__init__()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.FilebeatTargetsInterface.build_parser","text":"Build a parser from any BaseTargets and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface FilebeatTargetsInterface The FilebeatTargetsInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : FilebeatTargetsInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `BaseTargets` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `FilebeatTargetsInterface` instance you wish to append Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" target_options = parser . add_argument_group ( 'target options' ) for var in vars ( interface . config_obj ): args = ArgparseParameters . create_from_typing_annotation ( var , type ( getattr ( interface . config_obj , var )), required = False ) if var == 'enabled' : continue arg_description = interface . _get_description_for_instance_var ( var ) . replace ( ' \\n ' , ' ' ) args . add_description ( arg_description ) try : target_options . add_argument ( * args . flags , ** args . kwargs ) except argparse . ArgumentError : continue target_options . add_argument ( '--enable' , dest = 'enable' , action = 'store_true' , help = f 'Enable selected target.' ) target_options . add_argument ( '--disable' , dest = 'disable' , action = 'store_true' , help = f 'Disable selected target' ) return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.FilebeatTargetsInterface.execute","text":"Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Type Description Any Any value; completely depends on the parent interface's ConfigManager.commit method Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: `argparse.Namespace` created by a method such as `argparse.ArgumentParser().parse_args()` Returns: Any value; completely depends on the parent interface's `ConfigManager.commit` method \"\"\" self . changed_rows = [] headers = [ 'Config Option' , 'Value' ] table = [] for option , value in args . __dict__ . items (): if option in [ 'enable' , 'disable' ]: continue if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if not value : config_value = ( option . replace ( '_' , '-' ), getattr ( self . config_obj , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' table . append ( config_value ) else : self . changed_rows . append ([ option . replace ( '_' , '-' ), value ]) setattr ( self . config_obj , option , value ) if args . enable : self . changed_rows . append ([ 'enabled' , True ]) self . config_obj . enabled = True elif args . disable : self . changed_rows . append ([ 'enabled' , False ]) self . config_obj . enabled = False if not self . changed_rows : table . append ([ 'enabled' , self . config_obj . enabled ]) return tabulate ( table , tablefmt = 'fancy_grid' , headers = headers ) return self . config_obj","title":"execute()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.FilebeatTargetsInterface.get_parser","text":"For the given interface return an argparse.ArgumentParser object for a Filebeat BaseTargets object Returns: Type Description An argument parser instance for the instantiated FilebeatTargetsInterface derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Filebeat `BaseTargets` object Returns: An argument parser instance for the instantiated `FilebeatTargetsInterface` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.SuricataInterfaceConfigObjectsInterface","text":"","title":"SuricataInterfaceConfigObjectsInterface"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.SuricataInterfaceConfigObjectsInterface.build_parser","text":"Build a parser from a node.AfPacketInterfaces and argparse.ArgumentParser derived class Parameters: Name Type Description Default interface SuricataInterfaceConfigObjectsInterface The SuricataInterfaceConfigObjectsInterface instance you wish to append required parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required Returns: Type Description An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : SuricataInterfaceConfigObjectsInterface , parser : argparse . ArgumentParser ): \"\"\"Build a parser from a `node.AfPacketInterfaces` and `argparse.ArgumentParser` derived class Args: interface: The `SuricataInterfaceConfigObjectsInterface` instance you wish to append parser: The `argparse.ArgumentParser` instance that you want to add a new parser too Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" component_select = parser . add_argument_group ( 'component editor' ) component_select . add_argument ( '--select' , dest = 'select' , help = 'The interface you want to modify.' ) component_select . add_argument ( '--interface' , dest = 'inspect_interface' , required = False , help = 'A new inspection interface to monitor.' ) component_select . add_argument ( '--bpf-filter' , dest = 'bpf_filter' , required = False , help = 'A filter that can be used to drop packets before they are analyzed.' ) return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.SuricataInterfaceConfigObjectsInterface.execute","text":"Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : headers = [ 'Interface Name' , 'BPF Filter' ] changed_values_table = [] changed_value_headers = [ 'Interface Name' , 'Config Option' , 'Value' ] component_summaries = [( component . interface , component . bpf_filter ) for component in self . config_obj . interfaces ] if not args . select : return tabulate ( headers = headers , tabular_data = component_summaries , tablefmt = 'fancy_grid' ) else : modified_component = self . config_obj . get ( args . select ) if not modified_component : return None for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if option not in [ 'bpf_filter' , 'inspect_interface' ]: continue if option == 'inspect_interface' : option = 'interface' if not value : config_value = ( args . select , option . replace ( '_' , '-' ), getattr ( modified_component , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' changed_values_table . append ( config_value ) else : self . changed_rows . append ([ args . select , option . replace ( '_' , '-' ), value ]) setattr ( modified_component , option , value ) if not self . changed_rows : return tabulate ( changed_values_table , headers = changed_value_headers , tablefmt = 'fancy_grid' ) return self . config_obj","title":"execute()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.SuricataInterfaceConfigObjectsInterface.get_parser","text":"For the given interface return an argparse.ArgumentParser object for a Zeek node.BaseComponents object Returns: Type Description An argument parser instance for the instantiated node.BaseComponents derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Zeek `node.BaseComponents` object Returns: An argument parser instance for the instantiated `node.BaseComponents` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectInterface","text":"Convert any Zeek node BaseComponent derived class into a commandline utility.","title":"ZeekNodeConfigObjectInterface"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectInterface.build_parser","text":"Build a parser from any node.BaseComponent and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface ZeekNodeConfigObjectInterface The ZeekNodeConfigObjectInterface instance you wish to append required Returns: Type Description An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : ZeekNodeConfigObjectInterface , parser : argparse . ArgumentParser ): \"\"\"Build a parser from any `node.BaseComponent` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `ZeekNodeConfigObjectInterface` instance you wish to append Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" node_component_options = parser . add_argument_group ( 'component options' ) for var in vars ( interface . config_obj ): args = ArgparseParameters . create_from_typing_annotation ( var , type ( getattr ( interface . config_obj , var )), required = False ) if var == 'type' : continue arg_description = interface . _get_description_for_instance_var ( var ) . replace ( ' \\n ' , ' ' ) args . add_description ( arg_description ) try : node_component_options . add_argument ( * args . flags , ** args . kwargs ) except argparse . ArgumentError : continue return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectInterface.execute","text":"Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : self . changed_rows = [] headers = [ 'Config Option' , 'Value' ] table = [] for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if not value : config_value = ( option . replace ( '_' , '-' ), getattr ( self . config_obj , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' table . append ( config_value ) else : self . changed_rows . append ([ option . replace ( '_' , '-' ), value ]) setattr ( self . config_obj , option , value ) if not self . changed_rows : return tabulate ( table , tablefmt = 'fancy_grid' , headers = headers ) return self . config_obj","title":"execute()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectInterface.get_parser","text":"For the given interface return an argparse.ArgumentParser object for a Zeek node.BaseComponent object Returns: Type Description An argument parser instance for the instantiated node.BaseComponent derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Zeek `node.BaseComponent` object Returns: An argument parser instance for the instantiated `node.BaseComponent` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectsInterface","text":"","title":"ZeekNodeConfigObjectsInterface"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectsInterface.build_parser","text":"Build a parser from any node.BaseComponents and argparse.ArgumentParser derived class Parameters: Name Type Description Default interface ZeekNodeConfigObjectsInterface The ZeekNodeConfigObjectsInterface instance you wish to append required parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required Returns: Type Description An argument parser instance for the instantiated BaseTargets derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py @staticmethod def build_parser ( interface : ZeekNodeConfigObjectsInterface , parser : argparse . ArgumentParser ): \"\"\"Build a parser from any `node.BaseComponents` and `argparse.ArgumentParser` derived class Args: interface: The `ZeekNodeConfigObjectsInterface` instance you wish to append parser: The `argparse.ArgumentParser` instance that you want to add a new parser too Returns: An argument parser instance for the instantiated `BaseTargets` derived class \"\"\" component_select = parser . add_argument_group ( 'component editor' ) component_select . add_argument ( '--select' , dest = 'select' , help = 'The name of the component you want to modify.' ) component_select . add_argument ( '--name' , dest = 'name' , required = False , help = 'A new name for the component.' ) component_select . add_argument ( '--host' , dest = 'host' , required = False , help = 'A new host or ip address for this component.' ) # Dirty hack to add worker specific arguments if 'workers' in sys . argv : component_select . add_argument ( '--interface' , dest = 'inspect_interface' , required = False , help = 'A new inspection interface for the selected worker.' ) return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectsInterface.execute","text":"Interpret the parsed arguments and execute using the proper service.action class; can return any value. Parameters: Name Type Description Default args argparse.Namespace argparse.Namespace created by a method such as argparse.ArgumentParser().parse_args() required Returns: Can return any value; completely depends on the service.action being invoked Source code in dynamite_nsm/cmd/config_object_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : headers = [ 'Component Name' , 'Host' ] changed_values_table = [] changed_value_headers = [ 'Component Name' , 'Config Option' , 'Value' ] component_summaries = [( component . name , component . host ) for component in self . config_obj . components ] if isinstance ( self . config_obj , zeek_node_config . Workers ): headers = [ 'Component Name' , 'Host' , 'Inspection Interface' ] component_summaries = [( component . name , component . host , component . interface ) for component in self . config_obj . components ] if not args . select : return tabulate ( headers = headers , tabular_data = component_summaries , tablefmt = 'fancy_grid' ) else : modified_component = self . config_obj . get ( args . select ) if not modified_component : return None for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if option == 'select' : continue if option == 'inspect_interface' : option = 'interface' if not value : config_value = ( args . select , option . replace ( '_' , '-' ), getattr ( modified_component , option , None )) if not config_value [ 1 ]: config_value = option . replace ( '_' , '-' ), 'N/A' changed_values_table . append ( config_value ) else : self . changed_rows . append ([ args . select , option . replace ( '_' , '-' ), value ]) setattr ( modified_component , option , value ) if not self . changed_rows : return tabulate ( changed_values_table , headers = changed_value_headers , tablefmt = 'fancy_grid' ) return self . config_obj","title":"execute()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.ZeekNodeConfigObjectsInterface.get_parser","text":"For the given interface return an argparse.ArgumentParser object for a Zeek node.BaseComponents object Returns: Type Description An argument parser instance for the instantiated node.BaseComponents derived class Source code in dynamite_nsm/cmd/config_object_interfaces.py def get_parser ( self ): \"\"\" For the given interface return an `argparse.ArgumentParser` object for a Zeek `node.BaseComponents` object Returns: An argument parser instance for the instantiated `node.BaseComponents` derived class \"\"\" parser = argparse . ArgumentParser () return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.append_config_object_analyzer_interface_to_parser","text":"Append an AnalyzersInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface AnalyzersInterface The AnalyzerInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_analyzer_interface_to_parser ( parser : argparse . ArgumentParser , interface : AnalyzersInterface ) -> argparse . ArgumentParser : \"\"\"Append an `AnalyzersInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `AnalyzerInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_config_object_analyzer_interface_to_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.append_config_object_filebeat_targets_to_parser","text":"Append an FilebeatTargetsInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface FilebeatTargetsInterface The FilebeatTargetsInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_filebeat_targets_to_parser ( parser : argparse . ArgumentParser , interface : FilebeatTargetsInterface ) -> argparse . ArgumentParser : \"\"\"Append an `FilebeatTargetsInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `FilebeatTargetsInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_config_object_filebeat_targets_to_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.append_config_object_suricata_interface_obj_to_parser","text":"Append an SuricataInterfaceConfigObjectsInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface SuricataInterfaceConfigObjectsInterface The SuricataInterfaceConfigObjectsInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_suricata_interface_obj_to_parser ( parser : argparse . ArgumentParser , interface : SuricataInterfaceConfigObjectsInterface ) -> \\ argparse . ArgumentParser : \"\"\"Append an `SuricataInterfaceConfigObjectsInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `SuricataInterfaceConfigObjectsInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_config_object_suricata_interface_obj_to_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.append_config_object_zeek_node_obj_to_parser","text":"Append an ZeekNodeConfigObjectInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface ZeekNodeConfigObjectInterface The ZeekNodeConfigObjectInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_zeek_node_obj_to_parser ( parser : argparse . ArgumentParser , interface : ZeekNodeConfigObjectInterface ) -> argparse . ArgumentParser : \"\"\"Append an `ZeekNodeConfigObjectInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `ZeekNodeConfigObjectInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_config_object_zeek_node_obj_to_parser()"},{"location":"guides/developers/SDK/cmd/config_object_interfaces/#dynamite_nsm.cmd.config_object_interfaces.append_config_object_zeek_node_objs_to_parser","text":"Append an ZeekNodeConfigObjectsInterface interface into an existing parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface ZeekNodeConfigObjectsInterface The ZeekNodeConfigObjectInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser The modified parser Source code in dynamite_nsm/cmd/config_object_interfaces.py def append_config_object_zeek_node_objs_to_parser ( parser : argparse . ArgumentParser , interface : ZeekNodeConfigObjectsInterface ) -> argparse . ArgumentParser : \"\"\"Append an `ZeekNodeConfigObjectsInterface` interface into an existing parser. Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `ZeekNodeConfigObjectInterface` instance you wish to append Returns: The modified parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_config_object_zeek_node_objs_to_parser()"},{"location":"guides/developers/SDK/cmd/inspection_helpers/","text":"Contains a variety of functions for inspecting classes and class method data. To import... from dynamite_nsm.cmd import inspection_helpers ArgparseParameters Represent the **kwargs that can be provided to the argparse.ArgumentParser class __init__ ( self , name , ** kwargs ) special Setup from a dictionary Parameters: Name Type Description Default name The name of a commandline parameter (E.G setup, stdout, verbose, any_func_name) required kwargs A list of kwargs accepted by argparse.ArgumentParser.add_argument method {} Source code in dynamite_nsm/cmd/inspection_helpers.py def __init__ ( self , name , ** kwargs ): \"\"\"Setup from a dictionary Args: name: The name of a commandline parameter (E.G setup, stdout, verbose, any_func_name) kwargs: A list of kwargs accepted by argparse.ArgumentParser.add_argument method \"\"\" self . name = name self . flags = [ '--' + self . name . replace ( '_' , '-' )] self . kwargs = kwargs create_from_typing_annotation ( name , python_type , default = None , required = True ) classmethod Convenience method for creating argparse parameters from a python Parameters: Name Type Description Default name str The name of the commandline parameter required python_type type The datatype that best describes the parameter required default Optional[Any] The default value for the parameter being evaluated None required Optional[bool] If True, argparse will interpret this argument as required True Returns: Type Description None Source code in dynamite_nsm/cmd/inspection_helpers.py @classmethod def create_from_typing_annotation ( cls , name : str , python_type : type , default : Optional [ Any ] = None , required : Optional [ bool ] = True ): \"\"\"Convenience method for creating argparse parameters from a python <class type> Args: name: The name of the commandline parameter python_type: The datatype that best describes the parameter default: The default value for the parameter being evaluated required: If True, argparse will interpret this argument as required Returns: None \"\"\" return cls ( name , ** cls . derive_params_from_type_annotation ( python_type , default = default , required = required )) derive_params_from_type_annotation ( python_type , default = None , required = True ) staticmethod Convert from a typing annotation string to an argparse.ArgumentParser type Parameters: Name Type Description Default python_type Any A or typing derived class required default Optional[Any] The default value for the parameter being evaluated None required Optional[bool] If true, the required parameter will be added to the parameter dictionary True Returns: Type Description Dict A dictionary of supported **kwargs used to instantiate an argparse.ArgumentParser object Source code in dynamite_nsm/cmd/inspection_helpers.py @staticmethod def derive_params_from_type_annotation ( python_type : Any , default : Optional [ Any ] = None , required : Optional [ bool ] = True ) -> Dict : \"\"\"Convert from a typing annotation string to an `argparse.ArgumentParser` `type` Args: python_type: A <class 'type'> or typing derived class default: The default value for the parameter being evaluated required: If true, the `required` parameter will be added to the parameter dictionary Returns: A dictionary of supported **kwargs used to instantiate an `argparse.ArgumentParser` object \"\"\" python_type = str ( python_type ) action , default , nargs = None , default , None _type = str if default : required = False if 'Union' in python_type and 'NoneType' in python_type : required = False if 'Optional' in python_type : required = False if 'List' in python_type : nargs = '+' if 'list' in python_type : nargs = '+' if 'bool' in python_type : action = 'store_true' _type = None elif 'int' in python_type : _type = int elif 'float' in python_type : _type = float elif 'str' in python_type : _type = str derived_args = dict ( required = required , action = action , default = default , nargs = nargs , type = _type ) derived_args = { k : v for k , v in derived_args . items () if v is not None and v != '' } return derived_args get_argparse_parameters ( func_def , defaults ) Given a callable function returns a list of argparse compatible arguments Parameters: Name Type Description Default func_def Tuple[str, dict, str] A tuple containing the function.__name__ , function.__annotations__ , inspect.getdoc(function) required defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default it too. required Returns: Type Description List[dynamite_nsm.cmd.inspection_helpers.ArgparseParameters] A list of ArgparseParameters Source code in dynamite_nsm/cmd/inspection_helpers.py def get_argparse_parameters ( func_def : Tuple [ str , dict , str ], defaults : Optional [ Dict ]) -> List [ ArgparseParameters ]: \"\"\"Given a callable function returns a list of argparse compatible arguments Args: func_def: A tuple containing the `function.__name__`, `function.__annotations__`, `inspect.getdoc(function)` defaults: A dictionary where the key a parameter name and the value represents the value to default it too. Returns: A list of `ArgparseParameters` \"\"\" argparse_parameter_group = [] param_map = {} _ , annotations , docs = func_def try : docstring_params = docstring_parse ( docs ) . params except ValueError as e : newline_delim_doc_str = ' \\\\ n' . join ( docs . split ( ' \\n ' )) raise Exception ( f 'Docs: { newline_delim_doc_str } failed to parse: { e } likely because this docstring has a ' f 'newline in it somewhere.' ) for doc_param in docstring_params : _ , arg_name = doc_param . args if '***' in doc_param . description : split_token = '***' elif '---' in docstring_params : split_token = '---' else : split_token = '___' # If an explicit line break is detected in our docstrings we don't parse parameters passed that line break. param_map [ arg_name ] = doc_param . description . split ( split_token )[ 0 ] for param_name , data_type in annotations . items (): argparse_params = ArgparseParameters . create_from_typing_annotation ( name = param_name , python_type = data_type ) if param_name == 'return' : continue if defaults and defaults . get ( param_name ): argparse_params = ArgparseParameters . create_from_typing_annotation ( name = param_name , python_type = data_type , default = defaults . get ( param_name )) try : argparse_params . add_description ( param_map [ param_name ]) except KeyError : pass argparse_parameter_group . append ( argparse_params ) return argparse_parameter_group get_class_instance_methods ( cls , defaults = None , use_parent_init = True ) Given a class retrieves all the methods with their corresponding parameters Parameters: Name Type Description Default cls object The class that you wish to enumerate required use_parent_init Optional[bool] If True, the parent class' init arguments will be scanned as well True defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default it too. None Returns: Type Description Tuple[List[dynamite_nsm.cmd.inspection_helpers.ArgparseParameters], Dict[str, List[dynamite_nsm.cmd.inspection_helpers.ArgparseParameters]]] A tuple containing the base_params for the init method in the first position; and a dictionary containing a map of remaining function names to lists of their corresponding parameters (E.G {func_name [ArgparseParameters, ArgparseParam...], func_name_2 [ArgparseParameters, Argp...]} ) Source code in dynamite_nsm/cmd/inspection_helpers.py def get_class_instance_methods ( cls : object , defaults : Optional [ Dict ] = None , use_parent_init : Optional [ bool ] = True ) -> \\ Tuple [ List [ ArgparseParameters ], Dict [ str , List [ ArgparseParameters ]]]: \"\"\"Given a class retrieves all the methods with their corresponding parameters Args: cls: The class that you wish to enumerate use_parent_init: If True, the parent class' init arguments will be scanned as well defaults: A dictionary where the key a parameter name and the value represents the value to default it too. Returns: A tuple containing the base_params for the __init__ method in the first position; and a dictionary containing a map of remaining function names to lists of their corresponding parameters (E.G `{func_name [ArgparseParameters, ArgparseParam...], func_name_2 [ArgparseParameters, Argp...]}`) \"\"\" interface_functions = {} # Enumerate the class instance methods as well as any parent classes instance methods try : method_resolution_order = cls . __mro__ except AttributeError : method_resolution_order = cls . __class__ . __mro__ for c in method_resolution_order : for callable in c . __dict__ . values (): func_def = get_function_definition ( callable ) if not func_def : continue else : # func_name, annotations, docs func_name , _ , _ = func_def if func_name == '__init__' : continue # Store the rest of our method parameters in a dictionary # {func_name: [ArgparseParameters, ArgparseParam...], func_name_2: [ArgparseParameters, Argp...]} else : # Look first in the top level class then parent classes if func_name not in interface_functions . keys (): interface_functions [ func_name ] = get_argparse_parameters ( func_def , defaults = defaults ) # and parent class is selected try : parent_class = method_resolution_order [ 1 ] except IndexError : parent_class = cls if use_parent_init : func_def = get_function_definition ( parent_class . __init__ ) else : func_def = get_function_definition ( cls . __init__ ) base_params = get_argparse_parameters ( func_def , defaults = defaults ) return base_params , interface_functions get_function_definition ( func ) Given a callable function returns a three part definition for that function Parameters: Name Type Description Default func Callable A callable function required Returns: Type Description Optional[Tuple[str, dict, str]] A tuple with the ( function.__name__ , function.__annotations__ , inspect.getdoc(function) ) Source code in dynamite_nsm/cmd/inspection_helpers.py def get_function_definition ( func : Callable ) -> Union [ Tuple [ str , dict , str ], None ]: \"\"\"Given a callable function returns a three part definition for that function Args: func: A callable function Returns: A tuple with the (`function.__name__`, `function.__annotations__`, `inspect.getdoc(function)`) \"\"\" if not isinstance ( func , Callable ): return None if func . __name__ == '__init__' : name = '__init__' annotations = dict ( inspect . signature ( func ) . parameters . items ()) annotations . pop ( 'self' , None ) docs = inspect . getdoc ( func ) else : try : name , annotations = func . __name__ , func . __annotations__ docs = inspect . getdoc ( func ) except AttributeError : return None return name , annotations , docs","title":"inspection_helpers"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.ArgparseParameters","text":"Represent the **kwargs that can be provided to the argparse.ArgumentParser class","title":"ArgparseParameters"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.ArgparseParameters.__init__","text":"Setup from a dictionary Parameters: Name Type Description Default name The name of a commandline parameter (E.G setup, stdout, verbose, any_func_name) required kwargs A list of kwargs accepted by argparse.ArgumentParser.add_argument method {} Source code in dynamite_nsm/cmd/inspection_helpers.py def __init__ ( self , name , ** kwargs ): \"\"\"Setup from a dictionary Args: name: The name of a commandline parameter (E.G setup, stdout, verbose, any_func_name) kwargs: A list of kwargs accepted by argparse.ArgumentParser.add_argument method \"\"\" self . name = name self . flags = [ '--' + self . name . replace ( '_' , '-' )] self . kwargs = kwargs","title":"__init__()"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.ArgparseParameters.create_from_typing_annotation","text":"Convenience method for creating argparse parameters from a python Parameters: Name Type Description Default name str The name of the commandline parameter required python_type type The datatype that best describes the parameter required default Optional[Any] The default value for the parameter being evaluated None required Optional[bool] If True, argparse will interpret this argument as required True Returns: Type Description None Source code in dynamite_nsm/cmd/inspection_helpers.py @classmethod def create_from_typing_annotation ( cls , name : str , python_type : type , default : Optional [ Any ] = None , required : Optional [ bool ] = True ): \"\"\"Convenience method for creating argparse parameters from a python <class type> Args: name: The name of the commandline parameter python_type: The datatype that best describes the parameter default: The default value for the parameter being evaluated required: If True, argparse will interpret this argument as required Returns: None \"\"\" return cls ( name , ** cls . derive_params_from_type_annotation ( python_type , default = default , required = required ))","title":"create_from_typing_annotation()"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.ArgparseParameters.derive_params_from_type_annotation","text":"Convert from a typing annotation string to an argparse.ArgumentParser type Parameters: Name Type Description Default python_type Any A or typing derived class required default Optional[Any] The default value for the parameter being evaluated None required Optional[bool] If true, the required parameter will be added to the parameter dictionary True Returns: Type Description Dict A dictionary of supported **kwargs used to instantiate an argparse.ArgumentParser object Source code in dynamite_nsm/cmd/inspection_helpers.py @staticmethod def derive_params_from_type_annotation ( python_type : Any , default : Optional [ Any ] = None , required : Optional [ bool ] = True ) -> Dict : \"\"\"Convert from a typing annotation string to an `argparse.ArgumentParser` `type` Args: python_type: A <class 'type'> or typing derived class default: The default value for the parameter being evaluated required: If true, the `required` parameter will be added to the parameter dictionary Returns: A dictionary of supported **kwargs used to instantiate an `argparse.ArgumentParser` object \"\"\" python_type = str ( python_type ) action , default , nargs = None , default , None _type = str if default : required = False if 'Union' in python_type and 'NoneType' in python_type : required = False if 'Optional' in python_type : required = False if 'List' in python_type : nargs = '+' if 'list' in python_type : nargs = '+' if 'bool' in python_type : action = 'store_true' _type = None elif 'int' in python_type : _type = int elif 'float' in python_type : _type = float elif 'str' in python_type : _type = str derived_args = dict ( required = required , action = action , default = default , nargs = nargs , type = _type ) derived_args = { k : v for k , v in derived_args . items () if v is not None and v != '' } return derived_args","title":"derive_params_from_type_annotation()"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.get_argparse_parameters","text":"Given a callable function returns a list of argparse compatible arguments Parameters: Name Type Description Default func_def Tuple[str, dict, str] A tuple containing the function.__name__ , function.__annotations__ , inspect.getdoc(function) required defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default it too. required Returns: Type Description List[dynamite_nsm.cmd.inspection_helpers.ArgparseParameters] A list of ArgparseParameters Source code in dynamite_nsm/cmd/inspection_helpers.py def get_argparse_parameters ( func_def : Tuple [ str , dict , str ], defaults : Optional [ Dict ]) -> List [ ArgparseParameters ]: \"\"\"Given a callable function returns a list of argparse compatible arguments Args: func_def: A tuple containing the `function.__name__`, `function.__annotations__`, `inspect.getdoc(function)` defaults: A dictionary where the key a parameter name and the value represents the value to default it too. Returns: A list of `ArgparseParameters` \"\"\" argparse_parameter_group = [] param_map = {} _ , annotations , docs = func_def try : docstring_params = docstring_parse ( docs ) . params except ValueError as e : newline_delim_doc_str = ' \\\\ n' . join ( docs . split ( ' \\n ' )) raise Exception ( f 'Docs: { newline_delim_doc_str } failed to parse: { e } likely because this docstring has a ' f 'newline in it somewhere.' ) for doc_param in docstring_params : _ , arg_name = doc_param . args if '***' in doc_param . description : split_token = '***' elif '---' in docstring_params : split_token = '---' else : split_token = '___' # If an explicit line break is detected in our docstrings we don't parse parameters passed that line break. param_map [ arg_name ] = doc_param . description . split ( split_token )[ 0 ] for param_name , data_type in annotations . items (): argparse_params = ArgparseParameters . create_from_typing_annotation ( name = param_name , python_type = data_type ) if param_name == 'return' : continue if defaults and defaults . get ( param_name ): argparse_params = ArgparseParameters . create_from_typing_annotation ( name = param_name , python_type = data_type , default = defaults . get ( param_name )) try : argparse_params . add_description ( param_map [ param_name ]) except KeyError : pass argparse_parameter_group . append ( argparse_params ) return argparse_parameter_group","title":"get_argparse_parameters()"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.get_class_instance_methods","text":"Given a class retrieves all the methods with their corresponding parameters Parameters: Name Type Description Default cls object The class that you wish to enumerate required use_parent_init Optional[bool] If True, the parent class' init arguments will be scanned as well True defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default it too. None Returns: Type Description Tuple[List[dynamite_nsm.cmd.inspection_helpers.ArgparseParameters], Dict[str, List[dynamite_nsm.cmd.inspection_helpers.ArgparseParameters]]] A tuple containing the base_params for the init method in the first position; and a dictionary containing a map of remaining function names to lists of their corresponding parameters (E.G {func_name [ArgparseParameters, ArgparseParam...], func_name_2 [ArgparseParameters, Argp...]} ) Source code in dynamite_nsm/cmd/inspection_helpers.py def get_class_instance_methods ( cls : object , defaults : Optional [ Dict ] = None , use_parent_init : Optional [ bool ] = True ) -> \\ Tuple [ List [ ArgparseParameters ], Dict [ str , List [ ArgparseParameters ]]]: \"\"\"Given a class retrieves all the methods with their corresponding parameters Args: cls: The class that you wish to enumerate use_parent_init: If True, the parent class' init arguments will be scanned as well defaults: A dictionary where the key a parameter name and the value represents the value to default it too. Returns: A tuple containing the base_params for the __init__ method in the first position; and a dictionary containing a map of remaining function names to lists of their corresponding parameters (E.G `{func_name [ArgparseParameters, ArgparseParam...], func_name_2 [ArgparseParameters, Argp...]}`) \"\"\" interface_functions = {} # Enumerate the class instance methods as well as any parent classes instance methods try : method_resolution_order = cls . __mro__ except AttributeError : method_resolution_order = cls . __class__ . __mro__ for c in method_resolution_order : for callable in c . __dict__ . values (): func_def = get_function_definition ( callable ) if not func_def : continue else : # func_name, annotations, docs func_name , _ , _ = func_def if func_name == '__init__' : continue # Store the rest of our method parameters in a dictionary # {func_name: [ArgparseParameters, ArgparseParam...], func_name_2: [ArgparseParameters, Argp...]} else : # Look first in the top level class then parent classes if func_name not in interface_functions . keys (): interface_functions [ func_name ] = get_argparse_parameters ( func_def , defaults = defaults ) # and parent class is selected try : parent_class = method_resolution_order [ 1 ] except IndexError : parent_class = cls if use_parent_init : func_def = get_function_definition ( parent_class . __init__ ) else : func_def = get_function_definition ( cls . __init__ ) base_params = get_argparse_parameters ( func_def , defaults = defaults ) return base_params , interface_functions","title":"get_class_instance_methods()"},{"location":"guides/developers/SDK/cmd/inspection_helpers/#dynamite_nsm.cmd.inspection_helpers.get_function_definition","text":"Given a callable function returns a three part definition for that function Parameters: Name Type Description Default func Callable A callable function required Returns: Type Description Optional[Tuple[str, dict, str]] A tuple with the ( function.__name__ , function.__annotations__ , inspect.getdoc(function) ) Source code in dynamite_nsm/cmd/inspection_helpers.py def get_function_definition ( func : Callable ) -> Union [ Tuple [ str , dict , str ], None ]: \"\"\"Given a callable function returns a three part definition for that function Args: func: A callable function Returns: A tuple with the (`function.__name__`, `function.__annotations__`, `inspect.getdoc(function)`) \"\"\" if not isinstance ( func , Callable ): return None if func . __name__ == '__init__' : name = '__init__' annotations = dict ( inspect . signature ( func ) . parameters . items ()) annotations . pop ( 'self' , None ) docs = inspect . getdoc ( func ) else : try : name , annotations = func . __name__ , func . __annotations__ docs = inspect . getdoc ( func ) except AttributeError : return None return name , annotations , docs","title":"get_function_definition()"},{"location":"guides/developers/SDK/cmd/service_interfaces/","text":"Contains several classes capable of converting a variety of common design patterns into commandline utilities. To import... from dynamite_nsm.cmd import service_interfaces MultipleResponsibilityInterface Maps a class with several responsibilities to commandline interface For example ProcessManager's provides multiple methods that can be invoked to perform various tasks. MultipleResponsibilityInterface: Takes a single class and supported_method_names. Uses several introspection techniques to enumerate instance methods from that class Derives the **kwargs params for argparse.ArgumentParser.add_arguments method for the init , and selected exec_method Generates parser using annotation and docs Provide a method for executing the parsed argparse.Namespace against cls. init ( base_kwargs).{exec_method( interface_kwargs)} __init__ ( self , cls , supported_method_names , interface_name , interface_description = None , defaults = None ) special Initialize the interface Parameters: Name Type Description Default cls object The class we wish to turn into a commandline utility required supported_method_names List[str] A list of methods to create interfaces for required interface_name str The name of this commandline interface required interface_description Optional[str] A description of what this interface is supposed to do None defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default too. None Source code in dynamite_nsm/cmd/service_interfaces.py def __init__ ( self , cls : object , supported_method_names : List [ str ], interface_name : str , interface_description : Optional [ str ] = None , defaults : Optional [ Dict ] = None ): \"\"\"Initialize the interface Args: cls: The class we wish to turn into a commandline utility supported_method_names: A list of methods to create interfaces for interface_name: The name of this commandline interface interface_description: A description of what this interface is supposed to do defaults: A dictionary where the key a parameter name and the value represents the value to default too. \"\"\" super () . __init__ ( interface_name , interface_description , defaults = defaults ) self . cls = cls self . supported_method_names = supported_method_names if not interface_description : self . interfaceModuleType_description = inspect . getdoc ( cls ) self . base_params , self . interface_methods = get_class_instance_methods ( cls , defaults , use_parent_init = False ) # print(self.cls, [(item.name, item.flags, item.kwargs) for item in self.base_params]) build_parser ( interface , parser ) staticmethod Build a parser from any MultipleResponsibilityInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface MultipleResponsibilityInterface The MultipleResponsibilityInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the MultipleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py @staticmethod def build_parser ( interface : MultipleResponsibilityInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `MultipleResponsibilityInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `MultipleResponsibilityInterface` instance you wish to append Returns: An argument parser instance for the `MultipleResponsibilityInterface` derived class \"\"\" actions_subparsers = parser . add_subparsers () for method , params_group in interface . interface_methods . items (): if method in interface . supported_method_names : action_parser = actions_subparsers . add_parser ( method . replace ( '_' , '-' )) action_parser . set_defaults ( entry_method_name = method ) for params in interface . base_params : action_parser . add_argument ( * params . flags , ** params . kwargs ) for params in params_group : try : action_parser . add_argument ( * params . flags , ** params . kwargs ) except argparse . ArgumentError : continue return parser execute ( self , args ) Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description Any Any value; completely depends on the selected_method being invoked Source code in dynamite_nsm/cmd/service_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: Any value; completely depends on the `selected_method` being invoked \"\"\" constructor_kwargs = dict () entry_method_kwargs = dict () for param , value in vars ( args ) . items (): if param in [ p . name for p in self . base_params ]: constructor_kwargs [ param ] = value else : entry_method_kwargs [ param ] = value entry_method_kwargs . pop ( 'component' , None ) entry_method_kwargs . pop ( 'interface' , None ) entry_method_kwargs . pop ( 'sub_interface' , None ) entry_method_kwargs . pop ( 'entry_method_name' , None ) # Dynamically load our class klass = getattr ( self , 'cls' ) # Instantiate it with the constructor kwargs exec_inst = klass ( ** constructor_kwargs ) # Dynamically load our defined entry_method exec_entry_method = getattr ( exec_inst , args . entry_method_name ) # Call the entry method return exec_entry_method ( ** entry_method_kwargs ) get_parser ( self ) Get the current interface as an argparse.ArgumentParser instance Returns: Type Description argparse.ArgumentParser An argument parser instance for the MultipleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Get the current interface as an `argparse.ArgumentParser` instance Returns: An argument parser instance for the `MultipleResponsibilityInterface` derived class \"\"\" parser = argparse . ArgumentParser ( description = f ' { self . interface_name } - { self . interface_description } ' ) return self . build_parser ( self , parser ) SimpleConfigManagerInterface Based upon the SingleResponsibilityInterface maps a class with only one responsibility to commandline interface, but also makes all the instance variables of the configuration class available as commandline arguments __init__ ( self , config , interface_name , interface_description = None , pretty_print_status = True , defaults = None ) special Initialize the interface Parameters: Name Type Description Default config Union[config.GenericConfigManager, config.YamlConfigManager] The class we wish to turn into a commandline utility required interface_name str The name of this commandline interface required interface_description Optional[str] A description of what this interface is supposed to do None defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default too. None Source code in dynamite_nsm/cmd/service_interfaces.py def __init__ ( self , config : Union [ config . GenericConfigManager , config . YamlConfigManager ], interface_name : str , interface_description : Optional [ str ] = None , pretty_print_status : Optional [ bool ] = True , defaults : Optional [ Dict ] = None ): \"\"\"Initialize the interface Args: config: The class we wish to turn into a commandline utility interface_name: The name of this commandline interface interface_description: A description of what this interface is supposed to do defaults: A dictionary where the key a parameter name and the value represents the value to default too. \"\"\" self . config = config self . config_module_map = {} self . pretty_print_status = pretty_print_status super () . __init__ ( self . config . __class__ , 'commit' , interface_name , interface_description , defaults ) build_parser ( interface , parser ) staticmethod Build a parser from any SimpleConfigManagerInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface SimpleConfigManagerInterface The SimpleConfigManagerInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the SimpleConfigManagerInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py @staticmethod def build_parser ( interface : SimpleConfigManagerInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `SimpleConfigManagerInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `SimpleConfigManagerInterface` instance you wish to append Returns: An argument parser instance for the `SimpleConfigManagerInterface` derived class \"\"\" config_options = parser . add_argument_group ( 'configuration options' ) config_objects_subparser = parser . add_subparsers () for params in interface . base_params + interface . interface_params : parser . add_argument ( * params . flags , ** params . kwargs ) for var in vars ( interface . config ): if var in RESERVED_VARIABLE_NAMES : continue elif var . startswith ( '_' ): continue elif '_raw' in var : continue elif var in [ param . name for param in interface . base_params ]: continue elif 'config_objects' in str ( type ( getattr ( interface . config , var ))): complex_obj = getattr ( interface . config , var ) if isinstance ( complex_obj , Analyzers ): config_module_interface = AnalyzersInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , targets . BaseTargets ): config_module_interface = FilebeatTargetsInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , node . BaseComponent ): config_module_interface = ZeekNodeConfigObjectInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , node . BaseComponents ): config_module_interface = ZeekNodeConfigObjectsInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , misc . AfPacketInterfaces ): config_module_interface = SuricataInterfaceConfigObjectsInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) else : args = ArgparseParameters . create_from_typing_annotation ( var , type ( getattr ( interface . config , var )), required = False ) try : config_options . add_argument ( * args . flags , ** args . kwargs ) except argparse . ArgumentError : continue return parser execute ( self , args ) Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description Any Any value; depending on the value returned from the entry_method (usually a ConfigManager.commit ) Source code in dynamite_nsm/cmd/service_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: Any value; depending on the value returned from the `entry_method` (usually a `ConfigManager.commit`) \"\"\" changed_config = False if not getattr ( args , 'config_module' , None ): args . config_module = None headers = [ 'Config Option' , 'Value' ] table = [ headers ] changed_rows_only = [ headers ] # In the scenario we have configuration modules include them as config options in our display table table . extend ( [[ config_module_name , 'Configuration Module' ] for config_module_name in self . config_module_map . keys ()]) for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if not value : config_value = ( option , getattr ( self . config , option , None )) table . append ( config_value ) else : changed_config = True changed_config_value = ( option , value ) changed_rows_only . append ( changed_config_value ) setattr ( self . config , option , value ) if args . config_module : # Configuration module interfaces need to pass through relevant commandline defaults from parent interface self . config_module_map [ args . config_module ] . defaults = self . defaults selected_config_module = self . config_module_map [ args . config_module ] res = selected_config_module . execute ( args ) if isinstance ( res , Analyzers ): selected_analyzer_header = [ 'Id' , 'Name' , 'Enabled' , 'Value' ] setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = selected_analyzer_header , tablefmt = 'fancy_grid' ) elif isinstance ( res , targets . BaseTargets ): setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) elif isinstance ( res , node . BaseComponent ): setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) elif isinstance ( res , node . BaseComponents ): setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) elif isinstance ( res , misc . AfPacketInterfaces ): self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) else : return res else : if changed_config : self . config . commit () if self . pretty_print_status : return tabulate ( changed_rows_only , tablefmt = 'fancy_grid' ) return dict ( changed_rows_only ) else : if self . pretty_print_status : return tabulate ( table , tablefmt = 'fancy_grid' ) return dict ( table ) get_parser ( self ) Returns an argparse.ArgumentParser instance before parse_args has been called. Returns: Type Description argparse.ArgumentParser argparse.ArgumentParser instance Source code in dynamite_nsm/cmd/service_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Returns an argparse.ArgumentParser instance before parse_args has been called. Returns: argparse.ArgumentParser instance \"\"\" parser = super () . get_parser () return self . build_parser ( self , parser ) SingleResponsibilityInterface Maps a class with only one responsibility to commandline interface For example InstallManager's only need call one function (perform one responsibility) once instantiated. SingleResponsibilityInterface: Takes a single class and entry_method_name. Uses several introspection techniques to enumerate instance methods from that class Derives the **kwargs params for argparse.ArgumentParser.add_arguments method for the init , and entry_method Generates parser using annotation and docs Provide a method for executing the parsed argparse.Namespace against cls. init ( base_kwargs).{entry_method( interface_kwargs)} __init__ ( self , cls , entry_method_name , interface_name , interface_description = None , defaults = None ) special Initialize the interface Parameters: Name Type Description Default cls object The class we wish to turn into a commandline utility required entry_method_name str The name of the method inside the above class we wish to call at execution time required interface_name str The name of this commandline interface required interface_description Optional[str] A description of what this interface is supposed to do None defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default too. None Source code in dynamite_nsm/cmd/service_interfaces.py def __init__ ( self , cls : object , entry_method_name : str , interface_name : str , interface_description : Optional [ str ] = None , defaults : Optional [ Dict ] = None ): \"\"\"Initialize the interface Args: cls: The class we wish to turn into a commandline utility entry_method_name: The name of the method inside the above class we wish to call at execution time interface_name: The name of this commandline interface interface_description: A description of what this interface is supposed to do defaults: A dictionary where the key a parameter name and the value represents the value to default too. \"\"\" super () . __init__ ( interface_name , interface_description , defaults = defaults ) self . cls = cls self . entry_method_name = entry_method_name self . defaults = defaults if not self . defaults : self . defaults = dict () if not interface_description : self . interface_description = inspect . getdoc ( cls ) self . base_params , self . interface_methods = get_class_instance_methods ( cls , defaults , use_parent_init = False ) self . interface_params = self . interface_methods [ self . entry_method_name ] build_parser ( interface , parser ) staticmethod Build a parser from any SingleResponsibilityInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface SingleResponsibilityInterface The SingleResponsibilityInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the SingleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py @staticmethod def build_parser ( interface : SingleResponsibilityInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `SingleResponsibilityInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `SingleResponsibilityInterface` instance you wish to append Returns: An argument parser instance for the `SingleResponsibilityInterface` derived class \"\"\" for params in interface . base_params : parser . add_argument ( * params . flags , ** params . kwargs ) for params in interface . interface_params : parser . add_argument ( * params . flags , ** params . kwargs ) return parser execute ( self , args ) Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description Any Any value; depending on the value returned from the entry_method Source code in dynamite_nsm/cmd/service_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: Any value; depending on the value returned from the `entry_method` \"\"\" if getattr ( args , 'background' , None ): setattr ( args , 'background' , None ) return self . execute_in_background ( args ) constructor_kwargs = dict () entry_method_kwargs = dict () for param , value in vars ( args ) . items (): if param in [ p . name for p in self . base_params ]: constructor_kwargs [ param ] = value else : entry_method_kwargs [ param ] = value entry_method_kwargs . pop ( 'component' , None ) entry_method_kwargs . pop ( 'interface' , None ) entry_method_kwargs . pop ( 'sub_interface' , None ) entry_method_kwargs . pop ( 'background' , None ) # Dynamically load our class klass = getattr ( self , 'cls' ) # Instantiate it with the constructor kwargs exec_inst = klass ( ** constructor_kwargs ) # Dynamically load our defined entry_method exec_entry_method = getattr ( exec_inst , self . entry_method_name ) # Call the entry method return exec_entry_method ( ** entry_method_kwargs ) execute_in_background ( self , args ) Call execute, but run in the background inside a dedicated process. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description None None Source code in dynamite_nsm/cmd/service_interfaces.py def execute_in_background ( self , args : argparse . Namespace ) -> None : \"\"\"Call execute, but run in the background inside a dedicated process. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: None \"\"\" args . verbose = True args . stdout = False with daemon . DaemonContext (): self . execute ( args ) get_parser ( self ) Get the current interface as an argparse.ArgumentParser instance Returns: Type Description argparse.ArgumentParser An argument parser instance for the SingleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Get the current interface as an `argparse.ArgumentParser` instance Returns: An argument parser instance for the `SingleResponsibilityInterface` derived class \"\"\" parser = argparse . ArgumentParser ( description = f ' { self . interface_name } - { self . interface_description } ' ) return self . build_parser ( self , parser ) append_service_multiple_responsibility_interface_to_parser ( parser , interface ) Append an MultipleResponsibilityInterface to an existing parser as a sub-parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The parser to append our interface too required interface MultipleResponsibilityInterface The new interface to add to the parser required Returns: Type Description argparse.ArgumentParser A new parser Source code in dynamite_nsm/cmd/service_interfaces.py def append_service_multiple_responsibility_interface_to_parser ( parser : argparse . ArgumentParser , interface : MultipleResponsibilityInterface ) -> argparse . ArgumentParser : \"\"\" Append an `MultipleResponsibilityInterface` to an existing parser as a sub-parser. Args: parser: The parser to append our interface too interface: The new interface to add to the parser Returns: A new parser \"\"\" return interface . build_parser ( interface , parser ) append_service_simple_config_management_interface_to_parser ( parser , interface ) Append an SimpleConfigManagerInterface to an existing parser as a sub-parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The parser to append our interface too required interface SimpleConfigManagerInterface The new interface to add to the parser required Returns: Type Description argparse.ArgumentParser A new parser Source code in dynamite_nsm/cmd/service_interfaces.py def append_service_simple_config_management_interface_to_parser ( parser : argparse . ArgumentParser , interface : SimpleConfigManagerInterface ) -> \\ argparse . ArgumentParser : \"\"\"Append an `SimpleConfigManagerInterface` to an existing parser as a sub-parser. Args: parser: The parser to append our interface too interface: The new interface to add to the parser Returns: A new parser \"\"\" return interface . build_parser ( interface , parser ) append_service_single_responsibility_interface_to_parser ( parser , interface ) Append an SingleResponsibilityInterface to an existing parser as a sub-parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The parser to append our interface too required interface SingleResponsibilityInterface The new interface to add to the parser required Returns: Type Description argparse.ArgumentParser A new parser Source code in dynamite_nsm/cmd/service_interfaces.py def append_service_single_responsibility_interface_to_parser ( parser : argparse . ArgumentParser , interface : SingleResponsibilityInterface ) -> \\ argparse . ArgumentParser : \"\"\" Append an `SingleResponsibilityInterface` to an existing parser as a sub-parser. Args: parser: The parser to append our interface too interface: The new interface to add to the parser Returns: A new parser \"\"\" return interface . build_parser ( interface , parser )","title":"service_interfaces"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.MultipleResponsibilityInterface","text":"Maps a class with several responsibilities to commandline interface For example ProcessManager's provides multiple methods that can be invoked to perform various tasks. MultipleResponsibilityInterface: Takes a single class and supported_method_names. Uses several introspection techniques to enumerate instance methods from that class Derives the **kwargs params for argparse.ArgumentParser.add_arguments method for the init , and selected exec_method Generates parser using annotation and docs Provide a method for executing the parsed argparse.Namespace against cls. init ( base_kwargs).{exec_method( interface_kwargs)}","title":"MultipleResponsibilityInterface"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.MultipleResponsibilityInterface.__init__","text":"Initialize the interface Parameters: Name Type Description Default cls object The class we wish to turn into a commandline utility required supported_method_names List[str] A list of methods to create interfaces for required interface_name str The name of this commandline interface required interface_description Optional[str] A description of what this interface is supposed to do None defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default too. None Source code in dynamite_nsm/cmd/service_interfaces.py def __init__ ( self , cls : object , supported_method_names : List [ str ], interface_name : str , interface_description : Optional [ str ] = None , defaults : Optional [ Dict ] = None ): \"\"\"Initialize the interface Args: cls: The class we wish to turn into a commandline utility supported_method_names: A list of methods to create interfaces for interface_name: The name of this commandline interface interface_description: A description of what this interface is supposed to do defaults: A dictionary where the key a parameter name and the value represents the value to default too. \"\"\" super () . __init__ ( interface_name , interface_description , defaults = defaults ) self . cls = cls self . supported_method_names = supported_method_names if not interface_description : self . interfaceModuleType_description = inspect . getdoc ( cls ) self . base_params , self . interface_methods = get_class_instance_methods ( cls , defaults , use_parent_init = False ) # print(self.cls, [(item.name, item.flags, item.kwargs) for item in self.base_params])","title":"__init__()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.MultipleResponsibilityInterface.build_parser","text":"Build a parser from any MultipleResponsibilityInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface MultipleResponsibilityInterface The MultipleResponsibilityInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the MultipleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py @staticmethod def build_parser ( interface : MultipleResponsibilityInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `MultipleResponsibilityInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `MultipleResponsibilityInterface` instance you wish to append Returns: An argument parser instance for the `MultipleResponsibilityInterface` derived class \"\"\" actions_subparsers = parser . add_subparsers () for method , params_group in interface . interface_methods . items (): if method in interface . supported_method_names : action_parser = actions_subparsers . add_parser ( method . replace ( '_' , '-' )) action_parser . set_defaults ( entry_method_name = method ) for params in interface . base_params : action_parser . add_argument ( * params . flags , ** params . kwargs ) for params in params_group : try : action_parser . add_argument ( * params . flags , ** params . kwargs ) except argparse . ArgumentError : continue return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.MultipleResponsibilityInterface.execute","text":"Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description Any Any value; completely depends on the selected_method being invoked Source code in dynamite_nsm/cmd/service_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: Any value; completely depends on the `selected_method` being invoked \"\"\" constructor_kwargs = dict () entry_method_kwargs = dict () for param , value in vars ( args ) . items (): if param in [ p . name for p in self . base_params ]: constructor_kwargs [ param ] = value else : entry_method_kwargs [ param ] = value entry_method_kwargs . pop ( 'component' , None ) entry_method_kwargs . pop ( 'interface' , None ) entry_method_kwargs . pop ( 'sub_interface' , None ) entry_method_kwargs . pop ( 'entry_method_name' , None ) # Dynamically load our class klass = getattr ( self , 'cls' ) # Instantiate it with the constructor kwargs exec_inst = klass ( ** constructor_kwargs ) # Dynamically load our defined entry_method exec_entry_method = getattr ( exec_inst , args . entry_method_name ) # Call the entry method return exec_entry_method ( ** entry_method_kwargs )","title":"execute()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.MultipleResponsibilityInterface.get_parser","text":"Get the current interface as an argparse.ArgumentParser instance Returns: Type Description argparse.ArgumentParser An argument parser instance for the MultipleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Get the current interface as an `argparse.ArgumentParser` instance Returns: An argument parser instance for the `MultipleResponsibilityInterface` derived class \"\"\" parser = argparse . ArgumentParser ( description = f ' { self . interface_name } - { self . interface_description } ' ) return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SimpleConfigManagerInterface","text":"Based upon the SingleResponsibilityInterface maps a class with only one responsibility to commandline interface, but also makes all the instance variables of the configuration class available as commandline arguments","title":"SimpleConfigManagerInterface"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SimpleConfigManagerInterface.__init__","text":"Initialize the interface Parameters: Name Type Description Default config Union[config.GenericConfigManager, config.YamlConfigManager] The class we wish to turn into a commandline utility required interface_name str The name of this commandline interface required interface_description Optional[str] A description of what this interface is supposed to do None defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default too. None Source code in dynamite_nsm/cmd/service_interfaces.py def __init__ ( self , config : Union [ config . GenericConfigManager , config . YamlConfigManager ], interface_name : str , interface_description : Optional [ str ] = None , pretty_print_status : Optional [ bool ] = True , defaults : Optional [ Dict ] = None ): \"\"\"Initialize the interface Args: config: The class we wish to turn into a commandline utility interface_name: The name of this commandline interface interface_description: A description of what this interface is supposed to do defaults: A dictionary where the key a parameter name and the value represents the value to default too. \"\"\" self . config = config self . config_module_map = {} self . pretty_print_status = pretty_print_status super () . __init__ ( self . config . __class__ , 'commit' , interface_name , interface_description , defaults )","title":"__init__()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SimpleConfigManagerInterface.build_parser","text":"Build a parser from any SimpleConfigManagerInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface SimpleConfigManagerInterface The SimpleConfigManagerInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the SimpleConfigManagerInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py @staticmethod def build_parser ( interface : SimpleConfigManagerInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `SimpleConfigManagerInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `SimpleConfigManagerInterface` instance you wish to append Returns: An argument parser instance for the `SimpleConfigManagerInterface` derived class \"\"\" config_options = parser . add_argument_group ( 'configuration options' ) config_objects_subparser = parser . add_subparsers () for params in interface . base_params + interface . interface_params : parser . add_argument ( * params . flags , ** params . kwargs ) for var in vars ( interface . config ): if var in RESERVED_VARIABLE_NAMES : continue elif var . startswith ( '_' ): continue elif '_raw' in var : continue elif var in [ param . name for param in interface . base_params ]: continue elif 'config_objects' in str ( type ( getattr ( interface . config , var ))): complex_obj = getattr ( interface . config , var ) if isinstance ( complex_obj , Analyzers ): config_module_interface = AnalyzersInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , targets . BaseTargets ): config_module_interface = FilebeatTargetsInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , node . BaseComponent ): config_module_interface = ZeekNodeConfigObjectInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , node . BaseComponents ): config_module_interface = ZeekNodeConfigObjectsInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) elif isinstance ( complex_obj , misc . AfPacketInterfaces ): config_module_interface = SuricataInterfaceConfigObjectsInterface ( complex_obj ) interface . config_module_map . update ({ var : config_module_interface }) interface_operations . append_service_interface_to_parser ( config_objects_subparser , interface = config_module_interface , interface_name = var , interface_group_name = 'config_module' ) else : args = ArgparseParameters . create_from_typing_annotation ( var , type ( getattr ( interface . config , var )), required = False ) try : config_options . add_argument ( * args . flags , ** args . kwargs ) except argparse . ArgumentError : continue return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SimpleConfigManagerInterface.execute","text":"Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description Any Any value; depending on the value returned from the entry_method (usually a ConfigManager.commit ) Source code in dynamite_nsm/cmd/service_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: Any value; depending on the value returned from the `entry_method` (usually a `ConfigManager.commit`) \"\"\" changed_config = False if not getattr ( args , 'config_module' , None ): args . config_module = None headers = [ 'Config Option' , 'Value' ] table = [ headers ] changed_rows_only = [ headers ] # In the scenario we have configuration modules include them as config options in our display table table . extend ( [[ config_module_name , 'Configuration Module' ] for config_module_name in self . config_module_map . keys ()]) for option , value in args . __dict__ . items (): if option in self . defaults : continue if option in RESERVED_VARIABLE_NAMES : continue if not value : config_value = ( option , getattr ( self . config , option , None )) table . append ( config_value ) else : changed_config = True changed_config_value = ( option , value ) changed_rows_only . append ( changed_config_value ) setattr ( self . config , option , value ) if args . config_module : # Configuration module interfaces need to pass through relevant commandline defaults from parent interface self . config_module_map [ args . config_module ] . defaults = self . defaults selected_config_module = self . config_module_map [ args . config_module ] res = selected_config_module . execute ( args ) if isinstance ( res , Analyzers ): selected_analyzer_header = [ 'Id' , 'Name' , 'Enabled' , 'Value' ] setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = selected_analyzer_header , tablefmt = 'fancy_grid' ) elif isinstance ( res , targets . BaseTargets ): setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) elif isinstance ( res , node . BaseComponent ): setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) elif isinstance ( res , node . BaseComponents ): setattr ( self . config , args . config_module , res ) self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) elif isinstance ( res , misc . AfPacketInterfaces ): self . config . commit () return tabulate ( selected_config_module . changed_rows , headers = headers , tablefmt = 'fancy_grid' ) else : return res else : if changed_config : self . config . commit () if self . pretty_print_status : return tabulate ( changed_rows_only , tablefmt = 'fancy_grid' ) return dict ( changed_rows_only ) else : if self . pretty_print_status : return tabulate ( table , tablefmt = 'fancy_grid' ) return dict ( table )","title":"execute()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SimpleConfigManagerInterface.get_parser","text":"Returns an argparse.ArgumentParser instance before parse_args has been called. Returns: Type Description argparse.ArgumentParser argparse.ArgumentParser instance Source code in dynamite_nsm/cmd/service_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Returns an argparse.ArgumentParser instance before parse_args has been called. Returns: argparse.ArgumentParser instance \"\"\" parser = super () . get_parser () return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SingleResponsibilityInterface","text":"Maps a class with only one responsibility to commandline interface For example InstallManager's only need call one function (perform one responsibility) once instantiated. SingleResponsibilityInterface: Takes a single class and entry_method_name. Uses several introspection techniques to enumerate instance methods from that class Derives the **kwargs params for argparse.ArgumentParser.add_arguments method for the init , and entry_method Generates parser using annotation and docs Provide a method for executing the parsed argparse.Namespace against cls. init ( base_kwargs).{entry_method( interface_kwargs)}","title":"SingleResponsibilityInterface"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SingleResponsibilityInterface.__init__","text":"Initialize the interface Parameters: Name Type Description Default cls object The class we wish to turn into a commandline utility required entry_method_name str The name of the method inside the above class we wish to call at execution time required interface_name str The name of this commandline interface required interface_description Optional[str] A description of what this interface is supposed to do None defaults Optional[Dict] A dictionary where the key a parameter name and the value represents the value to default too. None Source code in dynamite_nsm/cmd/service_interfaces.py def __init__ ( self , cls : object , entry_method_name : str , interface_name : str , interface_description : Optional [ str ] = None , defaults : Optional [ Dict ] = None ): \"\"\"Initialize the interface Args: cls: The class we wish to turn into a commandline utility entry_method_name: The name of the method inside the above class we wish to call at execution time interface_name: The name of this commandline interface interface_description: A description of what this interface is supposed to do defaults: A dictionary where the key a parameter name and the value represents the value to default too. \"\"\" super () . __init__ ( interface_name , interface_description , defaults = defaults ) self . cls = cls self . entry_method_name = entry_method_name self . defaults = defaults if not self . defaults : self . defaults = dict () if not interface_description : self . interface_description = inspect . getdoc ( cls ) self . base_params , self . interface_methods = get_class_instance_methods ( cls , defaults , use_parent_init = False ) self . interface_params = self . interface_methods [ self . entry_method_name ]","title":"__init__()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SingleResponsibilityInterface.build_parser","text":"Build a parser from any SingleResponsibilityInterface and argparse.ArgumentParser derived class Parameters: Name Type Description Default parser argparse.ArgumentParser The argparse.ArgumentParser instance that you want to add a new parser too required interface SingleResponsibilityInterface The SingleResponsibilityInterface instance you wish to append required Returns: Type Description argparse.ArgumentParser An argument parser instance for the SingleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py @staticmethod def build_parser ( interface : SingleResponsibilityInterface , parser : argparse . ArgumentParser ) -> argparse . ArgumentParser : \"\"\"Build a parser from any `SingleResponsibilityInterface` and `argparse.ArgumentParser` derived class Args: parser: The `argparse.ArgumentParser` instance that you want to add a new parser too interface: The `SingleResponsibilityInterface` instance you wish to append Returns: An argument parser instance for the `SingleResponsibilityInterface` derived class \"\"\" for params in interface . base_params : parser . add_argument ( * params . flags , ** params . kwargs ) for params in interface . interface_params : parser . add_argument ( * params . flags , ** params . kwargs ) return parser","title":"build_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SingleResponsibilityInterface.execute","text":"Interpret the results of an argparse.ArgumentParser.parse_args() method and perform one or more operations. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description Any Any value; depending on the value returned from the entry_method Source code in dynamite_nsm/cmd/service_interfaces.py def execute ( self , args : argparse . Namespace ) -> Any : \"\"\"Interpret the results of an `argparse.ArgumentParser.parse_args()` method and perform one or more operations. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: Any value; depending on the value returned from the `entry_method` \"\"\" if getattr ( args , 'background' , None ): setattr ( args , 'background' , None ) return self . execute_in_background ( args ) constructor_kwargs = dict () entry_method_kwargs = dict () for param , value in vars ( args ) . items (): if param in [ p . name for p in self . base_params ]: constructor_kwargs [ param ] = value else : entry_method_kwargs [ param ] = value entry_method_kwargs . pop ( 'component' , None ) entry_method_kwargs . pop ( 'interface' , None ) entry_method_kwargs . pop ( 'sub_interface' , None ) entry_method_kwargs . pop ( 'background' , None ) # Dynamically load our class klass = getattr ( self , 'cls' ) # Instantiate it with the constructor kwargs exec_inst = klass ( ** constructor_kwargs ) # Dynamically load our defined entry_method exec_entry_method = getattr ( exec_inst , self . entry_method_name ) # Call the entry method return exec_entry_method ( ** entry_method_kwargs )","title":"execute()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SingleResponsibilityInterface.execute_in_background","text":"Call execute, but run in the background inside a dedicated process. Parameters: Name Type Description Default args argparse.Namespace The output of argparse.ArgumentParser.parse_args() function required Returns: Type Description None None Source code in dynamite_nsm/cmd/service_interfaces.py def execute_in_background ( self , args : argparse . Namespace ) -> None : \"\"\"Call execute, but run in the background inside a dedicated process. Args: args: The output of argparse.ArgumentParser.parse_args() function Returns: None \"\"\" args . verbose = True args . stdout = False with daemon . DaemonContext (): self . execute ( args )","title":"execute_in_background()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.SingleResponsibilityInterface.get_parser","text":"Get the current interface as an argparse.ArgumentParser instance Returns: Type Description argparse.ArgumentParser An argument parser instance for the SingleResponsibilityInterface derived class Source code in dynamite_nsm/cmd/service_interfaces.py def get_parser ( self ) -> argparse . ArgumentParser : \"\"\"Get the current interface as an `argparse.ArgumentParser` instance Returns: An argument parser instance for the `SingleResponsibilityInterface` derived class \"\"\" parser = argparse . ArgumentParser ( description = f ' { self . interface_name } - { self . interface_description } ' ) return self . build_parser ( self , parser )","title":"get_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.append_service_multiple_responsibility_interface_to_parser","text":"Append an MultipleResponsibilityInterface to an existing parser as a sub-parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The parser to append our interface too required interface MultipleResponsibilityInterface The new interface to add to the parser required Returns: Type Description argparse.ArgumentParser A new parser Source code in dynamite_nsm/cmd/service_interfaces.py def append_service_multiple_responsibility_interface_to_parser ( parser : argparse . ArgumentParser , interface : MultipleResponsibilityInterface ) -> argparse . ArgumentParser : \"\"\" Append an `MultipleResponsibilityInterface` to an existing parser as a sub-parser. Args: parser: The parser to append our interface too interface: The new interface to add to the parser Returns: A new parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_service_multiple_responsibility_interface_to_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.append_service_simple_config_management_interface_to_parser","text":"Append an SimpleConfigManagerInterface to an existing parser as a sub-parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The parser to append our interface too required interface SimpleConfigManagerInterface The new interface to add to the parser required Returns: Type Description argparse.ArgumentParser A new parser Source code in dynamite_nsm/cmd/service_interfaces.py def append_service_simple_config_management_interface_to_parser ( parser : argparse . ArgumentParser , interface : SimpleConfigManagerInterface ) -> \\ argparse . ArgumentParser : \"\"\"Append an `SimpleConfigManagerInterface` to an existing parser as a sub-parser. Args: parser: The parser to append our interface too interface: The new interface to add to the parser Returns: A new parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_service_simple_config_management_interface_to_parser()"},{"location":"guides/developers/SDK/cmd/service_interfaces/#dynamite_nsm.cmd.service_interfaces.append_service_single_responsibility_interface_to_parser","text":"Append an SingleResponsibilityInterface to an existing parser as a sub-parser. Parameters: Name Type Description Default parser argparse.ArgumentParser The parser to append our interface too required interface SingleResponsibilityInterface The new interface to add to the parser required Returns: Type Description argparse.ArgumentParser A new parser Source code in dynamite_nsm/cmd/service_interfaces.py def append_service_single_responsibility_interface_to_parser ( parser : argparse . ArgumentParser , interface : SingleResponsibilityInterface ) -> \\ argparse . ArgumentParser : \"\"\" Append an `SingleResponsibilityInterface` to an existing parser as a sub-parser. Args: parser: The parser to append our interface too interface: The new interface to add to the parser Returns: A new parser \"\"\" return interface . build_parser ( interface , parser )","title":"append_service_single_responsibility_interface_to_parser()"},{"location":"guides/developers/SDK/services/agent/install/","text":"Installation Manager that will install Zeek, Suricata, and Filebeat on the same physical instance. Make sure your computer is up to the task! To import... from dynamite_nsm.services.agent import install as agent_install InstallManager __init__ ( self , filebeat_install_directory , suricata_configuration_directory = None , suricata_install_directory = None , suricata_log_directory = None , zeek_configuration_directory = None , zeek_install_directory = None , stdout = False , verbose = False ) special Manage agent installation process Parameters: Name Type Description Default filebeat_install_directory str The path to the Filebeat install directory (Default - /opt/dynamite/filebeat) required suricata_configuration_directory Optional[str] The path to the Suricata config directory (Default - /etc/dynamite/suricata) None suricata_install_directory Optional[str] The path to the Suricata install directory (Default - /opt/dynamite/suricata) None suricata_log_directory Optional[str] The path to the Suricata log directory (Default - /var/log/suricata) None zeek_configuration_directory Optional[str] The path to the Zeek configuration directory (Default - /etc/dynamite/zeek) None zeek_install_directory Optional[str] The path to the Zeek installation directory (Default - /opt/dynamite/zeek) None stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/agent/install.py def __init__ ( self , filebeat_install_directory : str , suricata_configuration_directory : Optional [ str ] = None , suricata_install_directory : Optional [ str ] = None , suricata_log_directory : Optional [ str ] = None , zeek_configuration_directory : Optional [ str ] = None , zeek_install_directory : Optional [ str ] = None , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Manage agent installation process Args: filebeat_install_directory: The path to the Filebeat install directory (Default - /opt/dynamite/filebeat) suricata_configuration_directory: The path to the Suricata config directory (Default - /etc/dynamite/suricata) suricata_install_directory: The path to the Suricata install directory (Default - /opt/dynamite/suricata) suricata_log_directory: The path to the Suricata log directory (Default - /var/log/suricata) zeek_configuration_directory: The path to the Zeek configuration directory (Default - /etc/dynamite/zeek) zeek_install_directory: The path to the Zeek installation directory (Default - /opt/dynamite/zeek) stdout: Print the output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( 'agent.install' , stdout = stdout , verbose = verbose ) self . filebeat_install_directory = filebeat_install_directory self . suricata_configuration_directory = suricata_configuration_directory self . suricata_log_directory = suricata_log_directory self . suricata_install_directory = suricata_install_directory self . zeek_configuration_directory = zeek_configuration_directory self . zeek_install_directory = zeek_install_directory setup ( self , inspect_interfaces , targets , target_type = 'elasticsearch' ) Setup Zeek, Suricata and Filebeat on the same physical instance. Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required targets List[str] One or more URLs to send event/alerts to (E.G https://my_elasticsearch_collector.local:9200) required target_type Optional[str] The target type; current supported: elasticsearch (default), logstash, kafka, redis 'elasticsearch' Returns: Type Description None None Source code in dynamite_nsm/services/agent/install.py def setup ( self , inspect_interfaces : List [ str ], targets : List [ str ], target_type : Optional [ str ] = 'elasticsearch' ) -> None : \"\"\" Setup Zeek, Suricata and Filebeat on the same physical instance. Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) targets: One or more URLs to send event/alerts to (E.G https://my_elasticsearch_collector.local:9200) target_type: The target type; current supported: elasticsearch (default), logstash, kafka, redis Returns: None \"\"\" if self . suricata_install_directory or self . suricata_configuration_directory or self . suricata_log_directory : if not ( self . suricata_install_directory and self . suricata_configuration_directory and self . suricata_log_directory ): self . logger . error ( 'You must specify suricata-configuration-directory, suricata-install-directory, ' 'and suricata-log-directory.' ) return None suricata_install . InstallManager ( configuration_directory = self . suricata_configuration_directory , install_directory = self . suricata_install_directory , log_directory = self . suricata_log_directory , download_suricata_archive = True , stdout = self . stdout , verbose = self . verbose ) . setup ( inspect_interfaces ) if self . zeek_install_directory or self . zeek_install_directory : if not ( self . zeek_install_directory and self . zeek_configuration_directory ): self . logger . error ( 'You must specify both the zeek-configuration-directory and zeek-install-directory.' ) return None zeek_install . InstallManager ( configuration_directory = self . zeek_configuration_directory , install_directory = self . zeek_install_directory , download_zeek_archive = True , stdout = self . stdout , verbose = self . verbose ) . setup ( inspect_interfaces ) filebeat_install . InstallManager ( install_directory = self . filebeat_install_directory , download_filebeat_archive = True , stdout = self . stdout , verbose = self . verbose ) . setup ( targets = targets , target_type = target_type ) optimize . OptimizeThreadingManager ( self . suricata_configuration_directory , self . zeek_install_directory , stdout = self . stdout , verbose = self . verbose ) . optimize () UninstallManager __init__ ( self , stdout = False , verbose = False ) special Manage agent uninstall process Parameters: Name Type Description Default stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/agent/install.py def __init__ ( self , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Manage agent uninstall process Args: stdout: Print the output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( directories = [], name = 'agent.uninstall' , stdout = stdout , verbose = verbose ) uninstall ( self ) Uninstall Zeek, Suricata and Filebeat from this instance. Returns: Type Description None None Source code in dynamite_nsm/services/agent/install.py def uninstall ( self ) -> None : \"\"\"Uninstall Zeek, Suricata and Filebeat from this instance. Returns: None \"\"\" from dynamite_nsm.services.zeek import profile as zeek_profile from dynamite_nsm.services.suricata import profile as suricata_profile filebeat_install . UninstallManager ( self . stdout , self . verbose ) . uninstall () if zeek_profile . ProcessProfiler () . is_installed (): zeek_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall () if suricata_profile . ProcessProfiler () . is_installed (): suricata_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall ()","title":"install"},{"location":"guides/developers/SDK/services/agent/install/#dynamite_nsm.services.agent.install.InstallManager","text":"","title":"InstallManager"},{"location":"guides/developers/SDK/services/agent/install/#dynamite_nsm.services.agent.install.InstallManager.__init__","text":"Manage agent installation process Parameters: Name Type Description Default filebeat_install_directory str The path to the Filebeat install directory (Default - /opt/dynamite/filebeat) required suricata_configuration_directory Optional[str] The path to the Suricata config directory (Default - /etc/dynamite/suricata) None suricata_install_directory Optional[str] The path to the Suricata install directory (Default - /opt/dynamite/suricata) None suricata_log_directory Optional[str] The path to the Suricata log directory (Default - /var/log/suricata) None zeek_configuration_directory Optional[str] The path to the Zeek configuration directory (Default - /etc/dynamite/zeek) None zeek_install_directory Optional[str] The path to the Zeek installation directory (Default - /opt/dynamite/zeek) None stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/agent/install.py def __init__ ( self , filebeat_install_directory : str , suricata_configuration_directory : Optional [ str ] = None , suricata_install_directory : Optional [ str ] = None , suricata_log_directory : Optional [ str ] = None , zeek_configuration_directory : Optional [ str ] = None , zeek_install_directory : Optional [ str ] = None , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Manage agent installation process Args: filebeat_install_directory: The path to the Filebeat install directory (Default - /opt/dynamite/filebeat) suricata_configuration_directory: The path to the Suricata config directory (Default - /etc/dynamite/suricata) suricata_install_directory: The path to the Suricata install directory (Default - /opt/dynamite/suricata) suricata_log_directory: The path to the Suricata log directory (Default - /var/log/suricata) zeek_configuration_directory: The path to the Zeek configuration directory (Default - /etc/dynamite/zeek) zeek_install_directory: The path to the Zeek installation directory (Default - /opt/dynamite/zeek) stdout: Print the output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( 'agent.install' , stdout = stdout , verbose = verbose ) self . filebeat_install_directory = filebeat_install_directory self . suricata_configuration_directory = suricata_configuration_directory self . suricata_log_directory = suricata_log_directory self . suricata_install_directory = suricata_install_directory self . zeek_configuration_directory = zeek_configuration_directory self . zeek_install_directory = zeek_install_directory","title":"__init__()"},{"location":"guides/developers/SDK/services/agent/install/#dynamite_nsm.services.agent.install.InstallManager.setup","text":"Setup Zeek, Suricata and Filebeat on the same physical instance. Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required targets List[str] One or more URLs to send event/alerts to (E.G https://my_elasticsearch_collector.local:9200) required target_type Optional[str] The target type; current supported: elasticsearch (default), logstash, kafka, redis 'elasticsearch' Returns: Type Description None None Source code in dynamite_nsm/services/agent/install.py def setup ( self , inspect_interfaces : List [ str ], targets : List [ str ], target_type : Optional [ str ] = 'elasticsearch' ) -> None : \"\"\" Setup Zeek, Suricata and Filebeat on the same physical instance. Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) targets: One or more URLs to send event/alerts to (E.G https://my_elasticsearch_collector.local:9200) target_type: The target type; current supported: elasticsearch (default), logstash, kafka, redis Returns: None \"\"\" if self . suricata_install_directory or self . suricata_configuration_directory or self . suricata_log_directory : if not ( self . suricata_install_directory and self . suricata_configuration_directory and self . suricata_log_directory ): self . logger . error ( 'You must specify suricata-configuration-directory, suricata-install-directory, ' 'and suricata-log-directory.' ) return None suricata_install . InstallManager ( configuration_directory = self . suricata_configuration_directory , install_directory = self . suricata_install_directory , log_directory = self . suricata_log_directory , download_suricata_archive = True , stdout = self . stdout , verbose = self . verbose ) . setup ( inspect_interfaces ) if self . zeek_install_directory or self . zeek_install_directory : if not ( self . zeek_install_directory and self . zeek_configuration_directory ): self . logger . error ( 'You must specify both the zeek-configuration-directory and zeek-install-directory.' ) return None zeek_install . InstallManager ( configuration_directory = self . zeek_configuration_directory , install_directory = self . zeek_install_directory , download_zeek_archive = True , stdout = self . stdout , verbose = self . verbose ) . setup ( inspect_interfaces ) filebeat_install . InstallManager ( install_directory = self . filebeat_install_directory , download_filebeat_archive = True , stdout = self . stdout , verbose = self . verbose ) . setup ( targets = targets , target_type = target_type ) optimize . OptimizeThreadingManager ( self . suricata_configuration_directory , self . zeek_install_directory , stdout = self . stdout , verbose = self . verbose ) . optimize ()","title":"setup()"},{"location":"guides/developers/SDK/services/agent/install/#dynamite_nsm.services.agent.install.UninstallManager","text":"","title":"UninstallManager"},{"location":"guides/developers/SDK/services/agent/install/#dynamite_nsm.services.agent.install.UninstallManager.__init__","text":"Manage agent uninstall process Parameters: Name Type Description Default stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/agent/install.py def __init__ ( self , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Manage agent uninstall process Args: stdout: Print the output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( directories = [], name = 'agent.uninstall' , stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/agent/install/#dynamite_nsm.services.agent.install.UninstallManager.uninstall","text":"Uninstall Zeek, Suricata and Filebeat from this instance. Returns: Type Description None None Source code in dynamite_nsm/services/agent/install.py def uninstall ( self ) -> None : \"\"\"Uninstall Zeek, Suricata and Filebeat from this instance. Returns: None \"\"\" from dynamite_nsm.services.zeek import profile as zeek_profile from dynamite_nsm.services.suricata import profile as suricata_profile filebeat_install . UninstallManager ( self . stdout , self . verbose ) . uninstall () if zeek_profile . ProcessProfiler () . is_installed (): zeek_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall () if suricata_profile . ProcessProfiler () . is_installed (): suricata_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall ()","title":"uninstall()"},{"location":"guides/developers/SDK/services/agent/process/","text":"A simple process manager wrapping Zeek, Suricata, and Filebeat. To import... from dynamite_nsm.services.agent import process as agent_process ProcessManager Agent Process Manager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Agent Processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/agent/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Agent Processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" self . stdout = stdout self . verbose = verbose self . pretty_print_status = pretty_print_status log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( 'agent.process' , level = log_level , stdout = stdout ) start ( self ) Start agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def start ( self ) -> bool : \"\"\"Start agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . start () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . start () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . start () return filebeat_res and zeek_res and suricata_res status ( self ) Get the status of a processes Returns: Type Description Union[Dict, str] A dictionary containing process status or a tabulated string if pretty_print is True. Source code in dynamite_nsm/services/agent/process.py def status ( self ) -> Optional [ Union [ Dict , str ]]: \"\"\"Get the status of a processes Returns: A dictionary containing process status or a tabulated string if `pretty_print` is True. \"\"\" if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install filebeat to run this command.' ) return None agent_status = {} filebeat_status , zeek_status , suricata_status = {}, {}, {} filebeat_status = filebeat_process . ProcessManager () . status () agent_status . update ({ 'filebeat' : { 'running' : filebeat_status . get ( 'running' ), 'enabled_on_startup' : filebeat_status . get ( 'enabled_on_startup' )}}) if zeek_profile . ProcessProfiler () . is_installed (): zeek_status = zeek_process . ProcessManager () . status () agent_status . update ({ 'zeek' : { 'running' : zeek_status . get ( 'running' ), 'enabled_on_startup' : zeek_status . get ( 'enabled_on_startup' )}}) if suricata_profile . ProcessProfiler () . is_installed (): suricata_status = suricata_process . ProcessManager () . status () agent_status . update ({ 'suricata' : { 'running' : suricata_status . get ( 'running' ), 'enabled_on_startup' : suricata_status . get ( 'enabled_on_startup' )}}) if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize child_services = [ [ 'Service' , 'Running' , 'Enabled on Startup' ], [ 'filebeat' , colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' ) ] ] if zeek_status : child_services . append ( [ 'zeek' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) if suricata_status : child_services . append ( [ 'suricata' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) return tabulate . tabulate ( child_services , tablefmt = 'fancy_grid' ) return agent_status stop ( self ) Stop agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def stop ( self ) -> bool : \"\"\"Stop agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . stop () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . stop () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . stop () return filebeat_res and zeek_res and suricata_res","title":"process"},{"location":"guides/developers/SDK/services/agent/process/#dynamite_nsm.services.agent.process.ProcessManager","text":"Agent Process Manager","title":"ProcessManager"},{"location":"guides/developers/SDK/services/agent/process/#dynamite_nsm.services.agent.process.ProcessManager.__init__","text":"Manage Agent Processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/agent/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Agent Processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" self . stdout = stdout self . verbose = verbose self . pretty_print_status = pretty_print_status log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( 'agent.process' , level = log_level , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/services/agent/process/#dynamite_nsm.services.agent.process.ProcessManager.start","text":"Start agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def start ( self ) -> bool : \"\"\"Start agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . start () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . start () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . start () return filebeat_res and zeek_res and suricata_res","title":"start()"},{"location":"guides/developers/SDK/services/agent/process/#dynamite_nsm.services.agent.process.ProcessManager.status","text":"Get the status of a processes Returns: Type Description Union[Dict, str] A dictionary containing process status or a tabulated string if pretty_print is True. Source code in dynamite_nsm/services/agent/process.py def status ( self ) -> Optional [ Union [ Dict , str ]]: \"\"\"Get the status of a processes Returns: A dictionary containing process status or a tabulated string if `pretty_print` is True. \"\"\" if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install filebeat to run this command.' ) return None agent_status = {} filebeat_status , zeek_status , suricata_status = {}, {}, {} filebeat_status = filebeat_process . ProcessManager () . status () agent_status . update ({ 'filebeat' : { 'running' : filebeat_status . get ( 'running' ), 'enabled_on_startup' : filebeat_status . get ( 'enabled_on_startup' )}}) if zeek_profile . ProcessProfiler () . is_installed (): zeek_status = zeek_process . ProcessManager () . status () agent_status . update ({ 'zeek' : { 'running' : zeek_status . get ( 'running' ), 'enabled_on_startup' : zeek_status . get ( 'enabled_on_startup' )}}) if suricata_profile . ProcessProfiler () . is_installed (): suricata_status = suricata_process . ProcessManager () . status () agent_status . update ({ 'suricata' : { 'running' : suricata_status . get ( 'running' ), 'enabled_on_startup' : suricata_status . get ( 'enabled_on_startup' )}}) if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize child_services = [ [ 'Service' , 'Running' , 'Enabled on Startup' ], [ 'filebeat' , colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' ) ] ] if zeek_status : child_services . append ( [ 'zeek' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) if suricata_status : child_services . append ( [ 'suricata' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) return tabulate . tabulate ( child_services , tablefmt = 'fancy_grid' ) return agent_status","title":"status()"},{"location":"guides/developers/SDK/services/agent/process/#dynamite_nsm.services.agent.process.ProcessManager.stop","text":"Stop agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def stop ( self ) -> bool : \"\"\"Stop agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . stop () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . stop () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . stop () return filebeat_res and zeek_res and suricata_res","title":"stop()"},{"location":"guides/developers/SDK/services/base/config/","text":"Base configuration wrappers To import... from dynamite_nsm.services.base import config BackupConfigManager Manage backup and restoration process across various service configs list_backup_configs ( self ) List configuration backups Returns: Type Description List A list of dictionaries with the following keys [\"name\", \"path\", \"timestamp\"] Source code in dynamite_nsm/services/base/config.py def list_backup_configs ( self ) -> List : \"\"\"List configuration backups Returns: A list of dictionaries with the following keys [\"name\", \"path\", \"timestamp\"] \"\"\" return utilities . list_backup_configurations ( self . backup_directory ) restore_backup_config ( self , backup_name , restore_name ) Restore a configuration from our config store :param backup_name: The name of the configuration file or the keyword \"recent\" which will restore the most recent backup. :param restore_name: The name of the configuration file to write too :return: True, if successful Source code in dynamite_nsm/services/base/config.py def restore_backup_config ( self , backup_name : str , restore_name : str ): \"\"\" Restore a configuration from our config store :param backup_name: The name of the configuration file or the keyword \"recent\" which will restore the most recent backup. :param restore_name: The name of the configuration file to write too :return: True, if successful \"\"\" if backup_name == \"recent\" : configs = self . list_backup_configs () if configs : return utilities . restore_backup_configuration ( configs [ 0 ][ 'filepath' ], restore_name ) return utilities . restore_backup_configuration ( os . path . join ( self . backup_directory , backup_name ), restore_name ) GenericConfigManager __init__ ( self , config_data , name , verbose = False , stdout = True ) special A catchall configuration manager, generic enough to work on any configuration like file Parameters: Name Type Description Default config_data Dict Configuration data dictionary required name str The name of the configuration required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Source code in dynamite_nsm/services/base/config.py def __init__ ( self , config_data : Dict , name : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" A catchall configuration manager, generic enough to work on any configuration like file Args: config_data: Configuration data dictionary name: The name of the configuration verbose: Include detailed debug messages stdout: Print output to console \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . config_data = config_data self . formatted_data = json . dumps ( config_data ) log_level = logging . INFO if verbose : log_level = logging . DEBUG self . stdout = stdout self . verbose = verbose self . logger = get_logger ( str ( name ), level = log_level , stdout = stdout ) commit ( self , out_file_path , backup_directory = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path str The path to the output file required backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def commit ( self , out_file_path : str , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file backup_directory: The path to the backup directory Returns: None \"\"\" # Backup old configuration first out_file_name = os . path . basename ( out_file_path ) backup_file_name = out_file_name + '.backup' if backup_directory : utilities . backup_configuration_file ( out_file_path , backup_directory , destination_file_prefix = backup_file_name ) try : with open ( out_file_path , 'w' ) as config_raw_f : config_raw_f . write ( self . formatted_data ) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) except IOError as e : raise exceptions . WriteConfigError ( f 'An error occurred while writing the configuration file to disk. { e } ' ) self . logger . warning ( 'Configuration updated. Restart this service to apply.' ) get_printable_config ( self ) Get the configuration as a dictionary object Returns: Type Description Dict A dictionary of config keys and values Source code in dynamite_nsm/services/base/config.py def get_printable_config ( self ) -> Dict : \"\"\" Get the configuration as a dictionary object Returns: A dictionary of config keys and values \"\"\" variables = {} for var in vars ( self ): if var . startswith ( '_' ): continue variables [ var ] = str ( getattr ( self , var )) return variables reset ( self , out_file_path , default_config_path ) Reset a configuration file back to its default !!! out_file_path \"The path to the output file\" default_config_path: The path to the default configuration Returns: Type Description None Source code in dynamite_nsm/services/base/config.py def reset ( self , out_file_path : Optional [ str ], default_config_path : Optional [ str ]): \"\"\"Reset a configuration file back to its default Args: out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" self . logger . info ( f 'Restoring { out_file_path } to default state.' ) with open ( default_config_path , 'r' ) as default_conf_f_in : with open ( out_file_path , 'w' ) as conf_f_out : conf_f_out . write ( default_conf_f_in . read ()) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) JavaOptionsConfigManager A special base configuration manager for jvm.options configurations __init__ ( self , config_data , name , verbose = False , stdout = True ) special Work with jvm.options configurations Parameters: Name Type Description Default config_data Dict Configuration data dictionary required name str The name of the configuration required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Source code in dynamite_nsm/services/base/config.py def __init__ ( self , config_data : Dict , name : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Work with jvm.options configurations Args: config_data: Configuration data dictionary name: The name of the configuration verbose: Include detailed debug messages stdout: Print output to console \"\"\" super () . __init__ ( config_data , name = name , verbose = verbose , stdout = stdout ) self . initial_memory = None self . maximum_memory = None self . _raw_extra_params = None self . add_parser ( parser = lambda data : self . _parse_jvm_options ( data )[ 0 ], attribute_name = 'initial_memory' ) self . add_parser ( parser = lambda data : self . _parse_jvm_options ( data )[ 1 ], attribute_name = 'maximum_memory' ) self . add_parser ( parser = lambda data : self . _parse_jvm_options ( data )[ 2 ], attribute_name = '_raw_extra_params' ) commit ( self , out_file_path = None , backup_directory = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory Returns: None \"\"\" # Backup old configuration first out_file_name = os . path . basename ( out_file_path ) backup_file_name = out_file_name + '.backup' self . formatted_data = f '-Xms { self . initial_memory } \\n -Xmx { self . maximum_memory } \\n ' + \\ ' \\n ' . join ( self . _raw_extra_params ) if backup_directory : utilities . backup_configuration_file ( out_file_path , backup_directory , destination_file_prefix = backup_file_name ) try : with open ( out_file_path , 'w' ) as config_raw_f : config_raw_f . write ( self . formatted_data ) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) utilities . set_permissions_of_file ( out_file_path , 644 ) except IOError : raise exceptions . WriteConfigError ( 'An error occurred while writing the configuration file to disk.' ) YamlConfigManager A configuration manager for working with any YAML formatted configuration file __init__ ( self , config_data , name , verbose = False , stdout = True , ** extract_tokens ) special Work with YAML based configuration files Parameters: Name Type Description Default config_data Dict Configuration data dictionary required name str The name of the configuration required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True **extract_tokens Dict A dictionary object, where the keys represent the names of instance variables to create {} Source code in dynamite_nsm/services/base/config.py def __init__ ( self , config_data : Dict , name : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True , ** extract_tokens : Dict ): \"\"\"Work with YAML based configuration files Args: config_data: Configuration data dictionary name: The name of the configuration verbose: Include detailed debug messages stdout: Print output to console **extract_tokens: A dictionary object, where the keys represent the names of instance variables to create if the path to that variable exists. Paths are given using dot notation or as a Tuple. \"\"\" super () . __init__ ( config_data , name , verbose , stdout ) if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . config_data = config_data self . extract_tokens = extract_tokens log_level = logging . INFO if verbose : log_level = logging . DEBUG self . stdout = stdout self . verbose = verbose self . logger = get_logger ( str ( name ), level = log_level , stdout = stdout ) commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The text to be appended at the top of the config file (typically used for YAML version header) None Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The text to be appended at the top of the config file (typically used for YAML version header) Returns: None \"\"\" out_file_name = os . path . basename ( out_file_path ) backup_file_name = out_file_name + '.backup' def update_dict_from_path ( path , value ) -> None : \"\"\"Update the internal YAML dictionary object with the new values from our config Args: path: A tuple representing each level of a nested path in the yaml document ('vars', 'address-groups', 'HOME_NET') = /vars/address-groups/HOME_NET value: The new value Returns: None \"\"\" partial_config_data = self . config_data if len ( path ) > 1 : for i in range ( 0 , len ( path ) - 1 ): k = path [ i ] if isinstance ( partial_config_data , dict ): partial_config_data = partial_config_data [ k ] elif isinstance ( partial_config_data , list ): for list_entry in partial_config_data : if isinstance ( list_entry , dict ): if k in list_entry . keys (): partial_config_data = list_entry [ k ] else : break if value is None : return partial_config_data . update ({ path [ - 1 ]: value }) # Backup old configuration first if backup_directory : utilities . backup_configuration_file ( out_file_path , backup_directory , destination_file_prefix = backup_file_name ) for k , v in vars ( self ) . items (): if k not in self . extract_tokens : continue token_path = self . extract_tokens [ k ] update_dict_from_path ( token_path , v ) try : with open ( out_file_path , 'w' ) as config_yaml_f : if top_text : config_yaml_f . write ( f ' { top_text } \\n ' ) try : dump ( self . config_data , config_yaml_f , default_flow_style = False , Dumper = NoAliasDumper ) except RecursionError : dump ( self . config_data , config_yaml_f , default_flow_style = False ) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) except IOError : raise exceptions . WriteConfigError ( 'An error occurred while writing the configuration file to disk.' ) self . logger . warning ( 'Configuration updated. Restart this service to apply.' ) get_printable_config ( self , pretty_print = False ) Get the configuration as a dictionary object Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view False Returns: Type Description str A dictionary of config keys and values Source code in dynamite_nsm/services/base/config.py def get_printable_config ( self , pretty_print : Optional [ bool ] = False ) -> str : \"\"\" Get the configuration as a dictionary object Args: pretty_print: Print the log entry in a nice tabular view Returns: A dictionary of config keys and values \"\"\" reserved_keywords = [ 'logger' , 'config_data' , 'config_data_raw' , 'extract_tokens' ] variables = {} for var in vars ( self ): if var . startswith ( '_' ): continue elif var in reserved_keywords : continue variables [ var ] = getattr ( self , var ) if pretty_print : table = [[ 'Config Option' , 'Value' ]] table . extend ([( label , value ) for label , value in variables . items ()]) return tabulate ( table , tablefmt = 'fancy_grid' ) return json . dumps ( variables , indent = 1 ) parse_yaml_file ( self ) Parse the yaml file. Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def parse_yaml_file ( self ) -> None : \"\"\" Parse the yaml file. Returns: None \"\"\" def set_instance_var_from_token ( variable_name : str , data : Union [ Dict , List ]): \"\"\"Given a variable name, and data; create an instance variable (at parse-time) of that name Args: variable_name: The name of the instance variable to update data: The parsed yaml object Returns: True if successfully located \"\"\" if variable_name not in self . extract_tokens . keys (): return False key_path = self . extract_tokens [ variable_name ] value = data for k in key_path : if isinstance ( value , dict ): try : value = value [ k ] except KeyError : continue elif isinstance ( value , list ): for list_entry in value : if isinstance ( list_entry , dict ): if k in list_entry . keys (): value = list_entry [ k ] else : break setattr ( self , var_name , value ) return True for var_name in vars ( self ) . keys (): set_instance_var_from_token ( variable_name = var_name , data = self . config_data )","title":"config"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.BackupConfigManager","text":"Manage backup and restoration process across various service configs","title":"BackupConfigManager"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.BackupConfigManager.list_backup_configs","text":"List configuration backups Returns: Type Description List A list of dictionaries with the following keys [\"name\", \"path\", \"timestamp\"] Source code in dynamite_nsm/services/base/config.py def list_backup_configs ( self ) -> List : \"\"\"List configuration backups Returns: A list of dictionaries with the following keys [\"name\", \"path\", \"timestamp\"] \"\"\" return utilities . list_backup_configurations ( self . backup_directory )","title":"list_backup_configs()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.BackupConfigManager.restore_backup_config","text":"Restore a configuration from our config store :param backup_name: The name of the configuration file or the keyword \"recent\" which will restore the most recent backup. :param restore_name: The name of the configuration file to write too :return: True, if successful Source code in dynamite_nsm/services/base/config.py def restore_backup_config ( self , backup_name : str , restore_name : str ): \"\"\" Restore a configuration from our config store :param backup_name: The name of the configuration file or the keyword \"recent\" which will restore the most recent backup. :param restore_name: The name of the configuration file to write too :return: True, if successful \"\"\" if backup_name == \"recent\" : configs = self . list_backup_configs () if configs : return utilities . restore_backup_configuration ( configs [ 0 ][ 'filepath' ], restore_name ) return utilities . restore_backup_configuration ( os . path . join ( self . backup_directory , backup_name ), restore_name )","title":"restore_backup_config()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.GenericConfigManager","text":"","title":"GenericConfigManager"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.GenericConfigManager.__init__","text":"A catchall configuration manager, generic enough to work on any configuration like file Parameters: Name Type Description Default config_data Dict Configuration data dictionary required name str The name of the configuration required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Source code in dynamite_nsm/services/base/config.py def __init__ ( self , config_data : Dict , name : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" A catchall configuration manager, generic enough to work on any configuration like file Args: config_data: Configuration data dictionary name: The name of the configuration verbose: Include detailed debug messages stdout: Print output to console \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . config_data = config_data self . formatted_data = json . dumps ( config_data ) log_level = logging . INFO if verbose : log_level = logging . DEBUG self . stdout = stdout self . verbose = verbose self . logger = get_logger ( str ( name ), level = log_level , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.GenericConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path str The path to the output file required backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def commit ( self , out_file_path : str , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file backup_directory: The path to the backup directory Returns: None \"\"\" # Backup old configuration first out_file_name = os . path . basename ( out_file_path ) backup_file_name = out_file_name + '.backup' if backup_directory : utilities . backup_configuration_file ( out_file_path , backup_directory , destination_file_prefix = backup_file_name ) try : with open ( out_file_path , 'w' ) as config_raw_f : config_raw_f . write ( self . formatted_data ) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) except IOError as e : raise exceptions . WriteConfigError ( f 'An error occurred while writing the configuration file to disk. { e } ' ) self . logger . warning ( 'Configuration updated. Restart this service to apply.' )","title":"commit()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.GenericConfigManager.get_printable_config","text":"Get the configuration as a dictionary object Returns: Type Description Dict A dictionary of config keys and values Source code in dynamite_nsm/services/base/config.py def get_printable_config ( self ) -> Dict : \"\"\" Get the configuration as a dictionary object Returns: A dictionary of config keys and values \"\"\" variables = {} for var in vars ( self ): if var . startswith ( '_' ): continue variables [ var ] = str ( getattr ( self , var )) return variables","title":"get_printable_config()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.GenericConfigManager.reset","text":"Reset a configuration file back to its default !!! out_file_path \"The path to the output file\" default_config_path: The path to the default configuration Returns: Type Description None Source code in dynamite_nsm/services/base/config.py def reset ( self , out_file_path : Optional [ str ], default_config_path : Optional [ str ]): \"\"\"Reset a configuration file back to its default Args: out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" self . logger . info ( f 'Restoring { out_file_path } to default state.' ) with open ( default_config_path , 'r' ) as default_conf_f_in : with open ( out_file_path , 'w' ) as conf_f_out : conf_f_out . write ( default_conf_f_in . read ()) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path )","title":"reset()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.JavaOptionsConfigManager","text":"A special base configuration manager for jvm.options configurations","title":"JavaOptionsConfigManager"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.JavaOptionsConfigManager.__init__","text":"Work with jvm.options configurations Parameters: Name Type Description Default config_data Dict Configuration data dictionary required name str The name of the configuration required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Source code in dynamite_nsm/services/base/config.py def __init__ ( self , config_data : Dict , name : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Work with jvm.options configurations Args: config_data: Configuration data dictionary name: The name of the configuration verbose: Include detailed debug messages stdout: Print output to console \"\"\" super () . __init__ ( config_data , name = name , verbose = verbose , stdout = stdout ) self . initial_memory = None self . maximum_memory = None self . _raw_extra_params = None self . add_parser ( parser = lambda data : self . _parse_jvm_options ( data )[ 0 ], attribute_name = 'initial_memory' ) self . add_parser ( parser = lambda data : self . _parse_jvm_options ( data )[ 1 ], attribute_name = 'maximum_memory' ) self . add_parser ( parser = lambda data : self . _parse_jvm_options ( data )[ 2 ], attribute_name = '_raw_extra_params' )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.JavaOptionsConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory Returns: None \"\"\" # Backup old configuration first out_file_name = os . path . basename ( out_file_path ) backup_file_name = out_file_name + '.backup' self . formatted_data = f '-Xms { self . initial_memory } \\n -Xmx { self . maximum_memory } \\n ' + \\ ' \\n ' . join ( self . _raw_extra_params ) if backup_directory : utilities . backup_configuration_file ( out_file_path , backup_directory , destination_file_prefix = backup_file_name ) try : with open ( out_file_path , 'w' ) as config_raw_f : config_raw_f . write ( self . formatted_data ) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) utilities . set_permissions_of_file ( out_file_path , 644 ) except IOError : raise exceptions . WriteConfigError ( 'An error occurred while writing the configuration file to disk.' )","title":"commit()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.YamlConfigManager","text":"A configuration manager for working with any YAML formatted configuration file","title":"YamlConfigManager"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.YamlConfigManager.__init__","text":"Work with YAML based configuration files Parameters: Name Type Description Default config_data Dict Configuration data dictionary required name str The name of the configuration required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True **extract_tokens Dict A dictionary object, where the keys represent the names of instance variables to create {} Source code in dynamite_nsm/services/base/config.py def __init__ ( self , config_data : Dict , name : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True , ** extract_tokens : Dict ): \"\"\"Work with YAML based configuration files Args: config_data: Configuration data dictionary name: The name of the configuration verbose: Include detailed debug messages stdout: Print output to console **extract_tokens: A dictionary object, where the keys represent the names of instance variables to create if the path to that variable exists. Paths are given using dot notation or as a Tuple. \"\"\" super () . __init__ ( config_data , name , verbose , stdout ) if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . config_data = config_data self . extract_tokens = extract_tokens log_level = logging . INFO if verbose : log_level = logging . DEBUG self . stdout = stdout self . verbose = verbose self . logger = get_logger ( str ( name ), level = log_level , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.YamlConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The text to be appended at the top of the config file (typically used for YAML version header) None Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The text to be appended at the top of the config file (typically used for YAML version header) Returns: None \"\"\" out_file_name = os . path . basename ( out_file_path ) backup_file_name = out_file_name + '.backup' def update_dict_from_path ( path , value ) -> None : \"\"\"Update the internal YAML dictionary object with the new values from our config Args: path: A tuple representing each level of a nested path in the yaml document ('vars', 'address-groups', 'HOME_NET') = /vars/address-groups/HOME_NET value: The new value Returns: None \"\"\" partial_config_data = self . config_data if len ( path ) > 1 : for i in range ( 0 , len ( path ) - 1 ): k = path [ i ] if isinstance ( partial_config_data , dict ): partial_config_data = partial_config_data [ k ] elif isinstance ( partial_config_data , list ): for list_entry in partial_config_data : if isinstance ( list_entry , dict ): if k in list_entry . keys (): partial_config_data = list_entry [ k ] else : break if value is None : return partial_config_data . update ({ path [ - 1 ]: value }) # Backup old configuration first if backup_directory : utilities . backup_configuration_file ( out_file_path , backup_directory , destination_file_prefix = backup_file_name ) for k , v in vars ( self ) . items (): if k not in self . extract_tokens : continue token_path = self . extract_tokens [ k ] update_dict_from_path ( token_path , v ) try : with open ( out_file_path , 'w' ) as config_yaml_f : if top_text : config_yaml_f . write ( f ' { top_text } \\n ' ) try : dump ( self . config_data , config_yaml_f , default_flow_style = False , Dumper = NoAliasDumper ) except RecursionError : dump ( self . config_data , config_yaml_f , default_flow_style = False ) if utilities . is_root (): utilities . set_ownership_of_file ( out_file_path ) except IOError : raise exceptions . WriteConfigError ( 'An error occurred while writing the configuration file to disk.' ) self . logger . warning ( 'Configuration updated. Restart this service to apply.' )","title":"commit()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.YamlConfigManager.get_printable_config","text":"Get the configuration as a dictionary object Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view False Returns: Type Description str A dictionary of config keys and values Source code in dynamite_nsm/services/base/config.py def get_printable_config ( self , pretty_print : Optional [ bool ] = False ) -> str : \"\"\" Get the configuration as a dictionary object Args: pretty_print: Print the log entry in a nice tabular view Returns: A dictionary of config keys and values \"\"\" reserved_keywords = [ 'logger' , 'config_data' , 'config_data_raw' , 'extract_tokens' ] variables = {} for var in vars ( self ): if var . startswith ( '_' ): continue elif var in reserved_keywords : continue variables [ var ] = getattr ( self , var ) if pretty_print : table = [[ 'Config Option' , 'Value' ]] table . extend ([( label , value ) for label , value in variables . items ()]) return tabulate ( table , tablefmt = 'fancy_grid' ) return json . dumps ( variables , indent = 1 )","title":"get_printable_config()"},{"location":"guides/developers/SDK/services/base/config/#dynamite_nsm.services.base.config.YamlConfigManager.parse_yaml_file","text":"Parse the yaml file. Returns: Type Description None None Source code in dynamite_nsm/services/base/config.py def parse_yaml_file ( self ) -> None : \"\"\" Parse the yaml file. Returns: None \"\"\" def set_instance_var_from_token ( variable_name : str , data : Union [ Dict , List ]): \"\"\"Given a variable name, and data; create an instance variable (at parse-time) of that name Args: variable_name: The name of the instance variable to update data: The parsed yaml object Returns: True if successfully located \"\"\" if variable_name not in self . extract_tokens . keys (): return False key_path = self . extract_tokens [ variable_name ] value = data for k in key_path : if isinstance ( value , dict ): try : value = value [ k ] except KeyError : continue elif isinstance ( value , list ): for list_entry in value : if isinstance ( list_entry , dict ): if k in list_entry . keys (): value = list_entry [ k ] else : break setattr ( self , var_name , value ) return True for var_name in vars ( self ) . keys (): set_instance_var_from_token ( variable_name = var_name , data = self . config_data )","title":"parse_yaml_file()"},{"location":"guides/developers/SDK/services/base/install/","text":"Base installation helpers To import... from dynamite_nsm.services.base import install BaseInstallManager An interface used to assist with a variety of common service installation tasks __init__ ( self , name , verbose = False , requires_root = True , stdout = True , log_level = 20 ) special Build a custom service installer Parameters: Name Type Description Default name str The name of the service required requires_root Optional[bool] If True, then the uninstaller will check that the user is root True stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False log_level The minimum logging.LOG_LEVEL to be handled 20 Source code in dynamite_nsm/services/base/install.py def __init__ ( self , name : str , verbose : Optional [ bool ] = False , requires_root : Optional [ bool ] = True , stdout : Optional [ bool ] = True , log_level = logging . INFO ): \"\"\" Build a custom service installer Args: name: The name of the service requires_root: If True, then the uninstaller will check that the user is root stdout: Print output to console verbose: Include detailed debug messages log_level: The minimum logging.LOG_LEVEL to be handled \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () if requires_root and not utilities . is_root (): raise exceptions . RequiresRootError () if verbose : log_level = logging . DEBUG self . stdout = stdout self . verbose = verbose self . logger = get_logger ( str ( name ) . upper (), level = log_level , stdout = stdout ) self . dynamite_environ = utilities . get_environment_file_dict () utilities . makedirs ( const . INSTALL_CACHE ) utilities . create_dynamite_user () compile_source_package ( self , source_root_directory , compile_args = None , parallel_threads = None , expected_lines_printed = None ) A simple make; make install wrapper Parameters: Name Type Description Default source_root_directory str The directory containing the MAKEFILE required compile_args Optional[List[str]] make arguments None parallel_threads Optional[int] The number of parallel threads to use during compilation (jemalloc) None expected_lines_printed Optional[int] The number of lines produced by this process (used to generate a progressbar) None Returns: None Source code in dynamite_nsm/services/base/install.py def compile_source_package ( self , source_root_directory : str , compile_args : Optional [ List [ str ]] = None , parallel_threads : Optional [ int ] = None , expected_lines_printed : Optional [ int ] = None ) -> None : \"\"\"A simple make; make install wrapper Args: source_root_directory: The directory containing the MAKEFILE compile_args: make arguments parallel_threads: The number of parallel threads to use during compilation (jemalloc) expected_lines_printed: The number of lines produced by this process (used to generate a progressbar) Returns: None \"\"\" if not parallel_threads : parallel_threads = get_parallel_threads () if compile_args : compile_args . extend ([ '-j' , parallel_threads ]) else : compile_args = [ '-j' , parallel_threads ] temp_compile_args = [ f ' { const . SYS_BIN } /make' ] temp_compile_args . extend ( compile_args ) temp_compile_args = [ str ( a ) for a in temp_compile_args ] compile_args = temp_compile_args compile_args . extend ([ ';' , f ' { const . SYS_BIN } /make' , 'install' ]) self . logger . info ( f 'Compiling: { source_root_directory } .' ) self . logger . debug ( \" \" . join ( compile_args )) popen_make_args = dict ( args = ' ' . join ( compile_args ), shell = True , cwd = source_root_directory , ) if not self . verbose : popen_make_args [ 'stdout' ] = subprocess . PIPE popen_make_args [ 'stderr' ] = subprocess . PIPE ret = utilities . run_subprocess_with_status ( subprocess . Popen ( ** popen_make_args ), expected_lines = expected_lines_printed ) else : p = subprocess . Popen ( ** popen_make_args ) p . communicate () ret = p . returncode if ret != 0 : self . logger . error ( f 'Exited: { ret } ; Process Info: { compile_args } ' ) raise exceptions . CallProcessError ( f 'Exited with { ret } ' ) configure_source_package ( self , source_root_directory , configure_args = None ) A configure wrapper for the build/make process Parameters: Name Type Description Default source_root_directory str A directory containing the configuration.in file required configure_args Optional[List[str]] configure arguments None Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def configure_source_package ( self , source_root_directory : str , configure_args : Optional [ List [ str ]] = None ) -> None : \"\"\"A configure wrapper for the build/make process Args: source_root_directory: A directory containing the configuration.in file configure_args: configure arguments Returns: None \"\"\" temp_config_args = [ './configure' ] temp_config_args . extend ( configure_args ) temp_config_args = [ str ( a ) for a in temp_config_args ] configure_args = temp_config_args self . logger . info ( f 'Configuring build: { source_root_directory } .' ) self . logger . debug ( \" \" . join ( configure_args )) popen_args = dict ( args = ' ' . join ( configure_args ), shell = True , cwd = source_root_directory , ) if not self . verbose : popen_args [ 'stdout' ] = subprocess . PIPE popen_args [ 'stderr' ] = subprocess . PIPE ret = utilities . run_subprocess_with_status ( subprocess . Popen ( ** popen_args ), expected_lines = None ) else : p = subprocess . Popen ( ** popen_args ) p . communicate () ret = p . returncode if ret != 0 : self . logger . error ( f 'Exited: { ret } ; Process Info: { configure_args } ' ) raise exceptions . CallProcessError ( f 'Exited with { ret } ' ) copy_file_or_directory_to_destination ( self , file_or_dir , destination_file_or_dir ) Copy a file or directory to another file or directory Parameters: Name Type Description Default file_or_dir str The file or directory to copy required destination_file_or_dir str The file or directory destination required Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def copy_file_or_directory_to_destination ( self , file_or_dir : str , destination_file_or_dir : str ) -> None : \"\"\" Copy a file or directory to another file or directory Args: file_or_dir: The file or directory to copy destination_file_or_dir: The file or directory destination Returns: None \"\"\" file_or_dir = file_or_dir . rstrip ( '/' ) destination_location = f ' { destination_file_or_dir } / { os . path . basename ( file_or_dir ) } ' if os . path . isdir ( file_or_dir ): utilities . makedirs ( destination_location , exist_ok = True ) self . logger . debug ( f 'Creating directory: { destination_location } ' ) try : self . logger . debug ( f 'Copying directory { file_or_dir } -> { destination_location } ' ) utilities . copytree ( file_or_dir , destination_location ) except shutil . Error as e : if 'exist' in str ( e ): self . logger . warning ( f ' { destination_file_or_dir } directory already exists. Skipping.' ) else : raise e else : try : self . logger . debug ( f 'Copying file { file_or_dir } -> { destination_file_or_dir } ' ) shutil . copy ( file_or_dir , destination_file_or_dir ) except FileNotFoundError : parent_directory = os . path . dirname ( destination_file_or_dir ) self . logger . debug ( f 'Creating parent directory: { parent_directory } ' ) utilities . makedirs ( parent_directory ) shutil . copy ( file_or_dir , destination_file_or_dir ) except shutil . Error as e : if 'exist' in str ( e ): self . logger . warning ( f ' { destination_file_or_dir } file already exists. Skipping.' ) else : raise e create_update_env_variable ( self , name , value ) Write the environment variable into our root owned environment file Parameters: Name Type Description Default name str The name of the variable required value str The value of the variable required Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def create_update_env_variable ( self , name : str , value : str ) -> None : \"\"\"Write the environment variable into our root owned environment file Args: name: The name of the variable value: The value of the variable Returns: None \"\"\" name = str ( name ) value = str ( value ) env_file_path = f ' { const . CONFIG_PATH } /environment' if not os . path . exists ( env_file_path ): with open ( env_file_path , 'w' ) as env_f : env_f . write ( '' ) overwrite_line_no = - 1 with open ( env_file_path ) as env_fr : read_lines = env_fr . readlines () for idx , line in enumerate ( read_lines ): if str ( line ) . startswith ( name ): overwrite_line_no = idx break if overwrite_line_no == - 1 : with open ( env_file_path , 'a' ) as env_fa : env_fa . write ( f ' { name } = { value } \\n ' ) self . logger . debug ( f 'Setting { name } -> { value } ' ) else : self . logger . debug ( f 'Overwriting { name } -> { value } ' ) if value . endswith ( ' \\n ' ): read_lines [ overwrite_line_no ] = f ' { name } = { value } ' else : read_lines [ overwrite_line_no ] = f ' { name } = { value } \\n ' with open ( env_file_path , 'w' ) as env_fw : env_fw . writelines ( read_lines ) utilities . set_ownership_of_file ( env_file_path ) download_from_mirror ( self , mirror_path ) Download a Dynamite service from a mirror Parameters: Name Type Description Default mirror_path str The path to the mirror required Returns: Type Description Tuple[str, str, Optional[str]] The mirror url, archive name, and directory name (once the archive has been extracted) Source code in dynamite_nsm/services/base/install.py def download_from_mirror ( self , mirror_path : str ) -> Tuple [ str , str , Optional [ str ]]: \"\"\"Download a Dynamite service from a mirror Args: mirror_path: The path to the mirror Returns: The mirror url, archive name, and directory name (once the archive has been extracted) \"\"\" with open ( mirror_path ) as mirror_f : res , err = None , None for mirror in mirror_f . readlines (): try : url , archive_name , dir_name = [ token . strip () for token in mirror . split ( ',' )] except ValueError : url = mirror archive_name = os . path . basename ( url ) dir_name = None self . logger . info ( \"Downloading {} from {} \" . format ( archive_name , url )) fqdn_dir_name = f ' { const . INSTALL_CACHE } / { str ( dir_name ) } ' if os . path . exists ( fqdn_dir_name ): shutil . rmtree ( fqdn_dir_name , ignore_errors = True ) try : res = utilities . download_file ( url , archive_name , stdout = self . stdout ) except Exception as e : res , err = False , e self . logger . warning ( f 'Failed to download { archive_name } from { url } ; { e } ' ) if res : break if not res : self . logger . error ( f 'An error occurred while attempting to download: { err } ' ) raise exceptions . DownloadError ( f 'General error while attempting to download { archive_name } from all mirrors.' ) return url , archive_name , dir_name extract_archive ( archive_path ) staticmethod Extract a tar.gz archive to disk Parameters: Name Type Description Default archive_path str The full path to the archive. required Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py @staticmethod def extract_archive ( archive_path : str ) -> None : \"\"\" Extract a tar.gz archive to disk Args: archive_path: The full path to the archive. Returns: None \"\"\" try : tf = tarfile . open ( archive_path ) tf . extractall ( path = const . INSTALL_CACHE ) except IOError as e : raise exceptions . ArchiveExtractionError ( f 'Could not extract { archive_path } archive to { const . INSTALL_CACHE } ; { e } ' ) except Exception as e : raise exceptions . ArchiveExtractionError ( f 'General error while attempting to extract { archive_path } archive; { e } ' ) get_mirror_info ( mirror_path ) staticmethod Get information about a mirror Parameters: Name Type Description Default mirror_path str The path to the mirror required Returns: Type Description Tuple[str, str, Optional[str]] The mirror url, archive name, and directory name (once the archive has been extracted) Source code in dynamite_nsm/services/base/install.py @staticmethod def get_mirror_info ( mirror_path : str ) -> Tuple [ str , str , Optional [ str ]]: \"\"\" Get information about a mirror Args: mirror_path: The path to the mirror Returns: The mirror url, archive name, and directory name (once the archive has been extracted) \"\"\" with open ( mirror_path ) as mirror_f : for mirror in mirror_f . readlines (): try : url , archive_name , dir_name = [ token . strip () for token in mirror . split ( ',' )] except ValueError : url = mirror archive_name = os . path . basename ( url ) dir_name = None return url , archive_name , dir_name install_dependencies ( self , apt_get_packages = None , yum_packages = None , pre_install_function = None ) Install OS dependencies through the package manager Parameters: Name Type Description Default apt_get_packages Optional[List] The name of the packages available in apt-get supported repos None yum_packages Optional[List] The name of the packages available in yum supported repos None pre_install_function Optional[Callable] A Python function to run prior to installing these packages None Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def install_dependencies ( self , apt_get_packages : Optional [ List ] = None , yum_packages : Optional [ List ] = None , pre_install_function : Optional [ Callable ] = None ) -> None : \"\"\" Install OS dependencies through the package manager Args: apt_get_packages: The name of the packages available in apt-get supported repos yum_packages: The name of the packages available in yum supported repos pre_install_function: A Python function to run prior to installing these packages Returns: None \"\"\" pacman = package_manager . OSPackageManager ( stdout = self . stdout , verbose = self . verbose ) packages = [] if pacman . package_manager == 'apt-get' : self . logger . info ( 'apt-get detected. We will use this package manager to install dependencies.' ) packages = apt_get_packages elif pacman . package_manager == 'yum' : self . logger . info ( 'yum detected. We will use this package manager to install dependencies.' ) packages = yum_packages self . logger . info ( 'Refreshing package indexes' ) if pre_install_function : self . logger . info ( 'Running pre-installation function.' ) pre_install_function ( pacman . package_manager ) pacman . refresh_package_indexes () self . logger . debug ( f 'Packages: { packages } ' ) if packages : self . logger . info ( f 'Installing { len ( packages ) } new packages.' ) pacman . install_packages ( packages ) validate_inspect_interfaces ( inspect_interfaces ) staticmethod Determine if one or more capture interface actually exists Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interface names to evaluate. required Returns: Type Description bool True, if all interfaces are valid Source code in dynamite_nsm/services/base/install.py @staticmethod def validate_inspect_interfaces ( inspect_interfaces : List [ str ]) -> bool : \"\"\" Determine if one or more capture interface actually exists Args: inspect_interfaces: A list of network interface names to evaluate. Returns: True, if all interfaces are valid \"\"\" for interface in inspect_interfaces : if interface not in utilities . get_network_interface_names (): return False return True BaseUninstallManager An interface used to assist with a variety of common service uninstall tasks __init__ ( self , name , directories , environ_vars = None , process = None , sysctl_service_name = None , requires_root = True , verbose = False , stdout = True , log_level = 20 ) special Remove installed files for a given service Parameters: Name Type Description Default name str The name of the process required directories List[str] The directories to be removed required process Optional[dynamite_nsm.services.base.process.BaseProcessManager] The process to be terminated None sysctl_service_name Optional[str] The name any associated systemd unit file. None requires_root Optional[bool] If True, then the uninstaller will check that the user is root True stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False log_level The logging.LOG_LEVEL to use when logging 20 Source code in dynamite_nsm/services/base/install.py def __init__ ( self , name : str , directories : List [ str ], environ_vars : Optional [ List [ str ]] = None , process : Optional [ process . BaseProcessManager ] = None , sysctl_service_name : Optional [ str ] = None , requires_root : Optional [ bool ] = True , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True , log_level = logging . INFO ): \"\"\"Remove installed files for a given service Args: name: The name of the process directories: The directories to be removed process: The process to be terminated sysctl_service_name: The name any associated systemd unit file. requires_root: If True, then the uninstaller will check that the user is root stdout: Print output to console verbose: Include detailed debug messages log_level: The logging.LOG_LEVEL to use when logging \"\"\" self . name = name self . directories = directories self . environ_vars = environ_vars self . process = process self . verbose = verbose self . sysctl_service_name = sysctl_service_name self . stdout = stdout if verbose : log_level = logging . DEBUG self . logger = get_logger ( str ( name ) . upper (), level = log_level , stdout = stdout ) if requires_root and not utilities . is_root (): raise exceptions . RequiresRootError () uninstall ( self ) Stop and uninstall the service Returns: Type Description None Source code in dynamite_nsm/services/base/install.py def uninstall ( self ): \"\"\"Stop and uninstall the service Returns: None \"\"\" sysctl = systemctl . SystemCtl ( stdout = self . stdout , verbose = self . verbose ) if self . process : self . process . stop () for dir in self . directories : self . logger . info ( f 'Removing { dir } ' ) shutil . rmtree ( dir , ignore_errors = True ) if self . environ_vars : for var in self . environ_vars : self . delete_env_variable ( var ) if self . sysctl_service_name : try : self . logger . info ( f 'Uninstalling { self . sysctl_service_name } ' ) sysctl . uninstall_and_disable ( self . sysctl_service_name ) except FileNotFoundError : self . logger . debug ( 'Skipping service uninstallation as systemd was not implemented in this setup.' ) self . logger . info ( f 'Successfully uninstalled { self . name } ' ) NetworkInterfaceNotFound Thrown when attempting to disable a non-existing interface __init__ ( self , interfaces ) special :param interfaces: A network interface Source code in dynamite_nsm/services/base/install.py def __init__ ( self , interfaces : Union [ str , List ]): \"\"\" :param interfaces: A network interface \"\"\" msg = f 'Network interface(s) does not exist: { interfaces } .' super ( NetworkInterfaceNotFound , self ) . __init__ ( msg )","title":"install"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager","text":"An interface used to assist with a variety of common service installation tasks","title":"BaseInstallManager"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.__init__","text":"Build a custom service installer Parameters: Name Type Description Default name str The name of the service required requires_root Optional[bool] If True, then the uninstaller will check that the user is root True stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False log_level The minimum logging.LOG_LEVEL to be handled 20 Source code in dynamite_nsm/services/base/install.py def __init__ ( self , name : str , verbose : Optional [ bool ] = False , requires_root : Optional [ bool ] = True , stdout : Optional [ bool ] = True , log_level = logging . INFO ): \"\"\" Build a custom service installer Args: name: The name of the service requires_root: If True, then the uninstaller will check that the user is root stdout: Print output to console verbose: Include detailed debug messages log_level: The minimum logging.LOG_LEVEL to be handled \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () if requires_root and not utilities . is_root (): raise exceptions . RequiresRootError () if verbose : log_level = logging . DEBUG self . stdout = stdout self . verbose = verbose self . logger = get_logger ( str ( name ) . upper (), level = log_level , stdout = stdout ) self . dynamite_environ = utilities . get_environment_file_dict () utilities . makedirs ( const . INSTALL_CACHE ) utilities . create_dynamite_user ()","title":"__init__()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.compile_source_package","text":"A simple make; make install wrapper Parameters: Name Type Description Default source_root_directory str The directory containing the MAKEFILE required compile_args Optional[List[str]] make arguments None parallel_threads Optional[int] The number of parallel threads to use during compilation (jemalloc) None expected_lines_printed Optional[int] The number of lines produced by this process (used to generate a progressbar) None Returns: None Source code in dynamite_nsm/services/base/install.py def compile_source_package ( self , source_root_directory : str , compile_args : Optional [ List [ str ]] = None , parallel_threads : Optional [ int ] = None , expected_lines_printed : Optional [ int ] = None ) -> None : \"\"\"A simple make; make install wrapper Args: source_root_directory: The directory containing the MAKEFILE compile_args: make arguments parallel_threads: The number of parallel threads to use during compilation (jemalloc) expected_lines_printed: The number of lines produced by this process (used to generate a progressbar) Returns: None \"\"\" if not parallel_threads : parallel_threads = get_parallel_threads () if compile_args : compile_args . extend ([ '-j' , parallel_threads ]) else : compile_args = [ '-j' , parallel_threads ] temp_compile_args = [ f ' { const . SYS_BIN } /make' ] temp_compile_args . extend ( compile_args ) temp_compile_args = [ str ( a ) for a in temp_compile_args ] compile_args = temp_compile_args compile_args . extend ([ ';' , f ' { const . SYS_BIN } /make' , 'install' ]) self . logger . info ( f 'Compiling: { source_root_directory } .' ) self . logger . debug ( \" \" . join ( compile_args )) popen_make_args = dict ( args = ' ' . join ( compile_args ), shell = True , cwd = source_root_directory , ) if not self . verbose : popen_make_args [ 'stdout' ] = subprocess . PIPE popen_make_args [ 'stderr' ] = subprocess . PIPE ret = utilities . run_subprocess_with_status ( subprocess . Popen ( ** popen_make_args ), expected_lines = expected_lines_printed ) else : p = subprocess . Popen ( ** popen_make_args ) p . communicate () ret = p . returncode if ret != 0 : self . logger . error ( f 'Exited: { ret } ; Process Info: { compile_args } ' ) raise exceptions . CallProcessError ( f 'Exited with { ret } ' )","title":"compile_source_package()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.configure_source_package","text":"A configure wrapper for the build/make process Parameters: Name Type Description Default source_root_directory str A directory containing the configuration.in file required configure_args Optional[List[str]] configure arguments None Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def configure_source_package ( self , source_root_directory : str , configure_args : Optional [ List [ str ]] = None ) -> None : \"\"\"A configure wrapper for the build/make process Args: source_root_directory: A directory containing the configuration.in file configure_args: configure arguments Returns: None \"\"\" temp_config_args = [ './configure' ] temp_config_args . extend ( configure_args ) temp_config_args = [ str ( a ) for a in temp_config_args ] configure_args = temp_config_args self . logger . info ( f 'Configuring build: { source_root_directory } .' ) self . logger . debug ( \" \" . join ( configure_args )) popen_args = dict ( args = ' ' . join ( configure_args ), shell = True , cwd = source_root_directory , ) if not self . verbose : popen_args [ 'stdout' ] = subprocess . PIPE popen_args [ 'stderr' ] = subprocess . PIPE ret = utilities . run_subprocess_with_status ( subprocess . Popen ( ** popen_args ), expected_lines = None ) else : p = subprocess . Popen ( ** popen_args ) p . communicate () ret = p . returncode if ret != 0 : self . logger . error ( f 'Exited: { ret } ; Process Info: { configure_args } ' ) raise exceptions . CallProcessError ( f 'Exited with { ret } ' )","title":"configure_source_package()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.copy_file_or_directory_to_destination","text":"Copy a file or directory to another file or directory Parameters: Name Type Description Default file_or_dir str The file or directory to copy required destination_file_or_dir str The file or directory destination required Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def copy_file_or_directory_to_destination ( self , file_or_dir : str , destination_file_or_dir : str ) -> None : \"\"\" Copy a file or directory to another file or directory Args: file_or_dir: The file or directory to copy destination_file_or_dir: The file or directory destination Returns: None \"\"\" file_or_dir = file_or_dir . rstrip ( '/' ) destination_location = f ' { destination_file_or_dir } / { os . path . basename ( file_or_dir ) } ' if os . path . isdir ( file_or_dir ): utilities . makedirs ( destination_location , exist_ok = True ) self . logger . debug ( f 'Creating directory: { destination_location } ' ) try : self . logger . debug ( f 'Copying directory { file_or_dir } -> { destination_location } ' ) utilities . copytree ( file_or_dir , destination_location ) except shutil . Error as e : if 'exist' in str ( e ): self . logger . warning ( f ' { destination_file_or_dir } directory already exists. Skipping.' ) else : raise e else : try : self . logger . debug ( f 'Copying file { file_or_dir } -> { destination_file_or_dir } ' ) shutil . copy ( file_or_dir , destination_file_or_dir ) except FileNotFoundError : parent_directory = os . path . dirname ( destination_file_or_dir ) self . logger . debug ( f 'Creating parent directory: { parent_directory } ' ) utilities . makedirs ( parent_directory ) shutil . copy ( file_or_dir , destination_file_or_dir ) except shutil . Error as e : if 'exist' in str ( e ): self . logger . warning ( f ' { destination_file_or_dir } file already exists. Skipping.' ) else : raise e","title":"copy_file_or_directory_to_destination()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.create_update_env_variable","text":"Write the environment variable into our root owned environment file Parameters: Name Type Description Default name str The name of the variable required value str The value of the variable required Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def create_update_env_variable ( self , name : str , value : str ) -> None : \"\"\"Write the environment variable into our root owned environment file Args: name: The name of the variable value: The value of the variable Returns: None \"\"\" name = str ( name ) value = str ( value ) env_file_path = f ' { const . CONFIG_PATH } /environment' if not os . path . exists ( env_file_path ): with open ( env_file_path , 'w' ) as env_f : env_f . write ( '' ) overwrite_line_no = - 1 with open ( env_file_path ) as env_fr : read_lines = env_fr . readlines () for idx , line in enumerate ( read_lines ): if str ( line ) . startswith ( name ): overwrite_line_no = idx break if overwrite_line_no == - 1 : with open ( env_file_path , 'a' ) as env_fa : env_fa . write ( f ' { name } = { value } \\n ' ) self . logger . debug ( f 'Setting { name } -> { value } ' ) else : self . logger . debug ( f 'Overwriting { name } -> { value } ' ) if value . endswith ( ' \\n ' ): read_lines [ overwrite_line_no ] = f ' { name } = { value } ' else : read_lines [ overwrite_line_no ] = f ' { name } = { value } \\n ' with open ( env_file_path , 'w' ) as env_fw : env_fw . writelines ( read_lines ) utilities . set_ownership_of_file ( env_file_path )","title":"create_update_env_variable()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.download_from_mirror","text":"Download a Dynamite service from a mirror Parameters: Name Type Description Default mirror_path str The path to the mirror required Returns: Type Description Tuple[str, str, Optional[str]] The mirror url, archive name, and directory name (once the archive has been extracted) Source code in dynamite_nsm/services/base/install.py def download_from_mirror ( self , mirror_path : str ) -> Tuple [ str , str , Optional [ str ]]: \"\"\"Download a Dynamite service from a mirror Args: mirror_path: The path to the mirror Returns: The mirror url, archive name, and directory name (once the archive has been extracted) \"\"\" with open ( mirror_path ) as mirror_f : res , err = None , None for mirror in mirror_f . readlines (): try : url , archive_name , dir_name = [ token . strip () for token in mirror . split ( ',' )] except ValueError : url = mirror archive_name = os . path . basename ( url ) dir_name = None self . logger . info ( \"Downloading {} from {} \" . format ( archive_name , url )) fqdn_dir_name = f ' { const . INSTALL_CACHE } / { str ( dir_name ) } ' if os . path . exists ( fqdn_dir_name ): shutil . rmtree ( fqdn_dir_name , ignore_errors = True ) try : res = utilities . download_file ( url , archive_name , stdout = self . stdout ) except Exception as e : res , err = False , e self . logger . warning ( f 'Failed to download { archive_name } from { url } ; { e } ' ) if res : break if not res : self . logger . error ( f 'An error occurred while attempting to download: { err } ' ) raise exceptions . DownloadError ( f 'General error while attempting to download { archive_name } from all mirrors.' ) return url , archive_name , dir_name","title":"download_from_mirror()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.extract_archive","text":"Extract a tar.gz archive to disk Parameters: Name Type Description Default archive_path str The full path to the archive. required Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py @staticmethod def extract_archive ( archive_path : str ) -> None : \"\"\" Extract a tar.gz archive to disk Args: archive_path: The full path to the archive. Returns: None \"\"\" try : tf = tarfile . open ( archive_path ) tf . extractall ( path = const . INSTALL_CACHE ) except IOError as e : raise exceptions . ArchiveExtractionError ( f 'Could not extract { archive_path } archive to { const . INSTALL_CACHE } ; { e } ' ) except Exception as e : raise exceptions . ArchiveExtractionError ( f 'General error while attempting to extract { archive_path } archive; { e } ' )","title":"extract_archive()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.get_mirror_info","text":"Get information about a mirror Parameters: Name Type Description Default mirror_path str The path to the mirror required Returns: Type Description Tuple[str, str, Optional[str]] The mirror url, archive name, and directory name (once the archive has been extracted) Source code in dynamite_nsm/services/base/install.py @staticmethod def get_mirror_info ( mirror_path : str ) -> Tuple [ str , str , Optional [ str ]]: \"\"\" Get information about a mirror Args: mirror_path: The path to the mirror Returns: The mirror url, archive name, and directory name (once the archive has been extracted) \"\"\" with open ( mirror_path ) as mirror_f : for mirror in mirror_f . readlines (): try : url , archive_name , dir_name = [ token . strip () for token in mirror . split ( ',' )] except ValueError : url = mirror archive_name = os . path . basename ( url ) dir_name = None return url , archive_name , dir_name","title":"get_mirror_info()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.install_dependencies","text":"Install OS dependencies through the package manager Parameters: Name Type Description Default apt_get_packages Optional[List] The name of the packages available in apt-get supported repos None yum_packages Optional[List] The name of the packages available in yum supported repos None pre_install_function Optional[Callable] A Python function to run prior to installing these packages None Returns: Type Description None None Source code in dynamite_nsm/services/base/install.py def install_dependencies ( self , apt_get_packages : Optional [ List ] = None , yum_packages : Optional [ List ] = None , pre_install_function : Optional [ Callable ] = None ) -> None : \"\"\" Install OS dependencies through the package manager Args: apt_get_packages: The name of the packages available in apt-get supported repos yum_packages: The name of the packages available in yum supported repos pre_install_function: A Python function to run prior to installing these packages Returns: None \"\"\" pacman = package_manager . OSPackageManager ( stdout = self . stdout , verbose = self . verbose ) packages = [] if pacman . package_manager == 'apt-get' : self . logger . info ( 'apt-get detected. We will use this package manager to install dependencies.' ) packages = apt_get_packages elif pacman . package_manager == 'yum' : self . logger . info ( 'yum detected. We will use this package manager to install dependencies.' ) packages = yum_packages self . logger . info ( 'Refreshing package indexes' ) if pre_install_function : self . logger . info ( 'Running pre-installation function.' ) pre_install_function ( pacman . package_manager ) pacman . refresh_package_indexes () self . logger . debug ( f 'Packages: { packages } ' ) if packages : self . logger . info ( f 'Installing { len ( packages ) } new packages.' ) pacman . install_packages ( packages )","title":"install_dependencies()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseInstallManager.validate_inspect_interfaces","text":"Determine if one or more capture interface actually exists Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interface names to evaluate. required Returns: Type Description bool True, if all interfaces are valid Source code in dynamite_nsm/services/base/install.py @staticmethod def validate_inspect_interfaces ( inspect_interfaces : List [ str ]) -> bool : \"\"\" Determine if one or more capture interface actually exists Args: inspect_interfaces: A list of network interface names to evaluate. Returns: True, if all interfaces are valid \"\"\" for interface in inspect_interfaces : if interface not in utilities . get_network_interface_names (): return False return True","title":"validate_inspect_interfaces()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseUninstallManager","text":"An interface used to assist with a variety of common service uninstall tasks","title":"BaseUninstallManager"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseUninstallManager.__init__","text":"Remove installed files for a given service Parameters: Name Type Description Default name str The name of the process required directories List[str] The directories to be removed required process Optional[dynamite_nsm.services.base.process.BaseProcessManager] The process to be terminated None sysctl_service_name Optional[str] The name any associated systemd unit file. None requires_root Optional[bool] If True, then the uninstaller will check that the user is root True stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False log_level The logging.LOG_LEVEL to use when logging 20 Source code in dynamite_nsm/services/base/install.py def __init__ ( self , name : str , directories : List [ str ], environ_vars : Optional [ List [ str ]] = None , process : Optional [ process . BaseProcessManager ] = None , sysctl_service_name : Optional [ str ] = None , requires_root : Optional [ bool ] = True , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True , log_level = logging . INFO ): \"\"\"Remove installed files for a given service Args: name: The name of the process directories: The directories to be removed process: The process to be terminated sysctl_service_name: The name any associated systemd unit file. requires_root: If True, then the uninstaller will check that the user is root stdout: Print output to console verbose: Include detailed debug messages log_level: The logging.LOG_LEVEL to use when logging \"\"\" self . name = name self . directories = directories self . environ_vars = environ_vars self . process = process self . verbose = verbose self . sysctl_service_name = sysctl_service_name self . stdout = stdout if verbose : log_level = logging . DEBUG self . logger = get_logger ( str ( name ) . upper (), level = log_level , stdout = stdout ) if requires_root and not utilities . is_root (): raise exceptions . RequiresRootError ()","title":"__init__()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.BaseUninstallManager.uninstall","text":"Stop and uninstall the service Returns: Type Description None Source code in dynamite_nsm/services/base/install.py def uninstall ( self ): \"\"\"Stop and uninstall the service Returns: None \"\"\" sysctl = systemctl . SystemCtl ( stdout = self . stdout , verbose = self . verbose ) if self . process : self . process . stop () for dir in self . directories : self . logger . info ( f 'Removing { dir } ' ) shutil . rmtree ( dir , ignore_errors = True ) if self . environ_vars : for var in self . environ_vars : self . delete_env_variable ( var ) if self . sysctl_service_name : try : self . logger . info ( f 'Uninstalling { self . sysctl_service_name } ' ) sysctl . uninstall_and_disable ( self . sysctl_service_name ) except FileNotFoundError : self . logger . debug ( 'Skipping service uninstallation as systemd was not implemented in this setup.' ) self . logger . info ( f 'Successfully uninstalled { self . name } ' )","title":"uninstall()"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.NetworkInterfaceNotFound","text":"Thrown when attempting to disable a non-existing interface","title":"NetworkInterfaceNotFound"},{"location":"guides/developers/SDK/services/base/install/#dynamite_nsm.services.base.install.NetworkInterfaceNotFound.__init__","text":":param interfaces: A network interface Source code in dynamite_nsm/services/base/install.py def __init__ ( self , interfaces : Union [ str , List ]): \"\"\" :param interfaces: A network interface \"\"\" msg = f 'Network interface(s) does not exist: { interfaces } .' super ( NetworkInterfaceNotFound , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/logs/","text":"Base log objects To import... from dynamite_nsm.services.base import logs LogFile __init__ ( self , log_path , log_sample_size = 500 , gzip_decode = False ) special A Generic log object Parameters: Name Type Description Default log_path str The path to a log file required log_sample_size Optional[int] The number of most recent entries to include 500 gzip_decode Optional[bool] If True, we'll decode the log before reading it in False Source code in dynamite_nsm/services/base/logs.py def __init__ ( self , log_path : str , log_sample_size : Optional [ int ] = 500 , gzip_decode : Optional [ bool ] = False ): \"\"\"A Generic log object Args: log_path: The path to a log file log_sample_size: The number of most recent entries to include gzip_decode: If True, we'll decode the log before reading it in \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . log_path = log_path self . log_sample_size = log_sample_size self . exists = False self . current_line = 0 if gzip_decode and not log_path . endswith ( '.decoded' ): decoded_log_path = log_path + '.decoded' if not os . path . exists ( decoded_log_path ): with open ( decoded_log_path , 'w' ) as out : with gzip . open ( log_path , 'rb' ) as f : line = f . readline () . decode ( 'utf-8' , errors = 'ignore' ) while line : out . write ( line ) try : line = f . readline () . decode ( 'utf-8' , errors = 'ignore' ) except gzip . BadGzipFile : pass self . log_path = decoded_log_path linecache . updatecache ( self . log_path ) self . last_line_num = self . find_latest_line_offset () if self . last_line_num < self . log_sample_size : self . entries = [ entry for entry in self . iter_cache ( start = 1 )] else : self . entries = [ entry for entry in self . iter_cache ( start = self . last_line_num - self . log_sample_size + 1 )] find_latest_line_offset ( self , step = 500000 ) Relatively fast way of finding the latest offset; algorithm guesses high offset and if over divides the step by half and repeats Parameters: Name Type Description Default step Optional[int] The starting step between line offsets 500000 Returns: Type Description int Most recent line number Source code in dynamite_nsm/services/base/logs.py def find_latest_line_offset ( self , step : Optional [ int ] = 500000 ) -> int : \"\"\"Relatively fast way of finding the latest offset; algorithm guesses high offset and if over divides the step by half and repeats Args: step: The starting step between line offsets Returns: Most recent line number \"\"\" offset = 1 while step > 0 : for _ in self . iter_cache ( start = offset , step = step ): offset += step step = int ( step / 2 ) offset -= step return offset iter_cache ( self , start = 1 , step = 1 ) Relatively Memory efficient method of accessing very large files on disk Parameters: Name Type Description Default start Optional[int] The starting line 1 step Optional[int] The step between line offsets 1 Returns: Type Description Generator The line at a particular offset Source code in dynamite_nsm/services/base/logs.py def iter_cache ( self , start : Optional [ int ] = 1 , step : Optional [ int ] = 1 ) -> Generator : \"\"\"Relatively Memory efficient method of accessing very large files on disk Args: start: The starting line step: The step between line offsets Returns: The line at a particular offset \"\"\" i = start while True : line = linecache . getline ( self . log_path , i ) if line : yield line else : break i += step refresh ( self ) Refresh linecache Returns: Type Description None None Source code in dynamite_nsm/services/base/logs.py def refresh ( self ) -> None : \"\"\" Refresh linecache Returns: None \"\"\" linecache . updatecache ( self . log_path ) if self . last_line_num < self . log_sample_size : self . entries = [ entry for entry in self . iter_cache ( start = 1 )] else : self . entries = [ entry for entry in self . iter_cache ( start = self . last_line_num - self . log_sample_size + 1 )] size ( self ) Get the log file size with last offset reached Returns: A LogFileSize object containing the latest line offset and the total number of log entries available Source code in dynamite_nsm/services/base/logs.py def size ( self ) -> LogFileSize : \"\"\" Get the log file size with last offset reached Returns: A LogFileSize object containing the latest line offset and the total number of log entries available \"\"\" return LogFileSize ( self . find_latest_line_offset (), len ( self . entries )) LogFileSize __init__ ( self , file_line_count , loaded_entries ) special A simple object that represents the latest line offset reached and the number of entries loaded into memory. Parameters: Name Type Description Default file_line_count int The offset of the last reached (cached) line required loaded_entries int The number of entries loaded into memory required Source code in dynamite_nsm/services/base/logs.py def __init__ ( self , file_line_count : int , loaded_entries : int ): \"\"\" A simple object that represents the latest line offset reached and the number of entries loaded into memory. Args: file_line_count: The offset of the last reached (cached) line loaded_entries: The number of entries loaded into memory \"\"\" self . file_line_count = file_line_count self . loaded_entries = loaded_entries","title":"logs"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFile","text":"","title":"LogFile"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFile.__init__","text":"A Generic log object Parameters: Name Type Description Default log_path str The path to a log file required log_sample_size Optional[int] The number of most recent entries to include 500 gzip_decode Optional[bool] If True, we'll decode the log before reading it in False Source code in dynamite_nsm/services/base/logs.py def __init__ ( self , log_path : str , log_sample_size : Optional [ int ] = 500 , gzip_decode : Optional [ bool ] = False ): \"\"\"A Generic log object Args: log_path: The path to a log file log_sample_size: The number of most recent entries to include gzip_decode: If True, we'll decode the log before reading it in \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . log_path = log_path self . log_sample_size = log_sample_size self . exists = False self . current_line = 0 if gzip_decode and not log_path . endswith ( '.decoded' ): decoded_log_path = log_path + '.decoded' if not os . path . exists ( decoded_log_path ): with open ( decoded_log_path , 'w' ) as out : with gzip . open ( log_path , 'rb' ) as f : line = f . readline () . decode ( 'utf-8' , errors = 'ignore' ) while line : out . write ( line ) try : line = f . readline () . decode ( 'utf-8' , errors = 'ignore' ) except gzip . BadGzipFile : pass self . log_path = decoded_log_path linecache . updatecache ( self . log_path ) self . last_line_num = self . find_latest_line_offset () if self . last_line_num < self . log_sample_size : self . entries = [ entry for entry in self . iter_cache ( start = 1 )] else : self . entries = [ entry for entry in self . iter_cache ( start = self . last_line_num - self . log_sample_size + 1 )]","title":"__init__()"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFile.find_latest_line_offset","text":"Relatively fast way of finding the latest offset; algorithm guesses high offset and if over divides the step by half and repeats Parameters: Name Type Description Default step Optional[int] The starting step between line offsets 500000 Returns: Type Description int Most recent line number Source code in dynamite_nsm/services/base/logs.py def find_latest_line_offset ( self , step : Optional [ int ] = 500000 ) -> int : \"\"\"Relatively fast way of finding the latest offset; algorithm guesses high offset and if over divides the step by half and repeats Args: step: The starting step between line offsets Returns: Most recent line number \"\"\" offset = 1 while step > 0 : for _ in self . iter_cache ( start = offset , step = step ): offset += step step = int ( step / 2 ) offset -= step return offset","title":"find_latest_line_offset()"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFile.iter_cache","text":"Relatively Memory efficient method of accessing very large files on disk Parameters: Name Type Description Default start Optional[int] The starting line 1 step Optional[int] The step between line offsets 1 Returns: Type Description Generator The line at a particular offset Source code in dynamite_nsm/services/base/logs.py def iter_cache ( self , start : Optional [ int ] = 1 , step : Optional [ int ] = 1 ) -> Generator : \"\"\"Relatively Memory efficient method of accessing very large files on disk Args: start: The starting line step: The step between line offsets Returns: The line at a particular offset \"\"\" i = start while True : line = linecache . getline ( self . log_path , i ) if line : yield line else : break i += step","title":"iter_cache()"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFile.refresh","text":"Refresh linecache Returns: Type Description None None Source code in dynamite_nsm/services/base/logs.py def refresh ( self ) -> None : \"\"\" Refresh linecache Returns: None \"\"\" linecache . updatecache ( self . log_path ) if self . last_line_num < self . log_sample_size : self . entries = [ entry for entry in self . iter_cache ( start = 1 )] else : self . entries = [ entry for entry in self . iter_cache ( start = self . last_line_num - self . log_sample_size + 1 )]","title":"refresh()"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFile.size","text":"Get the log file size with last offset reached Returns: A LogFileSize object containing the latest line offset and the total number of log entries available Source code in dynamite_nsm/services/base/logs.py def size ( self ) -> LogFileSize : \"\"\" Get the log file size with last offset reached Returns: A LogFileSize object containing the latest line offset and the total number of log entries available \"\"\" return LogFileSize ( self . find_latest_line_offset (), len ( self . entries ))","title":"size()"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFileSize","text":"","title":"LogFileSize"},{"location":"guides/developers/SDK/services/base/logs/#dynamite_nsm.services.base.logs.LogFileSize.__init__","text":"A simple object that represents the latest line offset reached and the number of entries loaded into memory. Parameters: Name Type Description Default file_line_count int The offset of the last reached (cached) line required loaded_entries int The number of entries loaded into memory required Source code in dynamite_nsm/services/base/logs.py def __init__ ( self , file_line_count : int , loaded_entries : int ): \"\"\" A simple object that represents the latest line offset reached and the number of entries loaded into memory. Args: file_line_count: The offset of the last reached (cached) line loaded_entries: The number of entries loaded into memory \"\"\" self . file_line_count = file_line_count self . loaded_entries = loaded_entries","title":"__init__()"},{"location":"guides/developers/SDK/services/base/process/","text":"Base process managers To import... from dynamite_nsm.services.base import process BaseProcessManager A Systemd wrapper for process management __init__ ( self , systemd_service , name , log_path = None , create_pid_file = False , stdout = True , verbose = False , pretty_print_status = False ) special Manage a service process Parameters: Name Type Description Default systemd_service str The name of the systemd.service file required name str The name of the process manager required log_path Optional[str] The path to where the process logs None create_pid_file Optional[bool] If true will attempt to create a PID file False stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Source code in dynamite_nsm/services/base/process.py def __init__ ( self , systemd_service : str , name : str , log_path : Optional [ str ] = None , create_pid_file : Optional [ bool ] = False , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage a service process Args: systemd_service: The name of the systemd.service file name: The name of the process manager log_path: The path to where the process logs create_pid_file: If true will attempt to create a PID file stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( name , level = log_level , stdout = stdout ) self . pid_file = None self . pid = None self . systemd_service = systemd_service self . name = name self . log_path = log_path if create_pid_file : self . pid_file = f ' { const . PID_PATH } / { name } .pid' self . stdout = stdout self . verbose = verbose self . pretty_print_status = pretty_print_status self . sysctl = systemctl . SystemCtl () if create_pid_file : self . pid = self . _get_pid ( self . pid_file ) disable ( self ) Disable process Returns: Type Description bool True, if successfully disabled Source code in dynamite_nsm/services/base/process.py def disable ( self ) -> bool : \"\"\"Disable process Returns: True, if successfully disabled \"\"\" self . logger . info ( 'Disabling on startup: {} ' . format ( self . systemd_service )) return self . sysctl . disable ( self . systemd_service , daemon_reload = True ) enable ( self ) Enabled process Returns: Type Description bool True, if successfully enabled Source code in dynamite_nsm/services/base/process.py def enable ( self ) -> bool : \"\"\"Enabled process Returns: True, if successfully enabled \"\"\" self . logger . info ( 'Enabling on startup: {} ' . format ( self . systemd_service )) return self . sysctl . enable ( self . systemd_service , daemon_reload = True ) restart ( self ) Restart Process Returns: Type Description bool True, if the process was restarted Source code in dynamite_nsm/services/base/process.py def restart ( self ) -> bool : \"\"\"Restart Process Returns: True, if the process was restarted \"\"\" self . logger . info ( 'Attempting to restart {} ' . format ( self . systemd_service )) return self . sysctl . restart ( self . systemd_service ) start ( self ) Start process Returns: Type Description bool True, if successfully started Source code in dynamite_nsm/services/base/process.py def start ( self ) -> bool : \"\"\"Start process Returns: True, if successfully started \"\"\" self . logger . info ( 'Attempting to start {} ' . format ( self . systemd_service )) return self . sysctl . start ( self . systemd_service ) status ( self ) Get the status of a process Returns: Type Description Union[Dict, str] A dictionary containing process status or a tabulated string if pretty_print is True. Source code in dynamite_nsm/services/base/process.py def status ( self ) -> Union [ Dict , str ]: \"\"\"Get the status of a process Returns: A dictionary containing process status or a tabulated string if `pretty_print` is True. \"\"\" if self . pid_file : self . pid = self . _get_pid ( self . pid_file ) systemd_info = self . sysctl . status ( self . systemd_service ) info_dict = { 'command' : systemd_info . cmd , 'exit_code' : systemd_info . exit , } if self . verbose : info_dict . update ({ 'stdout' : utilities . wrap_text ( systemd_info . out ), 'stderr' : utilities . wrap_text ( systemd_info . err ) }) status = { 'running' : systemd_info . exit == 0 , 'enabled_on_startup' : self . sysctl . is_enabled ( self . systemd_service ) } if self . pid : status . update ({ 'pid' : self . pid }) if self . log_path : status . update ({ 'logs' : self . log_path }) status . update ({ 'info' : info_dict }) if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize status_tbl = [[ 'Service' , self . name , ], [ 'Running' , colorize ( 'yes' , 'green' ) if status [ 'running' ] else colorize ( 'no' , 'red' )], [ 'Enabled on Startup' , colorize ( 'yes' , 'green' ) if status [ 'enabled_on_startup' ] else colorize ( 'no' , 'red' )]] if status . get ( 'pid' ): status_tbl . append ([ 'PID' , status [ 'pid' ] ]) if status . get ( 'logs' ): status_tbl . append ([ 'Logs' , status [ 'logs' ] ]) if status [ 'info' ] . get ( 'command' ): status_tbl . append ([ 'Command' , status [ 'info' ] . get ( 'command' ) ]) if status [ 'info' ] . get ( 'exit_code' ): status_tbl . append ([ 'Exit Code' , status [ 'info' ] . get ( 'exit_code' ) ]) if status [ 'info' ] . get ( 'stdout' ): status_tbl . append ([ 'STDOUT' , status [ 'info' ] . get ( 'stdout' ) ]) if status [ 'info' ] . get ( 'stderr' ): status_tbl . append ([ 'STDERR' , status [ 'info' ] . get ( 'stderr' ) ]) return tabulate . tabulate ( status_tbl , tablefmt = 'fancy_grid' ) return status stop ( self ) Stop process Returns: Type Description bool True, if successfully stopped Source code in dynamite_nsm/services/base/process.py def stop ( self ) -> bool : \"\"\"Stop process Returns: True, if successfully stopped \"\"\" self . logger . info ( 'Attempting to stop {} ' . format ( self . systemd_service )) return self . sysctl . stop ( self . systemd_service )","title":"process"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager","text":"A Systemd wrapper for process management","title":"BaseProcessManager"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.__init__","text":"Manage a service process Parameters: Name Type Description Default systemd_service str The name of the systemd.service file required name str The name of the process manager required log_path Optional[str] The path to where the process logs None create_pid_file Optional[bool] If true will attempt to create a PID file False stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Source code in dynamite_nsm/services/base/process.py def __init__ ( self , systemd_service : str , name : str , log_path : Optional [ str ] = None , create_pid_file : Optional [ bool ] = False , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage a service process Args: systemd_service: The name of the systemd.service file name: The name of the process manager log_path: The path to where the process logs create_pid_file: If true will attempt to create a PID file stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( name , level = log_level , stdout = stdout ) self . pid_file = None self . pid = None self . systemd_service = systemd_service self . name = name self . log_path = log_path if create_pid_file : self . pid_file = f ' { const . PID_PATH } / { name } .pid' self . stdout = stdout self . verbose = verbose self . pretty_print_status = pretty_print_status self . sysctl = systemctl . SystemCtl () if create_pid_file : self . pid = self . _get_pid ( self . pid_file )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.disable","text":"Disable process Returns: Type Description bool True, if successfully disabled Source code in dynamite_nsm/services/base/process.py def disable ( self ) -> bool : \"\"\"Disable process Returns: True, if successfully disabled \"\"\" self . logger . info ( 'Disabling on startup: {} ' . format ( self . systemd_service )) return self . sysctl . disable ( self . systemd_service , daemon_reload = True )","title":"disable()"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.enable","text":"Enabled process Returns: Type Description bool True, if successfully enabled Source code in dynamite_nsm/services/base/process.py def enable ( self ) -> bool : \"\"\"Enabled process Returns: True, if successfully enabled \"\"\" self . logger . info ( 'Enabling on startup: {} ' . format ( self . systemd_service )) return self . sysctl . enable ( self . systemd_service , daemon_reload = True )","title":"enable()"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.restart","text":"Restart Process Returns: Type Description bool True, if the process was restarted Source code in dynamite_nsm/services/base/process.py def restart ( self ) -> bool : \"\"\"Restart Process Returns: True, if the process was restarted \"\"\" self . logger . info ( 'Attempting to restart {} ' . format ( self . systemd_service )) return self . sysctl . restart ( self . systemd_service )","title":"restart()"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.start","text":"Start process Returns: Type Description bool True, if successfully started Source code in dynamite_nsm/services/base/process.py def start ( self ) -> bool : \"\"\"Start process Returns: True, if successfully started \"\"\" self . logger . info ( 'Attempting to start {} ' . format ( self . systemd_service )) return self . sysctl . start ( self . systemd_service )","title":"start()"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.status","text":"Get the status of a process Returns: Type Description Union[Dict, str] A dictionary containing process status or a tabulated string if pretty_print is True. Source code in dynamite_nsm/services/base/process.py def status ( self ) -> Union [ Dict , str ]: \"\"\"Get the status of a process Returns: A dictionary containing process status or a tabulated string if `pretty_print` is True. \"\"\" if self . pid_file : self . pid = self . _get_pid ( self . pid_file ) systemd_info = self . sysctl . status ( self . systemd_service ) info_dict = { 'command' : systemd_info . cmd , 'exit_code' : systemd_info . exit , } if self . verbose : info_dict . update ({ 'stdout' : utilities . wrap_text ( systemd_info . out ), 'stderr' : utilities . wrap_text ( systemd_info . err ) }) status = { 'running' : systemd_info . exit == 0 , 'enabled_on_startup' : self . sysctl . is_enabled ( self . systemd_service ) } if self . pid : status . update ({ 'pid' : self . pid }) if self . log_path : status . update ({ 'logs' : self . log_path }) status . update ({ 'info' : info_dict }) if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize status_tbl = [[ 'Service' , self . name , ], [ 'Running' , colorize ( 'yes' , 'green' ) if status [ 'running' ] else colorize ( 'no' , 'red' )], [ 'Enabled on Startup' , colorize ( 'yes' , 'green' ) if status [ 'enabled_on_startup' ] else colorize ( 'no' , 'red' )]] if status . get ( 'pid' ): status_tbl . append ([ 'PID' , status [ 'pid' ] ]) if status . get ( 'logs' ): status_tbl . append ([ 'Logs' , status [ 'logs' ] ]) if status [ 'info' ] . get ( 'command' ): status_tbl . append ([ 'Command' , status [ 'info' ] . get ( 'command' ) ]) if status [ 'info' ] . get ( 'exit_code' ): status_tbl . append ([ 'Exit Code' , status [ 'info' ] . get ( 'exit_code' ) ]) if status [ 'info' ] . get ( 'stdout' ): status_tbl . append ([ 'STDOUT' , status [ 'info' ] . get ( 'stdout' ) ]) if status [ 'info' ] . get ( 'stderr' ): status_tbl . append ([ 'STDERR' , status [ 'info' ] . get ( 'stderr' ) ]) return tabulate . tabulate ( status_tbl , tablefmt = 'fancy_grid' ) return status","title":"status()"},{"location":"guides/developers/SDK/services/base/process/#dynamite_nsm.services.base.process.BaseProcessManager.stop","text":"Stop process Returns: Type Description bool True, if successfully stopped Source code in dynamite_nsm/services/base/process.py def stop ( self ) -> bool : \"\"\"Stop process Returns: True, if successfully stopped \"\"\" self . logger . info ( 'Attempting to stop {} ' . format ( self . systemd_service )) return self . sysctl . stop ( self . systemd_service )","title":"stop()"},{"location":"guides/developers/SDK/services/base/profile/","text":"Base process profilers To import... from dynamite_nsm.services.base import profile BaseProcessProfiler Process Profiler base class __init__ ( self , install_directory , config_directory , required_install_files = (), required_config_files = ()) special Build a process profiler for a service Parameters: Name Type Description Default install_directory The directory where the service is installed required config_directory The directory holding configuration related files required required_install_files The names of files required to consider the installation successful () required_config_files The names of config files to consider the installation properly configured. () Source code in dynamite_nsm/services/base/profile.py def __init__ ( self , install_directory , config_directory , required_install_files = (), required_config_files = ()): \"\"\"Build a process profiler for a service Args: install_directory: The directory where the service is installed config_directory: The directory holding configuration related files required_install_files: The names of files required to consider the installation successful required_config_files: The names of config files to consider the installation properly configured. \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . install_directory = install_directory self . config_directory = config_directory self . required_install_files = required_install_files self . required_config_files = required_config_files is_configured ( self ) Determine if the instance is properly configured Returns: Type Description bool True if properly configured Source code in dynamite_nsm/services/base/profile.py def is_configured ( self ) -> bool : \"\"\"Determine if the instance is properly configured Returns: True if properly configured \"\"\" if not self . config_directory : return False if not os . path . exists ( self . config_directory ): return False for config_file in self . required_config_files : if config_file not in os . listdir ( self . config_directory ): return False return True is_installed ( self ) Determine if the instance is properly installed Returns: Type Description bool True if properly installed Source code in dynamite_nsm/services/base/profile.py def is_installed ( self ) -> bool : \"\"\"Determine if the instance is properly installed Returns: True if properly installed \"\"\" if not self . install_directory : return False if not os . path . exists ( self . install_directory ): return False for install_file in self . required_install_files : if install_file not in os . listdir ( self . install_directory ): return False return True","title":"profile"},{"location":"guides/developers/SDK/services/base/profile/#dynamite_nsm.services.base.profile.BaseProcessProfiler","text":"Process Profiler base class","title":"BaseProcessProfiler"},{"location":"guides/developers/SDK/services/base/profile/#dynamite_nsm.services.base.profile.BaseProcessProfiler.__init__","text":"Build a process profiler for a service Parameters: Name Type Description Default install_directory The directory where the service is installed required config_directory The directory holding configuration related files required required_install_files The names of files required to consider the installation successful () required_config_files The names of config files to consider the installation properly configured. () Source code in dynamite_nsm/services/base/profile.py def __init__ ( self , install_directory , config_directory , required_install_files = (), required_config_files = ()): \"\"\"Build a process profiler for a service Args: install_directory: The directory where the service is installed config_directory: The directory holding configuration related files required_install_files: The names of files required to consider the installation successful required_config_files: The names of config files to consider the installation properly configured. \"\"\" if not utilities . is_setup (): raise exceptions . DynamiteNotSetupError () self . install_directory = install_directory self . config_directory = config_directory self . required_install_files = required_install_files self . required_config_files = required_config_files","title":"__init__()"},{"location":"guides/developers/SDK/services/base/profile/#dynamite_nsm.services.base.profile.BaseProcessProfiler.is_configured","text":"Determine if the instance is properly configured Returns: Type Description bool True if properly configured Source code in dynamite_nsm/services/base/profile.py def is_configured ( self ) -> bool : \"\"\"Determine if the instance is properly configured Returns: True if properly configured \"\"\" if not self . config_directory : return False if not os . path . exists ( self . config_directory ): return False for config_file in self . required_config_files : if config_file not in os . listdir ( self . config_directory ): return False return True","title":"is_configured()"},{"location":"guides/developers/SDK/services/base/profile/#dynamite_nsm.services.base.profile.BaseProcessProfiler.is_installed","text":"Determine if the instance is properly installed Returns: Type Description bool True if properly installed Source code in dynamite_nsm/services/base/profile.py def is_installed ( self ) -> bool : \"\"\"Determine if the instance is properly installed Returns: True if properly installed \"\"\" if not self . install_directory : return False if not os . path . exists ( self . install_directory ): return False for install_file in self . required_install_files : if install_file not in os . listdir ( self . install_directory ): return False return True","title":"is_installed()"},{"location":"guides/developers/SDK/services/base/config_objects/generic/","text":"Complex configuration objects To import... from dynamite_nsm.services.base.config_objects import generic Analyzer Analyzers are packages used for identifying Zeek scripts and signatures as well as Suricata rule-sets __init__ ( self , name , enabled = False , content = None ) special Create a simple analyzer object Parameters: Name Type Description Default name str The name (or often path) to the analyzer required enabled Optional[bool] True, if enabled False content Optional[str] If included the contents of the analyzer will be used to generate a unique hash. None Source code in dynamite_nsm/services/base/config_objects/generic.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False , content : Optional [ str ] = None ): \"\"\" Create a simple analyzer object Args: name: The name (or often path) to the analyzer enabled: True, if enabled content: If included the contents of the analyzer will be used to generate a unique hash. \"\"\" self . name = name self . enabled = enabled if not content : self . id = sha256 ( str ( name ) . encode ( \"utf-8\" )) . hexdigest ()[ 0 : 7 ] else : self . id = sha256 ( str ( content ) . encode ( \"utf-8\" )) . hexdigest ()[ 0 : 7 ] Analyzers A Group of Analyzers; provides some basic methods for filtering and display get_disabled ( self ) Get all analyzers that are disabled. Returns: Type Description List[dynamite_nsm.services.base.config_objects.generic.Analyzer] A list of disabled Analyzer packages Source code in dynamite_nsm/services/base/config_objects/generic.py def get_disabled ( self ) -> List [ Analyzer ]: \"\"\"Get all analyzers that are disabled. Returns: A list of disabled `Analyzer` packages \"\"\" return [ analyzer for analyzer in self . analyzers if not analyzer . enabled ] get_enabled ( self ) Get all analyzers that are enabled. Returns: Type Description List[dynamite_nsm.services.base.config_objects.generic.Analyzer] A list of enabled Analyzer packages Source code in dynamite_nsm/services/base/config_objects/generic.py def get_enabled ( self ) -> List [ Analyzer ]: \"\"\"Get all analyzers that are enabled. Returns: A list of enabled `Analyzer` packages \"\"\" return [ analyzer for analyzer in self . analyzers if analyzer . enabled ] get_raw ( self ) Get the analyzers in a format that can be directly written to a corresponding configuration Returns: Type Description List[str] A list of analyzer names. Source code in dynamite_nsm/services/base/config_objects/generic.py def get_raw ( self ) -> List [ str ]: \"\"\" Get the analyzers in a format that can be directly written to a corresponding configuration Returns: A list of analyzer names. \"\"\" return [ analyzer . name for analyzer in self . analyzers if analyzer . enabled ] GenericItem Empty Class GenericItemGroup __init__ ( self , identifier_attribute , items = None ) special A base class representing simple groups of configuration options, where each group is unique. Parameters: Name Type Description Default identifier_attribute str The name of an attribute found within the GenericItem list used for identification required items Optional[List[dynamite_nsm.services.base.config_objects.generic.GenericItem]] A list of GenericItems None Source code in dynamite_nsm/services/base/config_objects/generic.py def __init__ ( self , identifier_attribute : str , items : Optional [ List [ GenericItem ]] = None , ): \"\"\" A base class representing simple groups of configuration options, where each group is unique. Args: identifier_attribute: The name of an attribute found within the GenericItem list used for identification items: A list of GenericItems \"\"\" self . identifier_attribute = identifier_attribute self . items = items if items is None : self . items = [] self . _idx = 0","title":"generic.py"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.Analyzer","text":"Analyzers are packages used for identifying Zeek scripts and signatures as well as Suricata rule-sets","title":"Analyzer"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.Analyzer.__init__","text":"Create a simple analyzer object Parameters: Name Type Description Default name str The name (or often path) to the analyzer required enabled Optional[bool] True, if enabled False content Optional[str] If included the contents of the analyzer will be used to generate a unique hash. None Source code in dynamite_nsm/services/base/config_objects/generic.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False , content : Optional [ str ] = None ): \"\"\" Create a simple analyzer object Args: name: The name (or often path) to the analyzer enabled: True, if enabled content: If included the contents of the analyzer will be used to generate a unique hash. \"\"\" self . name = name self . enabled = enabled if not content : self . id = sha256 ( str ( name ) . encode ( \"utf-8\" )) . hexdigest ()[ 0 : 7 ] else : self . id = sha256 ( str ( content ) . encode ( \"utf-8\" )) . hexdigest ()[ 0 : 7 ]","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.Analyzers","text":"A Group of Analyzers; provides some basic methods for filtering and display","title":"Analyzers"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.Analyzers.get_disabled","text":"Get all analyzers that are disabled. Returns: Type Description List[dynamite_nsm.services.base.config_objects.generic.Analyzer] A list of disabled Analyzer packages Source code in dynamite_nsm/services/base/config_objects/generic.py def get_disabled ( self ) -> List [ Analyzer ]: \"\"\"Get all analyzers that are disabled. Returns: A list of disabled `Analyzer` packages \"\"\" return [ analyzer for analyzer in self . analyzers if not analyzer . enabled ]","title":"get_disabled()"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.Analyzers.get_enabled","text":"Get all analyzers that are enabled. Returns: Type Description List[dynamite_nsm.services.base.config_objects.generic.Analyzer] A list of enabled Analyzer packages Source code in dynamite_nsm/services/base/config_objects/generic.py def get_enabled ( self ) -> List [ Analyzer ]: \"\"\"Get all analyzers that are enabled. Returns: A list of enabled `Analyzer` packages \"\"\" return [ analyzer for analyzer in self . analyzers if analyzer . enabled ]","title":"get_enabled()"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.Analyzers.get_raw","text":"Get the analyzers in a format that can be directly written to a corresponding configuration Returns: Type Description List[str] A list of analyzer names. Source code in dynamite_nsm/services/base/config_objects/generic.py def get_raw ( self ) -> List [ str ]: \"\"\" Get the analyzers in a format that can be directly written to a corresponding configuration Returns: A list of analyzer names. \"\"\" return [ analyzer . name for analyzer in self . analyzers if analyzer . enabled ]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.GenericItem","text":"Empty Class","title":"GenericItem"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.GenericItemGroup","text":"","title":"GenericItemGroup"},{"location":"guides/developers/SDK/services/base/config_objects/generic/#dynamite_nsm.services.base.config_objects.generic.GenericItemGroup.__init__","text":"A base class representing simple groups of configuration options, where each group is unique. Parameters: Name Type Description Default identifier_attribute str The name of an attribute found within the GenericItem list used for identification required items Optional[List[dynamite_nsm.services.base.config_objects.generic.GenericItem]] A list of GenericItems None Source code in dynamite_nsm/services/base/config_objects/generic.py def __init__ ( self , identifier_attribute : str , items : Optional [ List [ GenericItem ]] = None , ): \"\"\" A base class representing simple groups of configuration options, where each group is unique. Args: identifier_attribute: The name of an attribute found within the GenericItem list used for identification items: A list of GenericItems \"\"\" self . identifier_attribute = identifier_attribute self . items = items if items is None : self . items = [] self . _idx = 0","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/","text":"Miscellaneous configuration objects for Filebeat To import... from dynamite_nsm.services.base.config_objects.filebeat import misc as filebeat_config_misc FieldProcessors __init__ ( self , originating_agent_tag ) special Add/remove/manipulate fields parsed by Filebeat Parameters: Name Type Description Default originating_agent_tag str The name for the Dynamite Agent which will be added to all events sent downstream required Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , originating_agent_tag : str ): \"\"\"Add/remove/manipulate fields parsed by Filebeat Args: originating_agent_tag: The name for the Dynamite Agent which will be **added** to all events sent downstream \"\"\" self . originating_agent_tag = originating_agent_tag get_raw ( self ) Get the raw representation of this config object. Returns: Type Description List[Dict] A dictionary of Filebeat field processors Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> List [ Dict ]: \"\"\"Get the raw representation of this config object. Returns: A dictionary of Filebeat field processors \"\"\" return [ dict ( add_fields = dict ( fields = dict ( originating_agent_tag = self . originating_agent_tag ) ) )] validate_agent_tag ( agent_tag ) staticmethod Validate that the agent tag given is valid Parameters: Name Type Description Default agent_tag str The name of the agent required Returns: Type Description bool True, if valid Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py @staticmethod def validate_agent_tag ( agent_tag : str ) -> bool : \"\"\"Validate that the agent tag given is valid Args: agent_tag: The name of the agent Returns: True, if valid \"\"\" import re agent_tag = str ( agent_tag ) tag_length_ok = 30 > len ( agent_tag ) > 5 tag_match_pattern = bool ( re . findall ( r \"^[a-zA-Z0-9_]*$\" , agent_tag )) return tag_length_ok and tag_match_pattern IndexTemplateSettings __init__ ( self , index_name , index_pattern = None , enabled = True , overwrite = True ) special Settings for index name and pattern for downstream Elasticsearch Parameters: Name Type Description Default index_name str The name of the index where to send logs (E.G dynamite-events-%{+yyyy.MM.dd}) required index_pattern Optional[str] The corresponding index pattern (E.G dynamite-events-*) None Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , index_name : str , index_pattern : Optional [ str ] = None , enabled : Optional [ bool ] = True , overwrite : Optional [ bool ] = True ): \"\"\"Settings for index name and pattern for downstream Elasticsearch Args: index_name: The name of the index where to send logs (E.G dynamite-events-%{+yyyy.MM.dd}) index_pattern: The corresponding index pattern (E.G dynamite-events-*) \"\"\" self . enabled = enabled self . overwrite = overwrite self . index_name = index_name if index_pattern : self . index_pattern = index_pattern else : if index_name : self . index_pattern = f ' { index_name } -*' else : self . index_pattern = f 'filebeat-*' get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A dictionary of index template settings Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A dictionary of index template settings \"\"\" return dict ( enabled = self . enabled , overwrite = self . overwrite , name = self . index_name , pattern = self . index_pattern ) InputLogs __init__ ( self , monitor_log_paths ) special A set of logs to monitor on the filesystem Parameters: Name Type Description Default monitor_log_paths List[str] A list of logs to monitor required Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , monitor_log_paths : List [ str ]): \"\"\"A set of logs to monitor on the filesystem Args: monitor_log_paths: A list of logs to monitor \"\"\" self . enabled = False self . monitor_log_paths = monitor_log_paths get_raw ( self ) Get the raw representation of this config object. Returns: Type Description List A list of input log paths Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> List : \"\"\"Get the raw representation of this config object. Returns: A list of input log paths \"\"\" return [ dict ( enabled = self . enabled , paths = self . monitor_log_paths , type = 'log' )] KibanaSettings __init__ ( self , kibana_target_str , kibana_protocol , enabled = False ) special Settings for configuring an upstream Kibana instance Parameters: Name Type Description Default kibana_target_str str The URL to the Kibana instance w/o the protocol prefix (E.G 192.168.0.5:5601) required kibana_protocol str http or https required Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , kibana_target_str : str , kibana_protocol : str , enabled : Optional [ bool ] = False ): \"\"\"Settings for configuring an upstream Kibana instance Args: kibana_target_str: The URL to the Kibana instance w/o the protocol prefix (E.G 192.168.0.5:5601) kibana_protocol: http or https \"\"\" self . enabled = enabled self . kibana_target_str = kibana_target_str self . kibana_protocol = kibana_protocol get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A dictionary of Kibana endpoint settings Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A dictionary of Kibana endpoint settings \"\"\" return dict ( enabled = self . enabled , host = self . kibana_target_str , protocol = self . kibana_protocol )","title":"misc.py"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.FieldProcessors","text":"","title":"FieldProcessors"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.FieldProcessors.__init__","text":"Add/remove/manipulate fields parsed by Filebeat Parameters: Name Type Description Default originating_agent_tag str The name for the Dynamite Agent which will be added to all events sent downstream required Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , originating_agent_tag : str ): \"\"\"Add/remove/manipulate fields parsed by Filebeat Args: originating_agent_tag: The name for the Dynamite Agent which will be **added** to all events sent downstream \"\"\" self . originating_agent_tag = originating_agent_tag","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.FieldProcessors.get_raw","text":"Get the raw representation of this config object. Returns: Type Description List[Dict] A dictionary of Filebeat field processors Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> List [ Dict ]: \"\"\"Get the raw representation of this config object. Returns: A dictionary of Filebeat field processors \"\"\" return [ dict ( add_fields = dict ( fields = dict ( originating_agent_tag = self . originating_agent_tag ) ) )]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.FieldProcessors.validate_agent_tag","text":"Validate that the agent tag given is valid Parameters: Name Type Description Default agent_tag str The name of the agent required Returns: Type Description bool True, if valid Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py @staticmethod def validate_agent_tag ( agent_tag : str ) -> bool : \"\"\"Validate that the agent tag given is valid Args: agent_tag: The name of the agent Returns: True, if valid \"\"\" import re agent_tag = str ( agent_tag ) tag_length_ok = 30 > len ( agent_tag ) > 5 tag_match_pattern = bool ( re . findall ( r \"^[a-zA-Z0-9_]*$\" , agent_tag )) return tag_length_ok and tag_match_pattern","title":"validate_agent_tag()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.IndexTemplateSettings","text":"","title":"IndexTemplateSettings"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.IndexTemplateSettings.__init__","text":"Settings for index name and pattern for downstream Elasticsearch Parameters: Name Type Description Default index_name str The name of the index where to send logs (E.G dynamite-events-%{+yyyy.MM.dd}) required index_pattern Optional[str] The corresponding index pattern (E.G dynamite-events-*) None Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , index_name : str , index_pattern : Optional [ str ] = None , enabled : Optional [ bool ] = True , overwrite : Optional [ bool ] = True ): \"\"\"Settings for index name and pattern for downstream Elasticsearch Args: index_name: The name of the index where to send logs (E.G dynamite-events-%{+yyyy.MM.dd}) index_pattern: The corresponding index pattern (E.G dynamite-events-*) \"\"\" self . enabled = enabled self . overwrite = overwrite self . index_name = index_name if index_pattern : self . index_pattern = index_pattern else : if index_name : self . index_pattern = f ' { index_name } -*' else : self . index_pattern = f 'filebeat-*'","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.IndexTemplateSettings.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A dictionary of index template settings Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A dictionary of index template settings \"\"\" return dict ( enabled = self . enabled , overwrite = self . overwrite , name = self . index_name , pattern = self . index_pattern )","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.InputLogs","text":"","title":"InputLogs"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.InputLogs.__init__","text":"A set of logs to monitor on the filesystem Parameters: Name Type Description Default monitor_log_paths List[str] A list of logs to monitor required Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , monitor_log_paths : List [ str ]): \"\"\"A set of logs to monitor on the filesystem Args: monitor_log_paths: A list of logs to monitor \"\"\" self . enabled = False self . monitor_log_paths = monitor_log_paths","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.InputLogs.get_raw","text":"Get the raw representation of this config object. Returns: Type Description List A list of input log paths Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> List : \"\"\"Get the raw representation of this config object. Returns: A list of input log paths \"\"\" return [ dict ( enabled = self . enabled , paths = self . monitor_log_paths , type = 'log' )]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.KibanaSettings","text":"","title":"KibanaSettings"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.KibanaSettings.__init__","text":"Settings for configuring an upstream Kibana instance Parameters: Name Type Description Default kibana_target_str str The URL to the Kibana instance w/o the protocol prefix (E.G 192.168.0.5:5601) required kibana_protocol str http or https required Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def __init__ ( self , kibana_target_str : str , kibana_protocol : str , enabled : Optional [ bool ] = False ): \"\"\"Settings for configuring an upstream Kibana instance Args: kibana_target_str: The URL to the Kibana instance w/o the protocol prefix (E.G 192.168.0.5:5601) kibana_protocol: http or https \"\"\" self . enabled = enabled self . kibana_target_str = kibana_target_str self . kibana_protocol = kibana_protocol","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/misc/#dynamite_nsm.services.base.config_objects.filebeat.misc.KibanaSettings.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A dictionary of Kibana endpoint settings Source code in dynamite_nsm/services/base/config_objects/filebeat/misc.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A dictionary of Kibana endpoint settings \"\"\" return dict ( enabled = self . enabled , host = self . kibana_target_str , protocol = self . kibana_protocol )","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/","text":"Downstream targets configuration objects for Filebeat To import... from dynamite_nsm.services.base.config_objects.filebeat import targets as filebeat_config_targets BaseTargets __init__ ( self , target_strings , ssl_certificate_authorities = None , ssl_certificate = None , ssl_key = None , ssl_verification_mode = 'certificate' , enabled = False , ssl_enabled = False ) special An abstract object from which all Filebeat targets are derived Parameters: Name Type Description Default target_strings List[str] The list of downstream servers to connect to required ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. This option is required if ssl_certificate is specified. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. This option is required if ssl_certificate is specified. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"An abstract object from which all Filebeat targets are derived Args: target_strings: The list of downstream servers to connect to ssl_certificate_authorities: The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) ssl_certificate: The path to the certificate for SSL client authentication. If the certificate is not specified, client authentication is not available. The connection might fail if the server requests client authentication. ssl_key: The client certificate key used for client authentication. This option is required if ssl_certificate is specified. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) ssl_certificate: The path to the certificate for SSL client authentication. If the certificate is not specified, client authentication is not available. The connection might fail if the server requests client authentication. ssl_key: The client certificate key used for client authentication. This option is required if ssl_certificate is specified. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" self . target_strings = target_strings self . ssl_certificate_authorities = ssl_certificate_authorities if not None else [] self . ssl_certificate = ssl_certificate self . ssl_key = ssl_key self . ssl_verification_mode = ssl_verification_mode self . enabled = enabled self . ssl_enabled = ssl_enabled get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a downstream connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a downstream connector where to send logs \"\"\" ssl = dict ( certificate_authorities = self . ssl_certificate_authorities , certificate = self . ssl_certificate , key = self . ssl_key , verification_mode = self . ssl_verification_mode ) ssl = { k : v for k , v in ssl . items () if v is not None } raw = dict ( hosts = self . target_strings , enabled = self . enabled , ) if self . ssl_enabled : raw . update ( ssl = ssl ) return raw ElasticsearchTargets __init__ ( self , target_strings , index = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , username = None , password = None , ssl_certificate_authorities = None , ssl_certificate = None , ssl_key = None , ssl_verification_mode = 'certificate' , enabled = False , ssl_enabled = False ) special Elasticsearch endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] The list of Elasticsearch nodes to connect to. required index Optional[str] The index name to write events to. 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' username Optional[str] The basic authentication username for connecting to Elasticsearch. None password Optional[str] The basic authentication password for connecting to Elasticsearch. None enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], index : Optional [ str ] = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , username : Optional [ str ] = None , password : Optional [ str ] = None , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Elasticsearch endpoint configuration where events should be sent Args: target_strings: The list of Elasticsearch nodes to connect to. index: The index name to write events to. username: The basic authentication username for connecting to Elasticsearch. password: The basic authentication password for connecting to Elasticsearch. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled = enabled , ssl_enabled = ssl_enabled ) self . index = index self . username = username self . password = password get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a elasticsearch connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a elasticsearch connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( index = self . index , username = self . username , password = self . password ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw InvalidTargetString __init__ ( self , target_string ) special Invalid Filebeat Target Parameters: Name Type Description Default target_string The full target string required Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_string ): \"\"\"Invalid Filebeat Target Args: target_string: The full target string \"\"\" msg = f 'Filebeat Target is invalid expected: (http|https)//(url|ip):port) got: { target_string } ' super ( InvalidTargetString , self ) . __init__ ( msg ) KafkaTargets __init__ ( self , target_strings , topic = None , username = None , password = None , ssl_certificate_authorities = None , ssl_certificate = None , ssl_key = None , ssl_verification_mode = None , enabled = False , ssl_enabled = False ) special Kafka endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] A list of Kafka brokers, and their service port (E.G [\"192.168.0.9 5044\"]) required topic Optional[str] A Kafka topic None username Optional[str] The username used to authenticate to Kafka broker None password Optional[str] The password used to authenticate to Kafka broker None enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. None Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], topic : Optional [ str ] = None , username : Optional [ str ] = None , password : Optional [ str ] = None , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = None , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Kafka endpoint configuration where events should be sent Args: target_strings: A list of Kafka brokers, and their service port (E.G [\"192.168.0.9 5044\"]) topic: A Kafka topic username: The username used to authenticate to Kafka broker password: The password used to authenticate to Kafka broker enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled , ssl_enabled = ssl_enabled ) self . topic = topic self . username = username self . password = password get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a kafka connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a kafka connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( topic = self . topic , username = self . username , password = self . password ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw LogstashTargets __init__ ( self , target_strings , index = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , load_balance = True , socks_5_proxy_url = None , pipelines = 2 , max_batch_size = 2048 , ssl_certificate_authorities = None , ssl_certificate = None , ssl_key = None , ssl_verification_mode = 'certificate' , enabled = False , ssl_enabled = False ) special Logstash endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] A list of Logstash hosts, and their service port (E.G [\"192.168.0.9 5044\"]) required load_balance Optional[bool] If included and multiple Logstash hosts are configured load-balance between them True index Optional[str] The name of the index to include in the @metadata.beat field 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' socks_5_proxy_url Optional[str] The full url to the SOCKS5 proxy used for encapsulating the beat protocol None pipelines Optional[int] Configures the number of batches to be sent asynchronously to Logstash 2 max_batch_size Optional[int] The maximum number of events to bulk in a single Logstash request. 2048 enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], index : Optional [ str ] = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , load_balance : Optional [ bool ] = True , socks_5_proxy_url : Optional [ str ] = None , pipelines : Optional [ int ] = 2 , max_batch_size : Optional [ int ] = 2048 , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Logstash endpoint configuration where events should be sent Args: target_strings: A list of Logstash hosts, and their service port (E.G [\"192.168.0.9 5044\"]) load_balance: If included and multiple Logstash hosts are configured load-balance between them index: The name of the index to include in the @metadata.beat field socks_5_proxy_url: The full url to the SOCKS5 proxy used for encapsulating the beat protocol pipelines: Configures the number of batches to be sent asynchronously to Logstash max_batch_size: The maximum number of events to bulk in a single Logstash request. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" self . index = index self . load_balance = load_balance self . socks_5_proxy_url = socks_5_proxy_url self . pipelines = pipelines self . max_batch_size = max_batch_size super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled , ssl_enabled = ssl_enabled ) get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a logstash connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a logstash connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( index = self . index , loadbalance = self . load_balance , proxy_url = self . socks_5_proxy_url , pipelining = self . pipelines , bulk_max_size = self . max_batch_size ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw RedisTargets __init__ ( self , target_strings , index = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , load_balance = True , socks_5_proxy_url = None , workers = 1 , max_batch_size = 2048 , db = 0 , password = None , ssl_certificate_authorities = None , ssl_certificate = None , ssl_key = None , ssl_verification_mode = 'certificate' , enabled = False , ssl_enabled = False ) special Redis endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] A list of Redis hosts, and their service port (E.G [\"192.168.0.9 6379\"] required index Optional[str] The key format string to use. 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' load_balance Optional[bool] If included and multiple Redis hosts are configured load-balance between them True socks_5_proxy_url Optional[str] The full url to the SOCKS5 proxy used for encapsulating the beat protocol None workers Optional[int] The number of workers to use for each host configured to publish events to Redis. 1 max_batch_size Optional[int] The maximum number of events to bulk in a single Redis request or pipeline. 2048 password Optional[str] The password to authenticate with. The default is no authentication. None db Optional[int] The Redis database number where the events are published. The default is 0. 0 enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], index : Optional [ str ] = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , load_balance : Optional [ bool ] = True , socks_5_proxy_url : Optional [ str ] = None , workers : Optional [ int ] = 1 , max_batch_size : Optional [ int ] = 2048 , db : Optional [ int ] = 0 , password : Optional [ str ] = None , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Redis endpoint configuration where events should be sent Args: target_strings: A list of Redis hosts, and their service port (E.G [\"192.168.0.9 6379\"] index: The key format string to use. load_balance: If included and multiple Redis hosts are configured load-balance between them socks_5_proxy_url: The full url to the SOCKS5 proxy used for encapsulating the beat protocol workers: The number of workers to use for each host configured to publish events to Redis. max_batch_size: The maximum number of events to bulk in a single Redis request or pipeline. password: The password to authenticate with. The default is no authentication. db: The Redis database number where the events are published. The default is 0. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled , ssl_enabled = ssl_enabled ) self . index = index self . socks_5_proxy_url = socks_5_proxy_url self . workers = workers self . max_batch_size = max_batch_size self . db = db self . load_balance = load_balance self . password = password get_raw ( self ) Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a redis connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a redis connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( index = self . index , proxy_url = self . socks_5_proxy_url , loadbalance = self . load_balance , worker = self . workers , bulk_max_size = self . max_batch_size , db = self . db , password = self . password ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw","title":"targets.py"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.BaseTargets","text":"","title":"BaseTargets"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.BaseTargets.__init__","text":"An abstract object from which all Filebeat targets are derived Parameters: Name Type Description Default target_strings List[str] The list of downstream servers to connect to required ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. This option is required if ssl_certificate is specified. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. This option is required if ssl_certificate is specified. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"An abstract object from which all Filebeat targets are derived Args: target_strings: The list of downstream servers to connect to ssl_certificate_authorities: The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) ssl_certificate: The path to the certificate for SSL client authentication. If the certificate is not specified, client authentication is not available. The connection might fail if the server requests client authentication. ssl_key: The client certificate key used for client authentication. This option is required if ssl_certificate is specified. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. If certificate_authorities is empty or not set, the trusted certificate authorities of the host system are used. (E.G [\"/etc/pki/root/ca.pem\"]) ssl_certificate: The path to the certificate for SSL client authentication. If the certificate is not specified, client authentication is not available. The connection might fail if the server requests client authentication. ssl_key: The client certificate key used for client authentication. This option is required if ssl_certificate is specified. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" self . target_strings = target_strings self . ssl_certificate_authorities = ssl_certificate_authorities if not None else [] self . ssl_certificate = ssl_certificate self . ssl_key = ssl_key self . ssl_verification_mode = ssl_verification_mode self . enabled = enabled self . ssl_enabled = ssl_enabled","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.BaseTargets.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a downstream connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a downstream connector where to send logs \"\"\" ssl = dict ( certificate_authorities = self . ssl_certificate_authorities , certificate = self . ssl_certificate , key = self . ssl_key , verification_mode = self . ssl_verification_mode ) ssl = { k : v for k , v in ssl . items () if v is not None } raw = dict ( hosts = self . target_strings , enabled = self . enabled , ) if self . ssl_enabled : raw . update ( ssl = ssl ) return raw","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.ElasticsearchTargets","text":"","title":"ElasticsearchTargets"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.ElasticsearchTargets.__init__","text":"Elasticsearch endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] The list of Elasticsearch nodes to connect to. required index Optional[str] The index name to write events to. 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' username Optional[str] The basic authentication username for connecting to Elasticsearch. None password Optional[str] The basic authentication password for connecting to Elasticsearch. None enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], index : Optional [ str ] = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , username : Optional [ str ] = None , password : Optional [ str ] = None , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Elasticsearch endpoint configuration where events should be sent Args: target_strings: The list of Elasticsearch nodes to connect to. index: The index name to write events to. username: The basic authentication username for connecting to Elasticsearch. password: The basic authentication password for connecting to Elasticsearch. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled = enabled , ssl_enabled = ssl_enabled ) self . index = index self . username = username self . password = password","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.ElasticsearchTargets.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a elasticsearch connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a elasticsearch connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( index = self . index , username = self . username , password = self . password ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.InvalidTargetString","text":"","title":"InvalidTargetString"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.InvalidTargetString.__init__","text":"Invalid Filebeat Target Parameters: Name Type Description Default target_string The full target string required Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_string ): \"\"\"Invalid Filebeat Target Args: target_string: The full target string \"\"\" msg = f 'Filebeat Target is invalid expected: (http|https)//(url|ip):port) got: { target_string } ' super ( InvalidTargetString , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.KafkaTargets","text":"","title":"KafkaTargets"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.KafkaTargets.__init__","text":"Kafka endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] A list of Kafka brokers, and their service port (E.G [\"192.168.0.9 5044\"]) required topic Optional[str] A Kafka topic None username Optional[str] The username used to authenticate to Kafka broker None password Optional[str] The password used to authenticate to Kafka broker None enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. None Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], topic : Optional [ str ] = None , username : Optional [ str ] = None , password : Optional [ str ] = None , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = None , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Kafka endpoint configuration where events should be sent Args: target_strings: A list of Kafka brokers, and their service port (E.G [\"192.168.0.9 5044\"]) topic: A Kafka topic username: The username used to authenticate to Kafka broker password: The password used to authenticate to Kafka broker enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled , ssl_enabled = ssl_enabled ) self . topic = topic self . username = username self . password = password","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.KafkaTargets.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a kafka connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a kafka connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( topic = self . topic , username = self . username , password = self . password ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.LogstashTargets","text":"","title":"LogstashTargets"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.LogstashTargets.__init__","text":"Logstash endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] A list of Logstash hosts, and their service port (E.G [\"192.168.0.9 5044\"]) required load_balance Optional[bool] If included and multiple Logstash hosts are configured load-balance between them True index Optional[str] The name of the index to include in the @metadata.beat field 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' socks_5_proxy_url Optional[str] The full url to the SOCKS5 proxy used for encapsulating the beat protocol None pipelines Optional[int] Configures the number of batches to be sent asynchronously to Logstash 2 max_batch_size Optional[int] The maximum number of events to bulk in a single Logstash request. 2048 enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], index : Optional [ str ] = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , load_balance : Optional [ bool ] = True , socks_5_proxy_url : Optional [ str ] = None , pipelines : Optional [ int ] = 2 , max_batch_size : Optional [ int ] = 2048 , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Logstash endpoint configuration where events should be sent Args: target_strings: A list of Logstash hosts, and their service port (E.G [\"192.168.0.9 5044\"]) load_balance: If included and multiple Logstash hosts are configured load-balance between them index: The name of the index to include in the @metadata.beat field socks_5_proxy_url: The full url to the SOCKS5 proxy used for encapsulating the beat protocol pipelines: Configures the number of batches to be sent asynchronously to Logstash max_batch_size: The maximum number of events to bulk in a single Logstash request. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" self . index = index self . load_balance = load_balance self . socks_5_proxy_url = socks_5_proxy_url self . pipelines = pipelines self . max_batch_size = max_batch_size super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled , ssl_enabled = ssl_enabled )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.LogstashTargets.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a logstash connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a logstash connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( index = self . index , loadbalance = self . load_balance , proxy_url = self . socks_5_proxy_url , pipelining = self . pipelines , bulk_max_size = self . max_batch_size ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.RedisTargets","text":"","title":"RedisTargets"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.RedisTargets.__init__","text":"Redis endpoint configuration where events should be sent Parameters: Name Type Description Default target_strings List[str] A list of Redis hosts, and their service port (E.G [\"192.168.0.9 6379\"] required index Optional[str] The key format string to use. 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' load_balance Optional[bool] If included and multiple Redis hosts are configured load-balance between them True socks_5_proxy_url Optional[str] The full url to the SOCKS5 proxy used for encapsulating the beat protocol None workers Optional[int] The number of workers to use for each host configured to publish events to Redis. 1 max_batch_size Optional[int] The maximum number of events to bulk in a single Redis request or pipeline. 2048 password Optional[str] The password to authenticate with. The default is no authentication. None db Optional[int] The Redis database number where the events are published. The default is 0. 0 enabled Optional[bool] If True, Filebeat will attempt to send events to this target False ssl_enabled Optional[bool] If True, The SSL transport settings will be used False ssl_certificate_authorities Optional[str] The list of root certificates for server verifications. None ssl_certificate Optional[str] The path to the certificate for SSL client authentication. None ssl_key Optional[str] The client certificate key used for client authentication. None ssl_verification_mode Optional[str] This option controls whether the client verifies server certificates and host names. 'certificate' Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def __init__ ( self , target_strings : List [ str ], index : Optional [ str ] = 'filebeat-%{[agent.version]}-%{+yyyy.MM.dd}' , load_balance : Optional [ bool ] = True , socks_5_proxy_url : Optional [ str ] = None , workers : Optional [ int ] = 1 , max_batch_size : Optional [ int ] = 2048 , db : Optional [ int ] = 0 , password : Optional [ str ] = None , ssl_certificate_authorities : Optional [ str ] = None , ssl_certificate : Optional [ str ] = None , ssl_key : Optional [ str ] = None , ssl_verification_mode : Optional [ str ] = 'certificate' , enabled : Optional [ bool ] = False , ssl_enabled : Optional [ bool ] = False ): \"\"\"Redis endpoint configuration where events should be sent Args: target_strings: A list of Redis hosts, and their service port (E.G [\"192.168.0.9 6379\"] index: The key format string to use. load_balance: If included and multiple Redis hosts are configured load-balance between them socks_5_proxy_url: The full url to the SOCKS5 proxy used for encapsulating the beat protocol workers: The number of workers to use for each host configured to publish events to Redis. max_batch_size: The maximum number of events to bulk in a single Redis request or pipeline. password: The password to authenticate with. The default is no authentication. db: The Redis database number where the events are published. The default is 0. enabled: If True, Filebeat will attempt to send events to this target ssl_enabled: If True, The SSL transport settings will be used ssl_certificate_authorities: The list of root certificates for server verifications. ssl_certificate: The path to the certificate for SSL client authentication. ssl_key: The client certificate key used for client authentication. ssl_verification_mode: This option controls whether the client verifies server certificates and host names. \"\"\" super () . __init__ ( target_strings , ssl_certificate_authorities , ssl_certificate , ssl_key , ssl_verification_mode , enabled , ssl_enabled = ssl_enabled ) self . index = index self . socks_5_proxy_url = socks_5_proxy_url self . workers = workers self . max_batch_size = max_batch_size self . db = db self . load_balance = load_balance self . password = password","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/filebeat/targets/#dynamite_nsm.services.base.config_objects.filebeat.targets.RedisTargets.get_raw","text":"Get the raw representation of this config object. Returns: Type Description Dict A configuration dictionary representing a redis connector where to send logs Source code in dynamite_nsm/services/base/config_objects/filebeat/targets.py def get_raw ( self ) -> Dict : \"\"\"Get the raw representation of this config object. Returns: A configuration dictionary representing a redis connector where to send logs \"\"\" orig_raw = super () . get_raw () orig_raw . update ( dict ( index = self . index , proxy_url = self . socks_5_proxy_url , loadbalance = self . load_balance , worker = self . workers , bulk_max_size = self . max_batch_size , db = self . db , password = self . password ) ) orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/","text":"Miscellaneous configuration objects for Suricata To import... from dynamite_nsm.services.base.config_objects.suricata import misc as suricata_config_misc AfPacketInterface __init__ ( self , interface_name , cluster_id = None , cluster_type = None , bpf_filter = None , threads = None ) special Suricata AF_PACKET interface Parameters: Name Type Description Default interface_name str The name of a network interface to monitor required cluster_id Optional[int] A unique integer associated with this worker maps to af_packet_fanout_id None cluster_type Optional[str] The algorithm used to spread traffic between sockets. None bpf_filter Optional[str] A filter that can be used to drop packets before they are analyzed None threads Union[int, str] The number of threads dedicated to monitoring this network interface None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , interface_name : str , cluster_id : Optional [ int ] = None , cluster_type : Optional [ str ] = None , bpf_filter : Optional [ str ] = None , threads : Union [ int , str ] = None ): \"\"\"Suricata AF_PACKET interface Args: interface_name: The name of a network interface to monitor cluster_id: A unique integer associated with this worker maps to af_packet_fanout_id cluster_type: The algorithm used to spread traffic between sockets. bpf_filter: A filter that can be used to drop packets before they are analyzed threads: The number of threads dedicated to monitoring this network interface \"\"\" self . interface = interface_name self . cluster_id = cluster_id if cluster_type : self . cluster_type = cluster_type . replace ( 'AF_Packet::' , '' ) if self . cluster_type in AF_PACKET_FANOUT_MODE_TO_CLUSTER_TYPE_MAP . keys (): self . cluster_type = AF_PACKET_FANOUT_MODE_TO_CLUSTER_TYPE_MAP . get ( self . cluster_type ) else : self . cluster_type = 'cluster_qm' self . bpf_filter = bpf_filter self . threads = threads if not threads : self . threads = 'auto' get_raw ( self ) Get a raw representation of this AfPacketInterface. Returns: Type Description Dict A dictionary that can be serialized to YAML then inserted into the suricata.yaml file. Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get_raw ( self ) -> Dict : \"\"\"Get a raw representation of this AfPacketInterface. Returns: A dictionary that can be serialized to YAML then inserted into the `suricata.yaml` file. \"\"\" orig_raw = { 'interface' : self . interface , 'cluster-id' : self . cluster_id , 'cluster-type' : self . cluster_type , 'bpf-filter' : self . bpf_filter , 'threads' : self . threads } orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw AfPacketInterfaces __init__ ( self , interfaces = None ) special A collection of AfPacketInterfaces. Parameters: Name Type Description Default interfaces Optional[List[dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface]] A list of AfPacketInterface objects None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , interfaces : Optional [ List [ AfPacketInterface ]] = None ): \"\"\"A collection of AfPacketInterfaces. Args: interfaces: A list of AfPacketInterface objects \"\"\" self . _idx = 0 self . interfaces = interfaces if not self . interfaces : self . interfaces = [] add ( self , interface ) Add a new AfPacketInterface Parameters: Name Type Description Default interface AfPacketInterface An AfPacketInterface object required Returns: Type Description None None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def add ( self , interface : AfPacketInterface ) -> None : \"\"\"Add a new AfPacketInterface Args: interface: An AfPacketInterface object Returns: None \"\"\" self . interfaces . append ( interface ) get ( self , interface_name ) Given the name of an interface retrieve the corresponding AfPacketInterface object Parameters: Name Type Description Default interface_name str The name of the network interface. required Returns: Type Description Optional[dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface] An AfPacketInterface if found, otherwise None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get ( self , interface_name : str ) -> Optional [ AfPacketInterface ]: \"\"\"Given the name of an interface retrieve the corresponding AfPacketInterface object Args: interface_name: The name of the network interface. Returns: An AfPacketInterface if found, otherwise `None` \"\"\" for interface in self . interfaces : if interface . interface == interface_name : return interface return None get_raw ( self ) Get a raw representation of AfPacketInterfaces that can be serialized and inserted into suricata.yaml file Returns: Type Description List[Dict] A list of dictionaries representing individual AfPacketInterface configurations Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get_raw ( self ) -> List [ Dict ]: \"\"\"Get a raw representation of AfPacketInterfaces that can be serialized and inserted into `suricata.yaml` file Returns: A list of dictionaries representing individual AfPacketInterface configurations \"\"\" return [ interface . get_raw () for interface in self . interfaces ] remove ( self , interface_name ) Given the name of an interface delete it Parameters: Name Type Description Default interface_name str The name of the network interface. required Returns: Type Description None None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def remove ( self , interface_name : str ) -> None : \"\"\"Given the name of an interface delete it Args: interface_name: The name of the network interface. Returns: None \"\"\" temp_interfaces = [] for interface in self . interfaces : if interface . interface == interface_name : continue temp_interfaces . append ( interface ) self . interfaces = temp_interfaces PcapInterfaces __init__ ( self , interface_names ) special :param interface_names: A list of network interface names Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , interface_names : List [ str ]): \"\"\" :param interface_names: A list of network interface names \"\"\" self . interfaces = interface_names Threading __init__ ( self , management_cpu_set = None , receive_cpu_set = None , worker_cpu_set = None ) special The threading configuration for Suricata Parameters: Name Type Description Default management_cpu_set Optional[Set] A set of integers representing CPU cores dedicated to management tasks None receive_cpu_set Optional[Set] A set of integers representing CPU cores dedicated to packet acquisition None worker_cpu_set Optional[Set] A set of integers representing CPU cores dedicated to analysis None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , management_cpu_set : Optional [ Set ] = None , receive_cpu_set : Optional [ Set ] = None , worker_cpu_set : Optional [ Set ] = None ): \"\"\"The threading configuration for Suricata Args: management_cpu_set: A set of integers representing CPU cores dedicated to management tasks receive_cpu_set: A set of integers representing CPU cores dedicated to packet acquisition worker_cpu_set: A set of integers representing CPU cores dedicated to analysis \"\"\" self . management_cpu_set = management_cpu_set self . receive_cpu_set = receive_cpu_set self . worker_cpu_set = worker_cpu_set get_raw ( self ) Get a raw representation of Threading that can be serialized and inserted into suricata.yaml file Returns: Type Description Dict A dictionary containing the threading families Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get_raw ( self ) -> Dict : \"\"\"Get a raw representation of Threading that can be serialized and inserted into `suricata.yaml` file Returns: A dictionary containing the threading families \"\"\" thread_families = [] if self . management_cpu_set : thread_families . append ( { 'management-cpu-set' : { 'cpu' : list ( self . management_cpu_set ) } } ) if self . receive_cpu_set : thread_families . append ( { 'receive-cpu-set' : { 'cpu' : list ( self . receive_cpu_set ) } } ) if self . worker_cpu_set : thread_families . append ( { 'worker-cpu-set' : { 'cpu' : list ( self . worker_cpu_set ), 'mode' : 'exclusive' , 'threads' : len ( self . worker_cpu_set ) } } ) return { 'set-cpu-affinity' : True , 'cpu-affinity' : thread_families }","title":"misc.py"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface","text":"","title":"AfPacketInterface"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface.__init__","text":"Suricata AF_PACKET interface Parameters: Name Type Description Default interface_name str The name of a network interface to monitor required cluster_id Optional[int] A unique integer associated with this worker maps to af_packet_fanout_id None cluster_type Optional[str] The algorithm used to spread traffic between sockets. None bpf_filter Optional[str] A filter that can be used to drop packets before they are analyzed None threads Union[int, str] The number of threads dedicated to monitoring this network interface None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , interface_name : str , cluster_id : Optional [ int ] = None , cluster_type : Optional [ str ] = None , bpf_filter : Optional [ str ] = None , threads : Union [ int , str ] = None ): \"\"\"Suricata AF_PACKET interface Args: interface_name: The name of a network interface to monitor cluster_id: A unique integer associated with this worker maps to af_packet_fanout_id cluster_type: The algorithm used to spread traffic between sockets. bpf_filter: A filter that can be used to drop packets before they are analyzed threads: The number of threads dedicated to monitoring this network interface \"\"\" self . interface = interface_name self . cluster_id = cluster_id if cluster_type : self . cluster_type = cluster_type . replace ( 'AF_Packet::' , '' ) if self . cluster_type in AF_PACKET_FANOUT_MODE_TO_CLUSTER_TYPE_MAP . keys (): self . cluster_type = AF_PACKET_FANOUT_MODE_TO_CLUSTER_TYPE_MAP . get ( self . cluster_type ) else : self . cluster_type = 'cluster_qm' self . bpf_filter = bpf_filter self . threads = threads if not threads : self . threads = 'auto'","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface.get_raw","text":"Get a raw representation of this AfPacketInterface. Returns: Type Description Dict A dictionary that can be serialized to YAML then inserted into the suricata.yaml file. Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get_raw ( self ) -> Dict : \"\"\"Get a raw representation of this AfPacketInterface. Returns: A dictionary that can be serialized to YAML then inserted into the `suricata.yaml` file. \"\"\" orig_raw = { 'interface' : self . interface , 'cluster-id' : self . cluster_id , 'cluster-type' : self . cluster_type , 'bpf-filter' : self . bpf_filter , 'threads' : self . threads } orig_raw = { k : v for k , v in orig_raw . items () if v is not None and v != '' } return orig_raw","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterfaces","text":"","title":"AfPacketInterfaces"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterfaces.__init__","text":"A collection of AfPacketInterfaces. Parameters: Name Type Description Default interfaces Optional[List[dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface]] A list of AfPacketInterface objects None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , interfaces : Optional [ List [ AfPacketInterface ]] = None ): \"\"\"A collection of AfPacketInterfaces. Args: interfaces: A list of AfPacketInterface objects \"\"\" self . _idx = 0 self . interfaces = interfaces if not self . interfaces : self . interfaces = []","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterfaces.add","text":"Add a new AfPacketInterface Parameters: Name Type Description Default interface AfPacketInterface An AfPacketInterface object required Returns: Type Description None None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def add ( self , interface : AfPacketInterface ) -> None : \"\"\"Add a new AfPacketInterface Args: interface: An AfPacketInterface object Returns: None \"\"\" self . interfaces . append ( interface )","title":"add()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterfaces.get","text":"Given the name of an interface retrieve the corresponding AfPacketInterface object Parameters: Name Type Description Default interface_name str The name of the network interface. required Returns: Type Description Optional[dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterface] An AfPacketInterface if found, otherwise None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get ( self , interface_name : str ) -> Optional [ AfPacketInterface ]: \"\"\"Given the name of an interface retrieve the corresponding AfPacketInterface object Args: interface_name: The name of the network interface. Returns: An AfPacketInterface if found, otherwise `None` \"\"\" for interface in self . interfaces : if interface . interface == interface_name : return interface return None","title":"get()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterfaces.get_raw","text":"Get a raw representation of AfPacketInterfaces that can be serialized and inserted into suricata.yaml file Returns: Type Description List[Dict] A list of dictionaries representing individual AfPacketInterface configurations Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get_raw ( self ) -> List [ Dict ]: \"\"\"Get a raw representation of AfPacketInterfaces that can be serialized and inserted into `suricata.yaml` file Returns: A list of dictionaries representing individual AfPacketInterface configurations \"\"\" return [ interface . get_raw () for interface in self . interfaces ]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.AfPacketInterfaces.remove","text":"Given the name of an interface delete it Parameters: Name Type Description Default interface_name str The name of the network interface. required Returns: Type Description None None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def remove ( self , interface_name : str ) -> None : \"\"\"Given the name of an interface delete it Args: interface_name: The name of the network interface. Returns: None \"\"\" temp_interfaces = [] for interface in self . interfaces : if interface . interface == interface_name : continue temp_interfaces . append ( interface ) self . interfaces = temp_interfaces","title":"remove()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.PcapInterfaces","text":"","title":"PcapInterfaces"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.PcapInterfaces.__init__","text":":param interface_names: A list of network interface names Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , interface_names : List [ str ]): \"\"\" :param interface_names: A list of network interface names \"\"\" self . interfaces = interface_names","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.Threading","text":"","title":"Threading"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.Threading.__init__","text":"The threading configuration for Suricata Parameters: Name Type Description Default management_cpu_set Optional[Set] A set of integers representing CPU cores dedicated to management tasks None receive_cpu_set Optional[Set] A set of integers representing CPU cores dedicated to packet acquisition None worker_cpu_set Optional[Set] A set of integers representing CPU cores dedicated to analysis None Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def __init__ ( self , management_cpu_set : Optional [ Set ] = None , receive_cpu_set : Optional [ Set ] = None , worker_cpu_set : Optional [ Set ] = None ): \"\"\"The threading configuration for Suricata Args: management_cpu_set: A set of integers representing CPU cores dedicated to management tasks receive_cpu_set: A set of integers representing CPU cores dedicated to packet acquisition worker_cpu_set: A set of integers representing CPU cores dedicated to analysis \"\"\" self . management_cpu_set = management_cpu_set self . receive_cpu_set = receive_cpu_set self . worker_cpu_set = worker_cpu_set","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/misc/#dynamite_nsm.services.base.config_objects.suricata.misc.Threading.get_raw","text":"Get a raw representation of Threading that can be serialized and inserted into suricata.yaml file Returns: Type Description Dict A dictionary containing the threading families Source code in dynamite_nsm/services/base/config_objects/suricata/misc.py def get_raw ( self ) -> Dict : \"\"\"Get a raw representation of Threading that can be serialized and inserted into `suricata.yaml` file Returns: A dictionary containing the threading families \"\"\" thread_families = [] if self . management_cpu_set : thread_families . append ( { 'management-cpu-set' : { 'cpu' : list ( self . management_cpu_set ) } } ) if self . receive_cpu_set : thread_families . append ( { 'receive-cpu-set' : { 'cpu' : list ( self . receive_cpu_set ) } } ) if self . worker_cpu_set : thread_families . append ( { 'worker-cpu-set' : { 'cpu' : list ( self . worker_cpu_set ), 'mode' : 'exclusive' , 'threads' : len ( self . worker_cpu_set ) } } ) return { 'set-cpu-affinity' : True , 'cpu-affinity' : thread_families }","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/rules/","text":"Ruleset configuration objects for Suricata To import... from dynamite_nsm.services.base.config_objects.suricata import rules as suricata_config_rules Rule __init__ ( self , name , enabled = False ) special Represents a Suricata ruleset that can be enabled or disabled. Parameters: Name Type Description Default name str The name of the ruleset required enabled Optional[bool] Whether the ruleset is enabled False Source code in dynamite_nsm/services/base/config_objects/suricata/rules.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False ): \"\"\" Represents a Suricata ruleset that can be enabled or disabled. Args: name: The name of the ruleset enabled: Whether the ruleset is enabled \"\"\" self . value = None self . name = name content = self . get_contents () super () . __init__ ( name , enabled , content = content ) get_contents ( self ) Get the content of the Suricata rule file. Returns: Type Description The contents of the Suricata rule Source code in dynamite_nsm/services/base/config_objects/suricata/rules.py def get_contents ( self ): \"\"\"Get the content of the Suricata rule file. Returns: The contents of the Suricata rule \"\"\" env = utilities . get_environment_file_dict () suricata_rules_root = f \" { env . get ( 'SURICATA_CONFIG' , const . CONFIG_PATH ) } /rules\" path_match_1 = f ' { suricata_rules_root } / { self . name } ' if os . path . exists ( path_match_1 ): with open ( path_match_1 ) as content_rule_in : return content_rule_in . read ( 5120 ) return None Rules __init__ ( self , rules = None ) special A collection of Suricata rulesets Parameters: Name Type Description Default rules Optional[List[dynamite_nsm.services.base.config_objects.suricata.rules.Rule]] A list of Rule objects None Source code in dynamite_nsm/services/base/config_objects/suricata/rules.py def __init__ ( self , rules : Optional [ List [ Rule ]] = None ): \"\"\"A collection of Suricata rulesets Args: rules: A list of Rule objects \"\"\" super () . __init__ ( rules ) self . rules = self . analyzers","title":"rules.py"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/rules/#dynamite_nsm.services.base.config_objects.suricata.rules.Rule","text":"","title":"Rule"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/rules/#dynamite_nsm.services.base.config_objects.suricata.rules.Rule.__init__","text":"Represents a Suricata ruleset that can be enabled or disabled. Parameters: Name Type Description Default name str The name of the ruleset required enabled Optional[bool] Whether the ruleset is enabled False Source code in dynamite_nsm/services/base/config_objects/suricata/rules.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False ): \"\"\" Represents a Suricata ruleset that can be enabled or disabled. Args: name: The name of the ruleset enabled: Whether the ruleset is enabled \"\"\" self . value = None self . name = name content = self . get_contents () super () . __init__ ( name , enabled , content = content )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/rules/#dynamite_nsm.services.base.config_objects.suricata.rules.Rule.get_contents","text":"Get the content of the Suricata rule file. Returns: Type Description The contents of the Suricata rule Source code in dynamite_nsm/services/base/config_objects/suricata/rules.py def get_contents ( self ): \"\"\"Get the content of the Suricata rule file. Returns: The contents of the Suricata rule \"\"\" env = utilities . get_environment_file_dict () suricata_rules_root = f \" { env . get ( 'SURICATA_CONFIG' , const . CONFIG_PATH ) } /rules\" path_match_1 = f ' { suricata_rules_root } / { self . name } ' if os . path . exists ( path_match_1 ): with open ( path_match_1 ) as content_rule_in : return content_rule_in . read ( 5120 ) return None","title":"get_contents()"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/rules/#dynamite_nsm.services.base.config_objects.suricata.rules.Rules","text":"","title":"Rules"},{"location":"guides/developers/SDK/services/base/config_objects/suricata/rules/#dynamite_nsm.services.base.config_objects.suricata.rules.Rules.__init__","text":"A collection of Suricata rulesets Parameters: Name Type Description Default rules Optional[List[dynamite_nsm.services.base.config_objects.suricata.rules.Rule]] A list of Rule objects None Source code in dynamite_nsm/services/base/config_objects/suricata/rules.py def __init__ ( self , rules : Optional [ List [ Rule ]] = None ): \"\"\"A collection of Suricata rulesets Args: rules: A list of Rule objects \"\"\" super () . __init__ ( rules ) self . rules = self . analyzers","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/bpf_filter/","text":"Configuration objects built around Dynamite's optional BPF filtering script. To import... from dynamite_nsm.services.base.config_objects.zeek import bpf_filter as zeek_config_bpf_filter BpfFilter __init__ ( self , interface_name , pattern ) special Represents a BPF filter applied to a single network interface. Parameters: Name Type Description Default interface_name str The name of the network interface (E.G eth0, en0, mon0) required pattern str A valid BPF filter (E.G udp dst port not 53) required Source code in dynamite_nsm/services/base/config_objects/zeek/bpf_filter.py def __init__ ( self , interface_name : str , pattern : str ): \"\"\" Represents a BPF filter applied to a single network interface. Args: interface_name: The name of the network interface (E.G eth0, en0, mon0) pattern: A valid BPF filter (E.G udp dst port not 53) \"\"\" self . interface = interface_name self . pattern = pattern get_raw ( self ) Get the representation of the value as it would appear the config. Returns: Type Description str A line containing both the network interface and pattern associated with it. Source code in dynamite_nsm/services/base/config_objects/zeek/bpf_filter.py def get_raw ( self ) -> str : \"\"\"Get the representation of the value as it would appear the config. Returns: A line containing both the network interface and pattern associated with it. \"\"\" return f ' { self . interface } \\t { self . pattern } ' BpfFilters __init__ ( self , bpf_filters = None ) special A collection of BpfFilters Parameters: Name Type Description Default bpf_filters Optional[List[dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilter]] A collection of BpfFilter objects None Source code in dynamite_nsm/services/base/config_objects/zeek/bpf_filter.py def __init__ ( self , bpf_filters : Optional [ List [ BpfFilter ]] = None ): \"\"\"A collection of BpfFilters Args: bpf_filters: A collection of BpfFilter objects \"\"\" super () . __init__ ( 'interface' , bpf_filters ) self . bpf_filters = self . items self . _idx = 0","title":"bpf_filter.py"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/bpf_filter/#dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilter","text":"","title":"BpfFilter"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/bpf_filter/#dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilter.__init__","text":"Represents a BPF filter applied to a single network interface. Parameters: Name Type Description Default interface_name str The name of the network interface (E.G eth0, en0, mon0) required pattern str A valid BPF filter (E.G udp dst port not 53) required Source code in dynamite_nsm/services/base/config_objects/zeek/bpf_filter.py def __init__ ( self , interface_name : str , pattern : str ): \"\"\" Represents a BPF filter applied to a single network interface. Args: interface_name: The name of the network interface (E.G eth0, en0, mon0) pattern: A valid BPF filter (E.G udp dst port not 53) \"\"\" self . interface = interface_name self . pattern = pattern","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/bpf_filter/#dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilter.get_raw","text":"Get the representation of the value as it would appear the config. Returns: Type Description str A line containing both the network interface and pattern associated with it. Source code in dynamite_nsm/services/base/config_objects/zeek/bpf_filter.py def get_raw ( self ) -> str : \"\"\"Get the representation of the value as it would appear the config. Returns: A line containing both the network interface and pattern associated with it. \"\"\" return f ' { self . interface } \\t { self . pattern } '","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/bpf_filter/#dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilters","text":"","title":"BpfFilters"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/bpf_filter/#dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilters.__init__","text":"A collection of BpfFilters Parameters: Name Type Description Default bpf_filters Optional[List[dynamite_nsm.services.base.config_objects.zeek.bpf_filter.BpfFilter]] A collection of BpfFilter objects None Source code in dynamite_nsm/services/base/config_objects/zeek/bpf_filter.py def __init__ ( self , bpf_filters : Optional [ List [ BpfFilter ]] = None ): \"\"\"A collection of BpfFilters Args: bpf_filters: A collection of BpfFilter objects \"\"\" super () . __init__ ( 'interface' , bpf_filters ) self . bpf_filters = self . items self . _idx = 0","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/","text":"Configuration objects built around the networks.cfg To import... from dynamite_nsm.services.base.config_objects.zeek import local_network as zeek_config_local_network LocalNetwork __init__ ( self , ip_and_cidr , description = None ) special A local network denoted by a IP/CIDR Parameters: Name Type Description Default ip_and_cidr str An IP/CIDR string representing a local network. required description Optional[str] A description of that network's purpose None Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def __init__ ( self , ip_and_cidr : str , description : Optional [ str ] = None ): \"\"\"A local network denoted by a IP/CIDR Args: ip_and_cidr: An IP/CIDR string representing a local network. description: A description of that network's purpose \"\"\" self . ip_and_cidr = ip_and_cidr self . description = description get_raw ( self ) Get a raw representation of this LocalNetwork Returns: Type Description str An assignment statement that can be inserted directly into Zeek's networks.cfg Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this LocalNetwork Returns: An assignment statement that can be inserted directly into Zeek's networks.cfg \"\"\" if self . description : return ' {0: <64} {1} \\n ' . format ( self . ip_and_cidr , self . description ) return ' {0: <64} {1} \\n ' . format ( self . ip_and_cidr , 'Undocumented Network' ) LocalNetworks __init__ ( self , local_networks = None ) special A collection of LocalNetworks Parameters: Name Type Description Default local_networks Optional[List[dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetwork]] A collection of LocalNetwork objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def __init__ ( self , local_networks : Optional [ List [ LocalNetwork ]] = None ): \"\"\"A collection of LocalNetworks Args: local_networks: A collection of LocalNetwork objects \"\"\" super () . __init__ ( 'ip_and_cidr' , local_networks ) self . local_networks = self . items self . _idx = 0 get_raw ( self ) Get a list of LocalNetworks that can be inserted directly into the network.cfg file Returns: Type Description List[str] A list of local network assignments Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of LocalNetworks that can be inserted directly into the network.cfg file Returns: A list of local network assignments \"\"\" return [ local_network . get_raw () for local_network in self . local_networks ]","title":"local_network.py"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/#dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetwork","text":"","title":"LocalNetwork"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/#dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetwork.__init__","text":"A local network denoted by a IP/CIDR Parameters: Name Type Description Default ip_and_cidr str An IP/CIDR string representing a local network. required description Optional[str] A description of that network's purpose None Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def __init__ ( self , ip_and_cidr : str , description : Optional [ str ] = None ): \"\"\"A local network denoted by a IP/CIDR Args: ip_and_cidr: An IP/CIDR string representing a local network. description: A description of that network's purpose \"\"\" self . ip_and_cidr = ip_and_cidr self . description = description","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/#dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetwork.get_raw","text":"Get a raw representation of this LocalNetwork Returns: Type Description str An assignment statement that can be inserted directly into Zeek's networks.cfg Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this LocalNetwork Returns: An assignment statement that can be inserted directly into Zeek's networks.cfg \"\"\" if self . description : return ' {0: <64} {1} \\n ' . format ( self . ip_and_cidr , self . description ) return ' {0: <64} {1} \\n ' . format ( self . ip_and_cidr , 'Undocumented Network' )","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/#dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetworks","text":"","title":"LocalNetworks"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/#dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetworks.__init__","text":"A collection of LocalNetworks Parameters: Name Type Description Default local_networks Optional[List[dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetwork]] A collection of LocalNetwork objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def __init__ ( self , local_networks : Optional [ List [ LocalNetwork ]] = None ): \"\"\"A collection of LocalNetworks Args: local_networks: A collection of LocalNetwork objects \"\"\" super () . __init__ ( 'ip_and_cidr' , local_networks ) self . local_networks = self . items self . _idx = 0","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_network/#dynamite_nsm.services.base.config_objects.zeek.local_network.LocalNetworks.get_raw","text":"Get a list of LocalNetworks that can be inserted directly into the network.cfg file Returns: Type Description List[str] A list of local network assignments Source code in dynamite_nsm/services/base/config_objects/zeek/local_network.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of LocalNetworks that can be inserted directly into the network.cfg file Returns: A list of local network assignments \"\"\" return [ local_network . get_raw () for local_network in self . local_networks ]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/","text":"Configuration objects built around the site/local.zeek file. To import... from dynamite_nsm.services.base.config_objects.zeek import local_site as zeek_config_local_site Definition __init__ ( self , name , value , enabled = False ) special A global variable applied at runtime. Parameters: Name Type Description Default name str The name of the definition required value str The value associated with the definition required enabled Optional[bool] Whether or not this definition should be enabled False Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , name : str , value : str , enabled : Optional [ bool ] = False ): \"\"\"A global variable applied at runtime. Args: name: The name of the definition value: The value associated with the definition enabled: Whether or not this definition should be enabled \"\"\" super () . __init__ ( name , enabled ) self . value = value get_raw ( self ) Get a raw representation of this Definition Returns: Type Description str A redef statement that can be inserted into Zeek's site/local.zeek Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this Definition Returns: A redef statement that can be inserted into Zeek's site/local.zeek \"\"\" if self . enabled : return f 'redef { self . name } = { self . value } ' return f '#redef { self . name } = { self . value } ' Definitions __init__ ( self , definitions = None ) special A collection of Definitions Parameters: Name Type Description Default definitions List[dynamite_nsm.services.base.config_objects.zeek.local_site.Definition] A collection of Definition objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , definitions : List [ Definition ] = None ): \"\"\"A collection of Definitions Args: definitions: A collection of Definition objects \"\"\" super () . __init__ ( definitions ) self . definitions = self . analyzers get_raw ( self ) Get a list of all the Definitions that can be inserted directly into the site/local.zeek file Returns: Type Description List[str] A list of redef statements Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of all the Definitions that can be inserted directly into the site/local.zeek file Returns: A list of redef statements \"\"\" return [ definition . get_raw () for definition in self . definitions ] Script __init__ ( self , name , enabled = False ) special A script that performs some form of analysis Parameters: Name Type Description Default name str The name of the definition required enabled Optional[bool] Whether this script should be enabled or not False Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False ): \"\"\"A script that performs some form of analysis Args: name: The name of the definition enabled: Whether this script should be enabled or not \"\"\" self . value = None self . name = name content = self . get_contents () super () . __init__ ( name , enabled , content = content ) get_contents ( self ) Get the content of the Zeek script. Returns: Type Description Optional[str] The contents of the Zeek script, if a directory is referenced then the contents of the first Zeek script located within the directory (ASCII order) Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_contents ( self ) -> Optional [ str ]: \"\"\"Get the content of the Zeek script. Returns: The contents of the Zeek script, if a directory is referenced then the contents of the first Zeek script located within the directory (ASCII order) \"\"\" env = utilities . get_environment_file_dict () zeek_scripts_root = env . get ( 'ZEEK_SCRIPTS' , f ' { const . CONFIG_PATH } /zeek/' ) path_pattern_1 = f ' { zeek_scripts_root } / { self . name } ' path_pattern_2 = f ' { zeek_scripts_root } / { self . name } .bro' path_pattern_3 = f ' { zeek_scripts_root } / { self . name } .zeek' path_pattern_4 = f ' { zeek_scripts_root } /base/ { self . name } ' path_pattern_5 = f ' { zeek_scripts_root } /base/ { self . name } .bro' path_pattern_6 = f ' { zeek_scripts_root } /base/ { self . name } .zeek' path_pattern_7 = f ' { zeek_scripts_root } /policy/ { self . name } ' path_pattern_8 = f ' { zeek_scripts_root } /policy/ { self . name } .bro' path_pattern_9 = f ' { zeek_scripts_root } /policy/ { self . name } .zeek' path_pattern_10 = f ' { zeek_scripts_root } /site/ { self . name } ' path_pattern_11 = f ' { zeek_scripts_root } /site/ { self . name } .bro' path_pattern_12 = f ' { zeek_scripts_root } /site/ { self . name } .zeek' path_pattern_13 = f ' { zeek_scripts_root } /site/packages/ { self . name } ' path_pattern_14 = f ' { zeek_scripts_root } /site/packages/ { self . name } .bro' path_pattern_15 = f ' { zeek_scripts_root } /site/packages/ { self . name } .zeek' search_paths = [ path_pattern_1 , path_pattern_2 , path_pattern_3 , path_pattern_4 , path_pattern_5 , path_pattern_6 , path_pattern_7 , path_pattern_8 , path_pattern_9 , path_pattern_10 , path_pattern_11 , path_pattern_12 , path_pattern_13 , path_pattern_14 , path_pattern_15 ] for path_match in search_paths : if os . path . exists ( path_match ): if os . path . isdir ( path_match ): load_directives = \\ [ s for s in os . listdir ( path_match ) if s . endswith ( '.bro' ) or s . endswith ( '.zeek' ) and '__load__' in s ] content_script = f ' { path_match } / { load_directives [ 0 ] } ' with open ( content_script , 'r' ) as content_script_in : return content_script_in . read ( 5120 ) elif os . path . isfile ( path_match ): content_script = path_match with open ( content_script , 'r' ) as content_script_in : return content_script_in . read ( 5120 ) return None get_raw ( self ) Get a raw representation of this Script Returns: Type Description str A @load statement that can be inserted into Zeek's site/local.zeek Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this Script Returns: A @load statement that can be inserted into Zeek's site/local.zeek \"\"\" if self . enabled : return f '@load { self . name } ' return f '#@load { self . name } ' Scripts __init__ ( self , scripts = None ) special A collection of Scripts Parameters: Name Type Description Default scripts Optional[List[dynamite_nsm.services.base.config_objects.zeek.local_site.Script]] A collection of Script objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , scripts : Optional [ List [ Script ]] = None ): \"\"\"A collection of Scripts Args: scripts: A collection of Script objects \"\"\" super () . __init__ ( scripts ) self . scripts = self . analyzers get_raw ( self ) Get a list of all the Scripts that can be inserted directly into the site/local.zeek file Returns: Type Description List[str] A list of @load statements Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of all the Scripts that can be inserted directly into the site/local.zeek file Returns: A list of @load statements \"\"\" return [ script . get_raw () for script in self . scripts ] Signature __init__ ( self , name , enabled = False ) special A signature set made available at runtime. Parameters: Name Type Description Default name str The name of the signature required enabled Optional[bool] Whether this definition should be enabled False Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False ): \"\"\"A signature set made available at runtime. Args: name: The name of the signature enabled: Whether this definition should be enabled \"\"\" self . value = None super () . __init__ ( name , enabled ) get_raw ( self ) Get a raw representation of this Signature Returns: Type Description str A @load-sig statement that can be inserted into Zeek's site/local.zeek Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this Signature Returns: A @load-sig statement that can be inserted into Zeek's site/local.zeek \"\"\" if self . enabled : return f '@load-sigs { self . name } ' return f '#@load-sigs { self . name } ' Signatures __init__ ( self , signatures = None ) special A collection of Signatures Parameters: Name Type Description Default signatures Optional[List[dynamite_nsm.services.base.config_objects.zeek.local_site.Signature]] A collection of Signature objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , signatures : Optional [ List [ Signature ]] = None ): \"\"\"A collection of Signatures Args: signatures: A collection of Signature objects \"\"\" super () . __init__ ( signatures ) self . signatures = self . analyzers get_raw ( self ) Get a list of all the Signatures that can be inserted directly into the site/local.zeek file Returns: Type Description List[str] A list of @load-sigs statements Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of all the Signatures that can be inserted directly into the site/local.zeek file Returns: A list of @load-sigs statements \"\"\" return [ signature . get_raw () for signature in self . signatures ]","title":"local_site.py"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Definition","text":"","title":"Definition"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Definition.__init__","text":"A global variable applied at runtime. Parameters: Name Type Description Default name str The name of the definition required value str The value associated with the definition required enabled Optional[bool] Whether or not this definition should be enabled False Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , name : str , value : str , enabled : Optional [ bool ] = False ): \"\"\"A global variable applied at runtime. Args: name: The name of the definition value: The value associated with the definition enabled: Whether or not this definition should be enabled \"\"\" super () . __init__ ( name , enabled ) self . value = value","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Definition.get_raw","text":"Get a raw representation of this Definition Returns: Type Description str A redef statement that can be inserted into Zeek's site/local.zeek Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this Definition Returns: A redef statement that can be inserted into Zeek's site/local.zeek \"\"\" if self . enabled : return f 'redef { self . name } = { self . value } ' return f '#redef { self . name } = { self . value } '","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Definitions","text":"","title":"Definitions"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Definitions.__init__","text":"A collection of Definitions Parameters: Name Type Description Default definitions List[dynamite_nsm.services.base.config_objects.zeek.local_site.Definition] A collection of Definition objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , definitions : List [ Definition ] = None ): \"\"\"A collection of Definitions Args: definitions: A collection of Definition objects \"\"\" super () . __init__ ( definitions ) self . definitions = self . analyzers","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Definitions.get_raw","text":"Get a list of all the Definitions that can be inserted directly into the site/local.zeek file Returns: Type Description List[str] A list of redef statements Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of all the Definitions that can be inserted directly into the site/local.zeek file Returns: A list of redef statements \"\"\" return [ definition . get_raw () for definition in self . definitions ]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Script","text":"","title":"Script"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Script.__init__","text":"A script that performs some form of analysis Parameters: Name Type Description Default name str The name of the definition required enabled Optional[bool] Whether this script should be enabled or not False Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False ): \"\"\"A script that performs some form of analysis Args: name: The name of the definition enabled: Whether this script should be enabled or not \"\"\" self . value = None self . name = name content = self . get_contents () super () . __init__ ( name , enabled , content = content )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Script.get_contents","text":"Get the content of the Zeek script. Returns: Type Description Optional[str] The contents of the Zeek script, if a directory is referenced then the contents of the first Zeek script located within the directory (ASCII order) Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_contents ( self ) -> Optional [ str ]: \"\"\"Get the content of the Zeek script. Returns: The contents of the Zeek script, if a directory is referenced then the contents of the first Zeek script located within the directory (ASCII order) \"\"\" env = utilities . get_environment_file_dict () zeek_scripts_root = env . get ( 'ZEEK_SCRIPTS' , f ' { const . CONFIG_PATH } /zeek/' ) path_pattern_1 = f ' { zeek_scripts_root } / { self . name } ' path_pattern_2 = f ' { zeek_scripts_root } / { self . name } .bro' path_pattern_3 = f ' { zeek_scripts_root } / { self . name } .zeek' path_pattern_4 = f ' { zeek_scripts_root } /base/ { self . name } ' path_pattern_5 = f ' { zeek_scripts_root } /base/ { self . name } .bro' path_pattern_6 = f ' { zeek_scripts_root } /base/ { self . name } .zeek' path_pattern_7 = f ' { zeek_scripts_root } /policy/ { self . name } ' path_pattern_8 = f ' { zeek_scripts_root } /policy/ { self . name } .bro' path_pattern_9 = f ' { zeek_scripts_root } /policy/ { self . name } .zeek' path_pattern_10 = f ' { zeek_scripts_root } /site/ { self . name } ' path_pattern_11 = f ' { zeek_scripts_root } /site/ { self . name } .bro' path_pattern_12 = f ' { zeek_scripts_root } /site/ { self . name } .zeek' path_pattern_13 = f ' { zeek_scripts_root } /site/packages/ { self . name } ' path_pattern_14 = f ' { zeek_scripts_root } /site/packages/ { self . name } .bro' path_pattern_15 = f ' { zeek_scripts_root } /site/packages/ { self . name } .zeek' search_paths = [ path_pattern_1 , path_pattern_2 , path_pattern_3 , path_pattern_4 , path_pattern_5 , path_pattern_6 , path_pattern_7 , path_pattern_8 , path_pattern_9 , path_pattern_10 , path_pattern_11 , path_pattern_12 , path_pattern_13 , path_pattern_14 , path_pattern_15 ] for path_match in search_paths : if os . path . exists ( path_match ): if os . path . isdir ( path_match ): load_directives = \\ [ s for s in os . listdir ( path_match ) if s . endswith ( '.bro' ) or s . endswith ( '.zeek' ) and '__load__' in s ] content_script = f ' { path_match } / { load_directives [ 0 ] } ' with open ( content_script , 'r' ) as content_script_in : return content_script_in . read ( 5120 ) elif os . path . isfile ( path_match ): content_script = path_match with open ( content_script , 'r' ) as content_script_in : return content_script_in . read ( 5120 ) return None","title":"get_contents()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Script.get_raw","text":"Get a raw representation of this Script Returns: Type Description str A @load statement that can be inserted into Zeek's site/local.zeek Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this Script Returns: A @load statement that can be inserted into Zeek's site/local.zeek \"\"\" if self . enabled : return f '@load { self . name } ' return f '#@load { self . name } '","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Scripts","text":"","title":"Scripts"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Scripts.__init__","text":"A collection of Scripts Parameters: Name Type Description Default scripts Optional[List[dynamite_nsm.services.base.config_objects.zeek.local_site.Script]] A collection of Script objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , scripts : Optional [ List [ Script ]] = None ): \"\"\"A collection of Scripts Args: scripts: A collection of Script objects \"\"\" super () . __init__ ( scripts ) self . scripts = self . analyzers","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Scripts.get_raw","text":"Get a list of all the Scripts that can be inserted directly into the site/local.zeek file Returns: Type Description List[str] A list of @load statements Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of all the Scripts that can be inserted directly into the site/local.zeek file Returns: A list of @load statements \"\"\" return [ script . get_raw () for script in self . scripts ]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Signature","text":"","title":"Signature"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Signature.__init__","text":"A signature set made available at runtime. Parameters: Name Type Description Default name str The name of the signature required enabled Optional[bool] Whether this definition should be enabled False Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , name : str , enabled : Optional [ bool ] = False ): \"\"\"A signature set made available at runtime. Args: name: The name of the signature enabled: Whether this definition should be enabled \"\"\" self . value = None super () . __init__ ( name , enabled )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Signature.get_raw","text":"Get a raw representation of this Signature Returns: Type Description str A @load-sig statement that can be inserted into Zeek's site/local.zeek Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> str : \"\"\"Get a raw representation of this Signature Returns: A @load-sig statement that can be inserted into Zeek's site/local.zeek \"\"\" if self . enabled : return f '@load-sigs { self . name } ' return f '#@load-sigs { self . name } '","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Signatures","text":"","title":"Signatures"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Signatures.__init__","text":"A collection of Signatures Parameters: Name Type Description Default signatures Optional[List[dynamite_nsm.services.base.config_objects.zeek.local_site.Signature]] A collection of Signature objects None Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def __init__ ( self , signatures : Optional [ List [ Signature ]] = None ): \"\"\"A collection of Signatures Args: signatures: A collection of Signature objects \"\"\" super () . __init__ ( signatures ) self . signatures = self . analyzers","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/local_site/#dynamite_nsm.services.base.config_objects.zeek.local_site.Signatures.get_raw","text":"Get a list of all the Signatures that can be inserted directly into the site/local.zeek file Returns: Type Description List[str] A list of @load-sigs statements Source code in dynamite_nsm/services/base/config_objects/zeek/local_site.py def get_raw ( self ) -> List [ str ]: \"\"\"Get a list of all the Signatures that can be inserted directly into the site/local.zeek file Returns: A list of @load-sigs statements \"\"\" return [ signature . get_raw () for signature in self . signatures ]","title":"get_raw()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/","text":"Configuration objects built around Zeek's node.cfg file. To import... from dynamite_nsm.services.base.config_objects.zeek import node as zeek_config_node Logger __init__ ( self , logger_name , host ) special A Zeek logger process Parameters: Name Type Description Default logger_name str The name of the logger required host str The host to bind to required Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , logger_name : str , host : str ): \"\"\" A Zeek logger process Args: logger_name: The name of the logger host: The host to bind to \"\"\" super () . __init__ ( logger_name , 'logger' , host ) Loggers __init__ ( self , loggers = None ) special A collection of one or more loggers Parameters: Name Type Description Default loggers Optional[List[dynamite_nsm.services.base.config_objects.zeek.node.Logger]] A Logger object None Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , loggers : Optional [ List [ Logger ]] = None ): \"\"\" A collection of one or more loggers Args: loggers: A Logger object \"\"\" super () . __init__ ( components = loggers ) Manager __init__ ( self , manager_name , host ) special A Zeek manager process Parameters: Name Type Description Default manager_name str The name of the logger required host str The host to bind to required Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , manager_name : str , host : str ): \"\"\" A Zeek manager process Args: manager_name: The name of the logger host: The host to bind to \"\"\" super () . __init__ ( manager_name , 'manager' , host ) Proxies A collection of one or more proxies Parameters: Name Type Description Default proxies A Proxy object required Proxy A Zeek proxy process Parameters: Name Type Description Default proxy_name The name of the logger required host The host to bind to required Worker __init__ ( self , worker_name , interface_name , cluster_id , cluster_type = 'FANOUT_HASH' , load_balance_processes = 1 , pinned_cpus = ( 0 ,), host = 'localhost' ) special A Zeek worker process that uses AF_PACKET for packet acquisition Parameters: Name Type Description Default worker_name str The name of the worker required interface_name str The name of a network interface to monitor required cluster_id int A unique integer associated with this worker. Maps to af_packet_fanout_id required cluster_type Optional[str] The algorithm used to spread traffic between sockets. Maps to af_packet_fanout_mode 'FANOUT_HASH' load_balance_processes Optional[int] The number of Zeek processes associated with a given worker 1 pinned_cpus Optional[Tuple] List of CPU cores that are dedicated to this worker (0,) host Optional[str] The host to bind to 'localhost' Returns: Type Description None Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , worker_name : str , interface_name : str , cluster_id : int , cluster_type : Optional [ str ] = 'FANOUT_HASH' , load_balance_processes : Optional [ int ] = 1 , pinned_cpus : Optional [ Tuple ] = ( 0 ,), host : Optional [ str ] = 'localhost' ): \"\"\"A Zeek worker process that uses AF_PACKET for packet acquisition Args: worker_name: The name of the worker interface_name: The name of a network interface to monitor cluster_id: A unique integer associated with this worker. Maps to af_packet_fanout_id cluster_type: The algorithm used to spread traffic between sockets. Maps to af_packet_fanout_mode load_balance_processes: The number of Zeek processes associated with a given worker pinned_cpus: List of CPU cores that are dedicated to this worker host: The host to bind to Returns: None \"\"\" super () . __init__ ( worker_name , 'worker' , host ) self . name = worker_name self . interface = interface_name . replace ( 'af_packet::' , '' ) self . cluster_id = cluster_id self . cluster_type = cluster_type . replace ( 'AF_Packet::' , '' ) if self . cluster_type in CLUSTER_TYPE_TO_AF_PACKET_FANOUT_MODE_MAP . keys (): self . cluster_type = CLUSTER_TYPE_TO_AF_PACKET_FANOUT_MODE_MAP . get ( self . cluster_type ) self . load_balance_processes = load_balance_processes self . pinned_cpus = list ( pinned_cpus ) Workers __init__ ( self , workers = None ) special A collection of one or more workers Parameters: Name Type Description Default workers Optional[List[dynamite_nsm.services.base.config_objects.zeek.node.Worker]] A Worker object None Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , workers : Optional [ List [ Worker ]] = None ): \"\"\" A collection of one or more workers Args: workers: A Worker object \"\"\" super () . __init__ ( components = workers )","title":"node.py"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Logger","text":"","title":"Logger"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Logger.__init__","text":"A Zeek logger process Parameters: Name Type Description Default logger_name str The name of the logger required host str The host to bind to required Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , logger_name : str , host : str ): \"\"\" A Zeek logger process Args: logger_name: The name of the logger host: The host to bind to \"\"\" super () . __init__ ( logger_name , 'logger' , host )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Loggers","text":"","title":"Loggers"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Loggers.__init__","text":"A collection of one or more loggers Parameters: Name Type Description Default loggers Optional[List[dynamite_nsm.services.base.config_objects.zeek.node.Logger]] A Logger object None Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , loggers : Optional [ List [ Logger ]] = None ): \"\"\" A collection of one or more loggers Args: loggers: A Logger object \"\"\" super () . __init__ ( components = loggers )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Manager","text":"","title":"Manager"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Manager.__init__","text":"A Zeek manager process Parameters: Name Type Description Default manager_name str The name of the logger required host str The host to bind to required Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , manager_name : str , host : str ): \"\"\" A Zeek manager process Args: manager_name: The name of the logger host: The host to bind to \"\"\" super () . __init__ ( manager_name , 'manager' , host )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Proxies","text":"A collection of one or more proxies Parameters: Name Type Description Default proxies A Proxy object required","title":"Proxies"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Proxy","text":"A Zeek proxy process Parameters: Name Type Description Default proxy_name The name of the logger required host The host to bind to required","title":"Proxy"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Worker","text":"","title":"Worker"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Worker.__init__","text":"A Zeek worker process that uses AF_PACKET for packet acquisition Parameters: Name Type Description Default worker_name str The name of the worker required interface_name str The name of a network interface to monitor required cluster_id int A unique integer associated with this worker. Maps to af_packet_fanout_id required cluster_type Optional[str] The algorithm used to spread traffic between sockets. Maps to af_packet_fanout_mode 'FANOUT_HASH' load_balance_processes Optional[int] The number of Zeek processes associated with a given worker 1 pinned_cpus Optional[Tuple] List of CPU cores that are dedicated to this worker (0,) host Optional[str] The host to bind to 'localhost' Returns: Type Description None Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , worker_name : str , interface_name : str , cluster_id : int , cluster_type : Optional [ str ] = 'FANOUT_HASH' , load_balance_processes : Optional [ int ] = 1 , pinned_cpus : Optional [ Tuple ] = ( 0 ,), host : Optional [ str ] = 'localhost' ): \"\"\"A Zeek worker process that uses AF_PACKET for packet acquisition Args: worker_name: The name of the worker interface_name: The name of a network interface to monitor cluster_id: A unique integer associated with this worker. Maps to af_packet_fanout_id cluster_type: The algorithm used to spread traffic between sockets. Maps to af_packet_fanout_mode load_balance_processes: The number of Zeek processes associated with a given worker pinned_cpus: List of CPU cores that are dedicated to this worker host: The host to bind to Returns: None \"\"\" super () . __init__ ( worker_name , 'worker' , host ) self . name = worker_name self . interface = interface_name . replace ( 'af_packet::' , '' ) self . cluster_id = cluster_id self . cluster_type = cluster_type . replace ( 'AF_Packet::' , '' ) if self . cluster_type in CLUSTER_TYPE_TO_AF_PACKET_FANOUT_MODE_MAP . keys (): self . cluster_type = CLUSTER_TYPE_TO_AF_PACKET_FANOUT_MODE_MAP . get ( self . cluster_type ) self . load_balance_processes = load_balance_processes self . pinned_cpus = list ( pinned_cpus )","title":"__init__()"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Workers","text":"","title":"Workers"},{"location":"guides/developers/SDK/services/base/config_objects/zeek/node/#dynamite_nsm.services.base.config_objects.zeek.node.Workers.__init__","text":"A collection of one or more workers Parameters: Name Type Description Default workers Optional[List[dynamite_nsm.services.base.config_objects.zeek.node.Worker]] A Worker object None Source code in dynamite_nsm/services/base/config_objects/zeek/node.py def __init__ ( self , workers : Optional [ List [ Worker ]] = None ): \"\"\" A collection of one or more workers Args: workers: A Worker object \"\"\" super () . __init__ ( components = workers )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/config/","text":"Configuration wrappers for the main Elasticsearch configuration file. To import... from dynamite_nsm.services.elasticsearch import config as elasticsearch_config ChangePasswordManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Manage an Elasticsearch internal users passwords Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage an Elasticsearch internal users passwords Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . elasticsearch_config_path = f ' { self . configuration_directory } /security/internal_users.yml' extract_tokens = { '_admin_hash' : ( 'admin' , 'hash' ), '_kibanaserver_hash' : ( 'kibanaserver' , 'hash' ), '_kibanaro_hash' : ( 'kibanaro' , 'hash' ), '_logstash_hash' : ( 'logstash' , 'hash' ), '_readall_hash' : ( 'readall' , 'hash' ), '_snapshotrestore_hash' : ( 'snapshotrestore' , 'hash' ) } self . _admin_hash = None self . _kibanaserver_hash = None self . _kibanaro_hash = None self . _logstash_hash = None self . _readall_hash = None self . _snapshotrestore_hash = None self . admin = 'Hidden' self . kibana_server = 'Hidden' self . kibana_readonly = 'Hidden' self . logstash = 'Hidden' self . readall = 'Hidden' self . snapshot_restore = 'Hidden' with open ( self . elasticsearch_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'elasticsearch.chpasswd' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The very first line of the YAML file None Returns: Type Description None None Source code in dynamite_nsm/services/elasticsearch/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The very first line of the YAML file Returns: None \"\"\" if not out_file_path : out_file_path = self . elasticsearch_config_path if self . admin != 'Hidden' : self . _admin_hash = self . hash_plaintext ( self . admin ) if self . kibana_server != 'Hidden' : self . _kibanaserver_hash = self . hash_plaintext ( self . kibana_server ) if self . kibana_readonly != 'Hidden' : self . _kibanaro_hash = self . hash_plaintext ( self . kibana_readonly ) if self . logstash != 'Hidden' : self . _logstash_hash = self . hash_plaintext ( self . logstash ) if self . snapshot_restore != 'Hidden' : self . _snapshotrestore_hash = self . hash_plaintext ( self . snapshot_restore ) super ( ChangePasswordManager , self ) . commit ( out_file_path , backup_directory ) main_config = ConfigManager ( self . configuration_directory ) self . logger . warning ( 'Cycling Elasticsearch service in order for changes to take effect.' ) setup_security . InstallElasticsearchCertificates ( network_host = main_config . network_host , terminate_elasticsearch = False ) . invoke () ConfigManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Manage an Elasticsearch configuration Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage an Elasticsearch configuration Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" extract_tokens = { 'node_name' : ( 'node.name' ,), 'cluster_name' : ( 'cluster.name' ,), 'seed_hosts' : ( 'discovery.seed_hosts' ,), 'initial_master_nodes' : ( 'cluster.initial_master_nodes' ,), 'network_host' : ( 'network.host' ,), 'http_port' : ( 'http.port' ,), 'path_data' : ( 'path.data' ,), 'path_logs' : ( 'path.logs' ,), 'search_max_buckets' : ( 'search.max_buckets' ,), 'transport_pem_cert_file' : ( 'opendistro_security.ssl.transport.pemcert_filepath' ,), 'transport_pem_key_file' : ( 'opendistro_security.ssl.transport.pemkey_filepath' ,), 'transport_trusted_cas_file' : ( 'opendistro_security.ssl.transport.pemtrustedcas_filepath' ,), 'rest_api_pem_cert_file' : ( 'opendistro_security.ssl.http.pemcert_filepath' ,), 'rest_api_pem_key_file' : ( 'opendistro_security.ssl.http.pemkey_filepath' ,), 'rest_api_trusted_cas_file' : ( 'opendistro_security.ssl.http.pemtrustedcas_filepath' ,), 'authcz_admin_distinguished_names' : ( 'opendistro_security.authcz.admin_dn' ,) } self . node_name = None self . cluster_name = None self . seed_hosts = None self . initial_master_nodes = None self . network_host = None self . http_port = None self . path_logs = None self . search_max_buckets = None self . rest_api_pem_cert_file = None self . rest_api_pem_key_file = None self . rest_api_trusted_cas_file = None self . transport_pem_cert_file = None self . transport_pem_cert_file = None self . transport_pem_key_file = None self . transport_trusted_cas_file = None self . authcz_admin_distinguished_names = None self . configuration_directory = configuration_directory self . elasticsearch_config_path = f ' { self . configuration_directory } /elasticsearch.yml' with open ( self . elasticsearch_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'elasticsearch.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The very first line of the YAML file None Returns: Type Description None None Source code in dynamite_nsm/services/elasticsearch/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The very first line of the YAML file Returns: None \"\"\" if not out_file_path : out_file_path = self . elasticsearch_config_path super ( ConfigManager , self ) . commit ( out_file_path , backup_directory ) JavaHeapOptionsConfigManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Configure Elasticsearch Java Heap Options Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configure Elasticsearch Java Heap Options Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . elasticsearch_jvm_config_path = f ' { self . configuration_directory } /jvm.options' with open ( self . elasticsearch_jvm_config_path ) as jvm_config : data = { 'data' : jvm_config . readlines ()} super () . __init__ ( data , name = 'elasticsearch.java' , verbose = verbose , stdout = stdout ) commit ( self , out_file_path = None , backup_directory = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None Source code in dynamite_nsm/services/elasticsearch/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory \"\"\" if not out_file_path : out_file_path = self . elasticsearch_jvm_config_path super ( JavaHeapOptionsConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"config"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.ChangePasswordManager","text":"","title":"ChangePasswordManager"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.ChangePasswordManager.__init__","text":"Manage an Elasticsearch internal users passwords Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage an Elasticsearch internal users passwords Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . elasticsearch_config_path = f ' { self . configuration_directory } /security/internal_users.yml' extract_tokens = { '_admin_hash' : ( 'admin' , 'hash' ), '_kibanaserver_hash' : ( 'kibanaserver' , 'hash' ), '_kibanaro_hash' : ( 'kibanaro' , 'hash' ), '_logstash_hash' : ( 'logstash' , 'hash' ), '_readall_hash' : ( 'readall' , 'hash' ), '_snapshotrestore_hash' : ( 'snapshotrestore' , 'hash' ) } self . _admin_hash = None self . _kibanaserver_hash = None self . _kibanaro_hash = None self . _logstash_hash = None self . _readall_hash = None self . _snapshotrestore_hash = None self . admin = 'Hidden' self . kibana_server = 'Hidden' self . kibana_readonly = 'Hidden' self . logstash = 'Hidden' self . readall = 'Hidden' self . snapshot_restore = 'Hidden' with open ( self . elasticsearch_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'elasticsearch.chpasswd' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file ()","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.ChangePasswordManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The very first line of the YAML file None Returns: Type Description None None Source code in dynamite_nsm/services/elasticsearch/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The very first line of the YAML file Returns: None \"\"\" if not out_file_path : out_file_path = self . elasticsearch_config_path if self . admin != 'Hidden' : self . _admin_hash = self . hash_plaintext ( self . admin ) if self . kibana_server != 'Hidden' : self . _kibanaserver_hash = self . hash_plaintext ( self . kibana_server ) if self . kibana_readonly != 'Hidden' : self . _kibanaro_hash = self . hash_plaintext ( self . kibana_readonly ) if self . logstash != 'Hidden' : self . _logstash_hash = self . hash_plaintext ( self . logstash ) if self . snapshot_restore != 'Hidden' : self . _snapshotrestore_hash = self . hash_plaintext ( self . snapshot_restore ) super ( ChangePasswordManager , self ) . commit ( out_file_path , backup_directory ) main_config = ConfigManager ( self . configuration_directory ) self . logger . warning ( 'Cycling Elasticsearch service in order for changes to take effect.' ) setup_security . InstallElasticsearchCertificates ( network_host = main_config . network_host , terminate_elasticsearch = False ) . invoke ()","title":"commit()"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.ConfigManager","text":"","title":"ConfigManager"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.ConfigManager.__init__","text":"Manage an Elasticsearch configuration Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage an Elasticsearch configuration Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" extract_tokens = { 'node_name' : ( 'node.name' ,), 'cluster_name' : ( 'cluster.name' ,), 'seed_hosts' : ( 'discovery.seed_hosts' ,), 'initial_master_nodes' : ( 'cluster.initial_master_nodes' ,), 'network_host' : ( 'network.host' ,), 'http_port' : ( 'http.port' ,), 'path_data' : ( 'path.data' ,), 'path_logs' : ( 'path.logs' ,), 'search_max_buckets' : ( 'search.max_buckets' ,), 'transport_pem_cert_file' : ( 'opendistro_security.ssl.transport.pemcert_filepath' ,), 'transport_pem_key_file' : ( 'opendistro_security.ssl.transport.pemkey_filepath' ,), 'transport_trusted_cas_file' : ( 'opendistro_security.ssl.transport.pemtrustedcas_filepath' ,), 'rest_api_pem_cert_file' : ( 'opendistro_security.ssl.http.pemcert_filepath' ,), 'rest_api_pem_key_file' : ( 'opendistro_security.ssl.http.pemkey_filepath' ,), 'rest_api_trusted_cas_file' : ( 'opendistro_security.ssl.http.pemtrustedcas_filepath' ,), 'authcz_admin_distinguished_names' : ( 'opendistro_security.authcz.admin_dn' ,) } self . node_name = None self . cluster_name = None self . seed_hosts = None self . initial_master_nodes = None self . network_host = None self . http_port = None self . path_logs = None self . search_max_buckets = None self . rest_api_pem_cert_file = None self . rest_api_pem_key_file = None self . rest_api_trusted_cas_file = None self . transport_pem_cert_file = None self . transport_pem_cert_file = None self . transport_pem_key_file = None self . transport_trusted_cas_file = None self . authcz_admin_distinguished_names = None self . configuration_directory = configuration_directory self . elasticsearch_config_path = f ' { self . configuration_directory } /elasticsearch.yml' with open ( self . elasticsearch_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'elasticsearch.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file ()","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.ConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The very first line of the YAML file None Returns: Type Description None None Source code in dynamite_nsm/services/elasticsearch/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The very first line of the YAML file Returns: None \"\"\" if not out_file_path : out_file_path = self . elasticsearch_config_path super ( ConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.JavaHeapOptionsConfigManager","text":"","title":"JavaHeapOptionsConfigManager"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.JavaHeapOptionsConfigManager.__init__","text":"Configure Elasticsearch Java Heap Options Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configure Elasticsearch Java Heap Options Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . elasticsearch_jvm_config_path = f ' { self . configuration_directory } /jvm.options' with open ( self . elasticsearch_jvm_config_path ) as jvm_config : data = { 'data' : jvm_config . readlines ()} super () . __init__ ( data , name = 'elasticsearch.java' , verbose = verbose , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/config/#dynamite_nsm.services.elasticsearch.config.JavaHeapOptionsConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None Source code in dynamite_nsm/services/elasticsearch/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory \"\"\" if not out_file_path : out_file_path = self . elasticsearch_jvm_config_path super ( JavaHeapOptionsConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/elasticsearch/install/","text":"Installation Manager for Elasticsearch and its dependencies. To import... from dynamite_nsm.services.elasticsearch import install as elasticsearch_install InstallManager __init__ ( self , configuration_directory , install_directory , log_directory , download_elasticsearch_archive = True , stdout = False , verbose = False ) special Install Elasticsearch Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required install_directory str Path to the install directory (E.G /opt/dynamite/elasticsearch/) required log_directory str Path to the log directory (E.G /var/log/dynamite/elasticsearch/) required download_elasticsearch_archive Optional[bool] If True, download the ElasticSearch archive from a mirror True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_elasticsearch_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Elasticsearch Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) install_directory: Path to the install directory (E.G /opt/dynamite/elasticsearch/) log_directory: Path to the log directory (E.G /var/log/dynamite/elasticsearch/) download_elasticsearch_archive: If True, download the ElasticSearch archive from a mirror stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . stdout = stdout self . verbose = verbose install . BaseInstallManager . __init__ ( self , 'elasticsearch.install' , verbose = self . verbose , stdout = stdout ) java_home = self . dynamite_environ . get ( 'JAVA_HOME' ) if not java_home : self . logger . info ( 'Installing compatible version of Java.' ) from dynamite_nsm.services.java import install as java_install java_install . InstallManager ( const . JVM_ROOT , stdout = stdout , verbose = verbose ) . setup () if download_elasticsearch_archive : self . logger . info ( \"Attempting to download Elasticsearch (OpenDistro) archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . ELASTICSEARCH_MIRRORS ) self . logger . info ( f 'Attempting to extract Elasticsearch archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . ELASTICSEARCH_MIRRORS ) copy_elasticsearch_files_and_directories ( self ) Copy the required Elasticsearch files from the install cache to their respective directories Source code in dynamite_nsm/services/elasticsearch/install.py def copy_elasticsearch_files_and_directories ( self ) -> None : \"\"\" Copy the required Elasticsearch files from the install cache to their respective directories \"\"\" elasticsearch_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'config/elasticsearch.yml' , 'config/jvm.options' , 'config/log4j2.properties' , 'config/opendistro-reports-scheduler' ] install_paths = [ 'bin/' , 'data/' , 'lib/' , 'logs/' , 'modules/' , 'plugins/' ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { elasticsearch_tarball_extracted } / { conf } ' , self . configuration_directory ) for inst in install_paths : self . copy_file_or_directory_to_destination ( f ' { elasticsearch_tarball_extracted } / { inst } ' , self . install_directory ) create_update_elasticsearch_environment_variables ( self ) Creates all the required ElasticSearch environmental variables Source code in dynamite_nsm/services/elasticsearch/install.py def create_update_elasticsearch_environment_variables ( self ) -> None : \"\"\" Creates all the required ElasticSearch environmental variables \"\"\" self . create_update_env_variable ( 'ES_PATH_CONF' , self . configuration_directory ) self . create_update_env_variable ( 'ES_HOME' , self . install_directory ) self . create_update_env_variable ( 'ES_LOGS' , self . log_directory ) setup ( self , node_name = None , network_host = None , port = None , initial_master_nodes = None , discover_seed_hosts = None , tls_cert_subject = None , heap_size_gigs = None ) Setup Elasticsearch Parameters: Name Type Description Default node_name Optional[str] The name of this elasticsearch node None network_host Optional[str] The IP address to listen on (E.G \"0.0.0.0\") None port Optional[int] The port that the ES API is bound to (E.G 9200) None initial_master_nodes Optional[List[str]] A list of nodes representing master (and master-eligible) nodes in this cluster None discover_seed_hosts Optional[List[str]] A list of IPs on other hosts you wish to form a cluster with None tls_cert_subject Optional[str] Denotes the thing being secured; E.G (/C=US/ST=GA/L=Atlanta/O=Dynamite Analytics/OU=R&D/CN=dynamite.ai) None heap_size_gigs Optional[int] The initial/max java heap space to allocate None Returns: Type Description None None Source code in dynamite_nsm/services/elasticsearch/install.py def setup ( self , node_name : Optional [ str ] = None , network_host : Optional [ str ] = None , port : Optional [ int ] = None , initial_master_nodes : Optional [ List [ str ]] = None , discover_seed_hosts : Optional [ List [ str ]] = None , tls_cert_subject : Optional [ str ] = None , heap_size_gigs : Optional [ int ] = None ) -> None : \"\"\"Setup Elasticsearch Args: node_name: The name of this elasticsearch node network_host: The IP address to listen on (E.G \"0.0.0.0\") port: The port that the ES API is bound to (E.G 9200) initial_master_nodes: A list of nodes representing master (and master-eligible) nodes in this cluster discover_seed_hosts: A list of IPs on other hosts you wish to form a cluster with tls_cert_subject: Denotes the thing being secured; E.G (/C=US/ST=GA/L=Atlanta/O=Dynamite Analytics/OU=R&D/CN=dynamite.ai) heap_size_gigs: The initial/max java heap space to allocate Returns: None \"\"\" sysctl = systemctl . SystemCtl () # System patching and directory setup self . logger . debug ( 'Patching sysctl.' ) utilities . update_sysctl () self . logger . debug ( 'Patching file-handle limits.' ) utilities . update_user_file_handle_limits () self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . debug ( f 'Creating directory: { self . log_directory } ' ) utilities . makedirs ( self . log_directory ) self . copy_elasticsearch_files_and_directories () self . create_update_elasticsearch_environment_variables () # Overwrite with dynamite default configurations self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /elasticsearch/elasticsearch.yml' , self . configuration_directory ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /elasticsearch/jvm.options' , self . configuration_directory ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /elasticsearch/security' , self . configuration_directory ) # Optimize Configurations es_main_config = config . ConfigManager ( self . configuration_directory , verbose = self . verbose , stdout = self . stdout ) es_java_config = config . JavaHeapOptionsConfigManager ( self . configuration_directory , verbose = self . verbose , stdout = self . stdout ) es_main_config . path_logs = self . log_directory if not node_name : node_name = utilities . get_default_es_node_name () if not network_host : network_host = utilities . get_primary_ip_address () if not port : port = 9200 if not initial_master_nodes : initial_master_nodes = [ node_name ] if not discover_seed_hosts : discover_seed_hosts = [ network_host ] if not tls_cert_subject : tls_cert_subject = '/C=US/ST=GA/L=Atlanta/O=Dynamite/OU=R&D/CN=dynamite.ai' else : tls_cert_subject = tls_cert_subject if not heap_size_gigs : reserved_memory = utilities . get_memory_available_bytes () * .75 heap_size_gigs = int (( reserved_memory / 10 ** 9 ) / 2 ) formatted_subj = tls_cert_subject . lstrip ( \"/\" ) . replace ( \"/\" , \",\" ) formatted_subj_2 = ',' . join ( reversed ( formatted_subj . split ( ',' ))) es_main_config . node_name = node_name es_main_config . network_host = network_host es_main_config . http_port = port es_main_config . initial_master_nodes = initial_master_nodes es_main_config . seed_hosts = discover_seed_hosts es_main_config . authcz_admin_distinguished_names = [ formatted_subj , formatted_subj_2 ] es_java_config . initial_memory = f ' { heap_size_gigs } g' es_java_config . maximum_memory = f ' { heap_size_gigs } g' self . logger . debug ( f 'Java Heap Initial & Max Memory = { heap_size_gigs } GB' ) es_main_config . commit () es_java_config . commit () self . logger . info ( 'Applying configuration.' ) # Fix Permissions self . logger . info ( 'Setting up file permissions.' ) utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . log_directory , user = 'dynamite' , group = 'dynamite' ) # Install and enable service self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/elasticsearch.service' ) sysctl . install_and_enable ( f ' { const . DEFAULT_CONFIGS } /systemd/elasticsearch.service' ) self . logger . info ( 'Generating SSL Certificates and Keys.' ) ssl_gen_task_results = setup_security . GenerateElasticsearchSSLCertificates ( subj = tls_cert_subject ) . invoke () for res in ssl_gen_task_results : cmd , out , err = res self . logger . debug ( f ' { \" \" . join ( cmd ) } -> out: { out } err: { err } ' ) self . logger . info ( 'Installing SSL Certificates and Keys' ) install_ssl_task_results = setup_security . InstallElasticsearchCertificates ( network_host = network_host ) . invoke () for res in install_ssl_task_results : cmd , out , err = res self . logger . debug ( f ' { \" \" . join ( cmd ) } -> out: { out } err: { err } ' ) self . logger . info ( 'Setting up persistent cluster settings.' ) configure_cluster_task_results = configure_cluster . UpdateClusterSettings ( network_host = network_host , http_port = port ) . invoke () rcode , msg = configure_cluster_task_results self . logger . debug ( f 'rcode: { rcode } -> msg: { msg } ' ) \"\"\" # We'll revisit this in another release self.logger.info('Install events_to_hosts job.') event_to_host_task = install_events_to_hosts.EventsToHostsTask('admin', 'admin', target=f'https://{network_host}:{port}') event_to_host_task.download_and_install() event_to_host_task.create_cronjob(interval_minutes=5) \"\"\" UninstallManager __init__ ( self , purge_config = True , stdout = False , verbose = False ) special Uninstall Elasticsearch Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/elasticsearch/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Elasticsearch Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.elasticsearch.process import ProcessManager env_vars = utilities . get_environment_file_dict () es_directories = [ env_vars . get ( 'ES_HOME' ), env_vars . get ( 'ES_LOGS' )] if purge_config : es_directories . append ( env_vars . get ( 'ES_PATH_CONF' )) super () . __init__ ( 'elasticsearch.uninstall' , directories = es_directories , environ_vars = [ 'ES_PATH_CONF' , 'ES_HOME' , 'ES_LOGS' ], process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'elasticsearch.service' , stdout = stdout , verbose = verbose )","title":"install"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.InstallManager","text":"","title":"InstallManager"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.InstallManager.__init__","text":"Install Elasticsearch Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) required install_directory str Path to the install directory (E.G /opt/dynamite/elasticsearch/) required log_directory str Path to the log directory (E.G /var/log/dynamite/elasticsearch/) required download_elasticsearch_archive Optional[bool] If True, download the ElasticSearch archive from a mirror True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/elasticsearch/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_elasticsearch_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Elasticsearch Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/elasticsearch/) install_directory: Path to the install directory (E.G /opt/dynamite/elasticsearch/) log_directory: Path to the log directory (E.G /var/log/dynamite/elasticsearch/) download_elasticsearch_archive: If True, download the ElasticSearch archive from a mirror stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . stdout = stdout self . verbose = verbose install . BaseInstallManager . __init__ ( self , 'elasticsearch.install' , verbose = self . verbose , stdout = stdout ) java_home = self . dynamite_environ . get ( 'JAVA_HOME' ) if not java_home : self . logger . info ( 'Installing compatible version of Java.' ) from dynamite_nsm.services.java import install as java_install java_install . InstallManager ( const . JVM_ROOT , stdout = stdout , verbose = verbose ) . setup () if download_elasticsearch_archive : self . logger . info ( \"Attempting to download Elasticsearch (OpenDistro) archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . ELASTICSEARCH_MIRRORS ) self . logger . info ( f 'Attempting to extract Elasticsearch archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . ELASTICSEARCH_MIRRORS )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.InstallManager.copy_elasticsearch_files_and_directories","text":"Copy the required Elasticsearch files from the install cache to their respective directories Source code in dynamite_nsm/services/elasticsearch/install.py def copy_elasticsearch_files_and_directories ( self ) -> None : \"\"\" Copy the required Elasticsearch files from the install cache to their respective directories \"\"\" elasticsearch_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'config/elasticsearch.yml' , 'config/jvm.options' , 'config/log4j2.properties' , 'config/opendistro-reports-scheduler' ] install_paths = [ 'bin/' , 'data/' , 'lib/' , 'logs/' , 'modules/' , 'plugins/' ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { elasticsearch_tarball_extracted } / { conf } ' , self . configuration_directory ) for inst in install_paths : self . copy_file_or_directory_to_destination ( f ' { elasticsearch_tarball_extracted } / { inst } ' , self . install_directory )","title":"copy_elasticsearch_files_and_directories()"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.InstallManager.create_update_elasticsearch_environment_variables","text":"Creates all the required ElasticSearch environmental variables Source code in dynamite_nsm/services/elasticsearch/install.py def create_update_elasticsearch_environment_variables ( self ) -> None : \"\"\" Creates all the required ElasticSearch environmental variables \"\"\" self . create_update_env_variable ( 'ES_PATH_CONF' , self . configuration_directory ) self . create_update_env_variable ( 'ES_HOME' , self . install_directory ) self . create_update_env_variable ( 'ES_LOGS' , self . log_directory )","title":"create_update_elasticsearch_environment_variables()"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.InstallManager.setup","text":"Setup Elasticsearch Parameters: Name Type Description Default node_name Optional[str] The name of this elasticsearch node None network_host Optional[str] The IP address to listen on (E.G \"0.0.0.0\") None port Optional[int] The port that the ES API is bound to (E.G 9200) None initial_master_nodes Optional[List[str]] A list of nodes representing master (and master-eligible) nodes in this cluster None discover_seed_hosts Optional[List[str]] A list of IPs on other hosts you wish to form a cluster with None tls_cert_subject Optional[str] Denotes the thing being secured; E.G (/C=US/ST=GA/L=Atlanta/O=Dynamite Analytics/OU=R&D/CN=dynamite.ai) None heap_size_gigs Optional[int] The initial/max java heap space to allocate None Returns: Type Description None None Source code in dynamite_nsm/services/elasticsearch/install.py def setup ( self , node_name : Optional [ str ] = None , network_host : Optional [ str ] = None , port : Optional [ int ] = None , initial_master_nodes : Optional [ List [ str ]] = None , discover_seed_hosts : Optional [ List [ str ]] = None , tls_cert_subject : Optional [ str ] = None , heap_size_gigs : Optional [ int ] = None ) -> None : \"\"\"Setup Elasticsearch Args: node_name: The name of this elasticsearch node network_host: The IP address to listen on (E.G \"0.0.0.0\") port: The port that the ES API is bound to (E.G 9200) initial_master_nodes: A list of nodes representing master (and master-eligible) nodes in this cluster discover_seed_hosts: A list of IPs on other hosts you wish to form a cluster with tls_cert_subject: Denotes the thing being secured; E.G (/C=US/ST=GA/L=Atlanta/O=Dynamite Analytics/OU=R&D/CN=dynamite.ai) heap_size_gigs: The initial/max java heap space to allocate Returns: None \"\"\" sysctl = systemctl . SystemCtl () # System patching and directory setup self . logger . debug ( 'Patching sysctl.' ) utilities . update_sysctl () self . logger . debug ( 'Patching file-handle limits.' ) utilities . update_user_file_handle_limits () self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . debug ( f 'Creating directory: { self . log_directory } ' ) utilities . makedirs ( self . log_directory ) self . copy_elasticsearch_files_and_directories () self . create_update_elasticsearch_environment_variables () # Overwrite with dynamite default configurations self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /elasticsearch/elasticsearch.yml' , self . configuration_directory ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /elasticsearch/jvm.options' , self . configuration_directory ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /elasticsearch/security' , self . configuration_directory ) # Optimize Configurations es_main_config = config . ConfigManager ( self . configuration_directory , verbose = self . verbose , stdout = self . stdout ) es_java_config = config . JavaHeapOptionsConfigManager ( self . configuration_directory , verbose = self . verbose , stdout = self . stdout ) es_main_config . path_logs = self . log_directory if not node_name : node_name = utilities . get_default_es_node_name () if not network_host : network_host = utilities . get_primary_ip_address () if not port : port = 9200 if not initial_master_nodes : initial_master_nodes = [ node_name ] if not discover_seed_hosts : discover_seed_hosts = [ network_host ] if not tls_cert_subject : tls_cert_subject = '/C=US/ST=GA/L=Atlanta/O=Dynamite/OU=R&D/CN=dynamite.ai' else : tls_cert_subject = tls_cert_subject if not heap_size_gigs : reserved_memory = utilities . get_memory_available_bytes () * .75 heap_size_gigs = int (( reserved_memory / 10 ** 9 ) / 2 ) formatted_subj = tls_cert_subject . lstrip ( \"/\" ) . replace ( \"/\" , \",\" ) formatted_subj_2 = ',' . join ( reversed ( formatted_subj . split ( ',' ))) es_main_config . node_name = node_name es_main_config . network_host = network_host es_main_config . http_port = port es_main_config . initial_master_nodes = initial_master_nodes es_main_config . seed_hosts = discover_seed_hosts es_main_config . authcz_admin_distinguished_names = [ formatted_subj , formatted_subj_2 ] es_java_config . initial_memory = f ' { heap_size_gigs } g' es_java_config . maximum_memory = f ' { heap_size_gigs } g' self . logger . debug ( f 'Java Heap Initial & Max Memory = { heap_size_gigs } GB' ) es_main_config . commit () es_java_config . commit () self . logger . info ( 'Applying configuration.' ) # Fix Permissions self . logger . info ( 'Setting up file permissions.' ) utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . log_directory , user = 'dynamite' , group = 'dynamite' ) # Install and enable service self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/elasticsearch.service' ) sysctl . install_and_enable ( f ' { const . DEFAULT_CONFIGS } /systemd/elasticsearch.service' ) self . logger . info ( 'Generating SSL Certificates and Keys.' ) ssl_gen_task_results = setup_security . GenerateElasticsearchSSLCertificates ( subj = tls_cert_subject ) . invoke () for res in ssl_gen_task_results : cmd , out , err = res self . logger . debug ( f ' { \" \" . join ( cmd ) } -> out: { out } err: { err } ' ) self . logger . info ( 'Installing SSL Certificates and Keys' ) install_ssl_task_results = setup_security . InstallElasticsearchCertificates ( network_host = network_host ) . invoke () for res in install_ssl_task_results : cmd , out , err = res self . logger . debug ( f ' { \" \" . join ( cmd ) } -> out: { out } err: { err } ' ) self . logger . info ( 'Setting up persistent cluster settings.' ) configure_cluster_task_results = configure_cluster . UpdateClusterSettings ( network_host = network_host , http_port = port ) . invoke () rcode , msg = configure_cluster_task_results self . logger . debug ( f 'rcode: { rcode } -> msg: { msg } ' ) \"\"\" # We'll revisit this in another release self.logger.info('Install events_to_hosts job.') event_to_host_task = install_events_to_hosts.EventsToHostsTask('admin', 'admin', target=f'https://{network_host}:{port}') event_to_host_task.download_and_install() event_to_host_task.create_cronjob(interval_minutes=5) \"\"\"","title":"setup()"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.UninstallManager","text":"","title":"UninstallManager"},{"location":"guides/developers/SDK/services/elasticsearch/install/#dynamite_nsm.services.elasticsearch.install.UninstallManager.__init__","text":"Uninstall Elasticsearch Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/elasticsearch/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Elasticsearch Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.elasticsearch.process import ProcessManager env_vars = utilities . get_environment_file_dict () es_directories = [ env_vars . get ( 'ES_HOME' ), env_vars . get ( 'ES_LOGS' )] if purge_config : es_directories . append ( env_vars . get ( 'ES_PATH_CONF' )) super () . __init__ ( 'elasticsearch.uninstall' , directories = es_directories , environ_vars = [ 'ES_PATH_CONF' , 'ES_HOME' , 'ES_LOGS' ], process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'elasticsearch.service' , stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/process/","text":"Process Manager for Elasticsearch process To import... from dynamite_nsm.services.elasticsearch import process as elasticsearch_process CallElasticProcessError __init__ ( self , message ) special Thrown when elasticsearch process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/elasticsearch/process.py def __init__ ( self , message ): \"\"\"Thrown when elasticsearch process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling elasticsearch process: {} \" . format ( message ) super ( CallElasticProcessError , self ) . __init__ ( msg ) ProcessManager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Elasticsearch Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/elasticsearch/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Elasticsearch Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" environ = utilities . get_environment_file_dict () process . BaseProcessManager . __init__ ( self , 'elasticsearch.service' , 'elasticsearch.process' , log_path = environ . get ( 'ES_LOGS' ), create_pid_file = False , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not elasticsearch_profile . ProcessProfiler () . is_installed (): self . logger . error ( \"Elasticsearch is not installed. Install it with 'dynamite elasticsearch install -h'\" ) raise CallElasticProcessError ( \"Elasticsearch is not installed.\" )","title":"process"},{"location":"guides/developers/SDK/services/elasticsearch/process/#dynamite_nsm.services.elasticsearch.process.CallElasticProcessError","text":"","title":"CallElasticProcessError"},{"location":"guides/developers/SDK/services/elasticsearch/process/#dynamite_nsm.services.elasticsearch.process.CallElasticProcessError.__init__","text":"Thrown when elasticsearch process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/elasticsearch/process.py def __init__ ( self , message ): \"\"\"Thrown when elasticsearch process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling elasticsearch process: {} \" . format ( message ) super ( CallElasticProcessError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/process/#dynamite_nsm.services.elasticsearch.process.ProcessManager","text":"","title":"ProcessManager"},{"location":"guides/developers/SDK/services/elasticsearch/process/#dynamite_nsm.services.elasticsearch.process.ProcessManager.__init__","text":"Manage Elasticsearch Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/elasticsearch/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Elasticsearch Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" environ = utilities . get_environment_file_dict () process . BaseProcessManager . __init__ ( self , 'elasticsearch.service' , 'elasticsearch.process' , log_path = environ . get ( 'ES_LOGS' ), create_pid_file = False , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not elasticsearch_profile . ProcessProfiler () . is_installed (): self . logger . error ( \"Elasticsearch is not installed. Install it with 'dynamite elasticsearch install -h'\" ) raise CallElasticProcessError ( \"Elasticsearch is not installed.\" )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/profile/","text":"Profile Elasticsearch processes to ensure they are running and installed properly. To import... from dynamite_nsm.services.elasticsearch import profile as elasticsearch_profile ProcessProfiler __init__ ( self ) special Get information about the Elasticsearch service Source code in dynamite_nsm/services/elasticsearch/profile.py def __init__ ( self ): \"\"\" Get information about the Elasticsearch service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . elasticsearch_home = self . env_dict . get ( 'ES_HOME' ) self . elasticsearch_config = self . env_dict . get ( 'ES_PATH_CONF' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . elasticsearch_home , config_directory = self . elasticsearch_config , required_install_files = [ 'bin' , 'data' , 'lib' , 'modules' ], required_config_files = [ 'elasticsearch.yml' , 'jvm.options' ] ) is_listening ( self ) Check if Elasticsearch is listening Returns: Type Description bool True, if Elasticsearch HTTP service is listening Source code in dynamite_nsm/services/elasticsearch/profile.py def is_listening ( self ) -> bool : \"\"\" Check if Elasticsearch is listening Returns: True, if Elasticsearch HTTP service is listening \"\"\" if not self . elasticsearch_config : return False if not os . path . exists ( self . elasticsearch_config ): return False es_config_obj = elastic_configs . ConfigManager ( configuration_directory = self . elasticsearch_config ) host = es_config_obj . network_host port = es_config_obj . http_port return utilities . check_socket ( host , port ) is_running ( self ) Check if Elasticsearch is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/elasticsearch/profile.py def is_running ( self ) -> bool : \"\"\"Check if Elasticsearch is running Returns: True, if running \"\"\" if self . elasticsearch_home : try : return elastic_process . ProcessManager () . status ()[ 'running' ] except KeyError : return elastic_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"profile"},{"location":"guides/developers/SDK/services/elasticsearch/profile/#dynamite_nsm.services.elasticsearch.profile.ProcessProfiler","text":"","title":"ProcessProfiler"},{"location":"guides/developers/SDK/services/elasticsearch/profile/#dynamite_nsm.services.elasticsearch.profile.ProcessProfiler.__init__","text":"Get information about the Elasticsearch service Source code in dynamite_nsm/services/elasticsearch/profile.py def __init__ ( self ): \"\"\" Get information about the Elasticsearch service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . elasticsearch_home = self . env_dict . get ( 'ES_HOME' ) self . elasticsearch_config = self . env_dict . get ( 'ES_PATH_CONF' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . elasticsearch_home , config_directory = self . elasticsearch_config , required_install_files = [ 'bin' , 'data' , 'lib' , 'modules' ], required_config_files = [ 'elasticsearch.yml' , 'jvm.options' ] )","title":"__init__()"},{"location":"guides/developers/SDK/services/elasticsearch/profile/#dynamite_nsm.services.elasticsearch.profile.ProcessProfiler.is_listening","text":"Check if Elasticsearch is listening Returns: Type Description bool True, if Elasticsearch HTTP service is listening Source code in dynamite_nsm/services/elasticsearch/profile.py def is_listening ( self ) -> bool : \"\"\" Check if Elasticsearch is listening Returns: True, if Elasticsearch HTTP service is listening \"\"\" if not self . elasticsearch_config : return False if not os . path . exists ( self . elasticsearch_config ): return False es_config_obj = elastic_configs . ConfigManager ( configuration_directory = self . elasticsearch_config ) host = es_config_obj . network_host port = es_config_obj . http_port return utilities . check_socket ( host , port )","title":"is_listening()"},{"location":"guides/developers/SDK/services/elasticsearch/profile/#dynamite_nsm.services.elasticsearch.profile.ProcessProfiler.is_running","text":"Check if Elasticsearch is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/elasticsearch/profile.py def is_running ( self ) -> bool : \"\"\"Check if Elasticsearch is running Returns: True, if running \"\"\" if self . elasticsearch_home : try : return elastic_process . ProcessManager () . status ()[ 'running' ] except KeyError : return elastic_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"is_running()"},{"location":"guides/developers/SDK/services/filebeat/config/","text":"Configuration wrappers for the main Filebeat configuration file. To import... from dynamite_nsm.services.filebeat import config as filebeat_config ConfigManager Manage main Filebeat Configuration __init__ ( self , install_directory , verbose = False , stdout = True ) special Configure Filebeat Parameters: Name Type Description Default install_directory str The path to the filebeat installation directory (E.G /opt/dynamite/filebeat) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Instance Variables input_logs - A misc.InputLogs instance representing the log paths to be monitored (if ECS is disabled) field_processors - A misc.FieldProcessors instance representing fields to be manipulated at parse time. index_template_settings - A misc.IndexTemplateSettings instance representing the Elasticsearch index template settings to be used elasticsearch_targets - A targets.ElasticsearchTargets instance used when sending logs directly to Elasticsearch. logstash_targets - A targets.LogstashTargets instance used when sending logs to Logstash. kafka_targets - A targets.KafkaTargets instance used when sending logs to Kafka. redis_targets - A targets.RedisTargets instance used when sending logs to Redis. Source code in dynamite_nsm/services/filebeat/config.py def __init__ ( self , install_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure Filebeat Args: install_directory: The path to the filebeat installation directory (E.G /opt/dynamite/filebeat) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables - `input_logs` - A `misc.InputLogs` instance representing the log paths to be monitored (if ECS is disabled) - `field_processors` - A `misc.FieldProcessors` instance representing fields to be manipulated at parse time. - `index_template_settings` - A `misc.IndexTemplateSettings` instance representing the Elasticsearch index template settings to be used - `elasticsearch_targets` - A `targets.ElasticsearchTargets` instance used when sending logs directly to Elasticsearch. - `logstash_targets` - A `targets.LogstashTargets` instance used when sending logs to Logstash. - `kafka_targets` - A `targets.KafkaTargets` instance used when sending logs to Kafka. - `redis_targets` - A `targets.RedisTargets` instance used when sending logs to Redis. \"\"\" extract_tokens = { '_inputs_raw' : ( 'filebeat.inputs' ,), '_elasticsearch_targets_raw' : ( 'output.elasticsearch' ,), '_logstash_targets_raw' : ( 'output.logstash' ,), '_kafka_targets_raw' : ( 'output.kafka' ,), '_redis_targets_raw' : ( 'output.redis' ,), '_index_template_settings_raw' : ( 'setup.template' ,), '_kibana_settings_raw' : ( 'setup.kibana' ,), '_processors_raw' : ( 'processors' ,) } self . _inputs_raw = {} self . _processors_raw = {} self . _index_template_settings_raw = {} self . _kibana_settings_raw = {} self . _elasticsearch_targets_raw = {} self . _logstash_targets_raw = {} self . _kafka_targets_raw = {} self . _redis_targets_raw = {} self . install_directory = install_directory self . filebeat_config_path = f ' { self . install_directory } /filebeat.yml' with open ( self . filebeat_config_path , 'r' ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'filebeat.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () try : agent_tag = self . _processors_raw [ 0 ] . get ( 'add_fields' , {}) . get ( 'fields' , {}) . get ( 'originating_agent_tag' ) except IndexError : agent_tag = None try : log_paths = self . _inputs_raw [ 0 ] . get ( 'paths' , []) except IndexError : log_paths = [] self . input_logs = InputLogs ( monitor_log_paths = log_paths ) self . field_processors = FieldProcessors ( originating_agent_tag = agent_tag ) self . index_template_settings = IndexTemplateSettings ( enabled = self . _index_template_settings_raw . get ( 'enabled' , False ), overwrite = self . _index_template_settings_raw . get ( 'overwrite' , False ), index_name = self . _index_template_settings_raw . get ( 'name' , 'filebeat' ), index_pattern = self . _index_template_settings_raw . get ( 'pattern' , 'filebeat-*' ), ) self . kibana_settings = KibanaSettings ( enabled = self . _kibana_settings_raw . get ( 'enabled' ), kibana_target_str = self . _kibana_settings_raw . get ( 'host' ), kibana_protocol = self . _kibana_settings_raw . get ( 'protocol' ) ) self . elasticsearch_targets = ElasticsearchTargets ( target_strings = self . _elasticsearch_targets_raw . get ( 'hosts' ), index = self . _elasticsearch_targets_raw . get ( 'index' ), ssl_certificate_authorities = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), username = self . _elasticsearch_targets_raw . get ( 'username' ), password = self . _elasticsearch_targets_raw . get ( 'password' ), ssl_enabled = self . _elasticsearch_targets_raw . get ( 'ssl' , False ), ssl_certificate = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _elasticsearch_targets_raw . get ( 'enabled' , False ) ) self . logstash_targets = LogstashTargets ( target_strings = self . _logstash_targets_raw . get ( 'hosts' ), index = self . _logstash_targets_raw . get ( 'index' ), load_balance = self . _logstash_targets_raw . get ( 'loadbalance' ), socks_5_proxy_url = self . _logstash_targets_raw . get ( 'proxy_url' ), pipelines = self . _logstash_targets_raw . get ( 'pipelining' ), max_batch_size = self . _logstash_targets_raw . get ( 'bulk_max_size' ), ssl_enabled = self . _logstash_targets_raw . get ( 'ssl' , False ), ssl_certificate_authorities = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), ssl_certificate = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _logstash_targets_raw . get ( 'enabled' , False ) ) self . kafka_targets = KafkaTargets ( target_strings = self . _kafka_targets_raw . get ( 'hosts' ), topic = self . _kafka_targets_raw . get ( 'topic' ), username = self . _kafka_targets_raw . get ( 'username' ), password = self . _kafka_targets_raw . get ( 'password' ), ssl_enabled = self . _kafka_targets_raw . get ( 'ssl' , False ), ssl_certificate_authorities = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), ssl_certificate = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _kafka_targets_raw . get ( 'enabled' , False ) ) self . redis_targets = RedisTargets ( target_strings = self . _redis_targets_raw . get ( 'hosts' ), index = self . _redis_targets_raw . get ( 'index' ), password = self . _redis_targets_raw . get ( 'password' ), load_balance = self . _redis_targets_raw . get ( 'loadbalance' '' ), db = self . _redis_targets_raw . get ( 'db' ), ssl_enabled = self . _redis_targets_raw . get ( 'ssl' , False ), ssl_certificate_authorities = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), ssl_certificate = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _redis_targets_raw . get ( 'enabled' , False ) ) commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] If specified, the first line of the configuration file will be set to the value of your choosing. None Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: If specified, the first line of the configuration file will be set to the value of your choosing. Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . install_directory } /filebeat.yml' self . _inputs_raw = self . input_logs . get_raw () self . _processors_raw = self . field_processors . get_raw () self . _index_template_settings_raw = self . index_template_settings . get_raw () self . _kibana_settings_raw = self . kibana_settings . get_raw () self . _elasticsearch_targets_raw = self . elasticsearch_targets . get_raw () self . _kafka_targets_raw = self . kafka_targets . get_raw () self . _logstash_targets_raw = self . logstash_targets . get_raw () self . _redis_targets_raw = self . redis_targets . get_raw () super ( ConfigManager , self ) . commit ( out_file_path , backup_directory ) disable_ecs_normalization ( self ) Disable ECS normalization for Zeek/Suricata logs Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def disable_ecs_normalization ( self ) -> None : \"\"\" Disable ECS normalization for Zeek/Suricata logs Returns: None \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) if not os . path . exists ( modules_path ): return if os . path . exists ( os . path . join ( modules_path , 'zeek.yml' )): os . rename ( os . path . join ( modules_path , 'zeek.yml' ), os . path . join ( modules_path , 'zeek.yml.disabled' )) if os . path . exists ( os . path . join ( modules_path , 'suricata.yml' )): os . rename ( os . path . join ( modules_path , 'suricata.yml' ), os . path . join ( modules_path , 'suricata.yml.disabled' )) self . input_logs . enabled = True enable_ecs_normalization ( self ) Enable ECS normalization for Zeek/Suricata logs Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def enable_ecs_normalization ( self ) -> None : \"\"\"Enable ECS normalization for Zeek/Suricata logs Returns: None \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) if not os . path . exists ( modules_path ): return if os . path . exists ( os . path . join ( modules_path , 'zeek.yml.disabled' )): os . rename ( os . path . join ( modules_path , 'zeek.yml.disabled' ), os . path . join ( modules_path , 'zeek.yml' )) if os . path . exists ( os . path . join ( modules_path , 'suricata.yml.disabled' )): os . rename ( os . path . join ( modules_path , 'suricata.yml.disabled' ), os . path . join ( modules_path , 'suricata.yml' )) self . input_logs . enabled = False from_raw_text ( raw_text , install_directory = None ) classmethod Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required install_directory Optional[str] The install directory for Filebeat None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/filebeat/config.py @classmethod def from_raw_text ( cls , raw_text : str , install_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file install_directory: The install directory for Filebeat Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp' tmp_config = f ' { tmp_dir } /filebeat.yml' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( install_directory = tmp_dir ) if install_directory : c . install_directory = install_directory return c is_ecs_normalization_available ( self ) Check if the applicable modules (zeek/suricata) have been patched to point to the correct log locations Returns: Type Description bool True, if ECS normalization is available (can be enabled) Source code in dynamite_nsm/services/filebeat/config.py def is_ecs_normalization_available ( self ) -> bool : \"\"\"Check if the applicable modules (zeek/suricata) have been patched to point to the correct log locations Returns: True, if ECS normalization is available (can be enabled) \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) return os . path . exists ( os . path . join ( modules_path , '.patched' )) is_ecs_normalization_enabled ( self ) Check if ECS normalization is enabled over generic inputs Returns: Type Description bool True, if ECS normalization is enabled. Source code in dynamite_nsm/services/filebeat/config.py def is_ecs_normalization_enabled ( self ) -> bool : \"\"\"Check if ECS normalization is enabled over generic inputs Returns: True, if ECS normalization is enabled. \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) zeek_module_exists = os . path . exists ( os . path . join ( modules_path , 'zeek.yml' )) suricata_module_exists = os . path . exists ( os . path . join ( modules_path , 'suricata.yml' )) return zeek_module_exists and suricata_module_exists patch_modules ( self , zeek_log_directory , suricata_log_directory ) and patch the directory paths to point to the Dynamite configured paths Parameters: Name Type Description Default zeek_log_directory str The path to the Zeek current log directory required suricata_log_directory str The path to the Suricata log directory required Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def patch_modules ( self , zeek_log_directory : str , suricata_log_directory : str ) -> None : \"\"\"and patch the directory paths to point to the Dynamite configured paths Args: zeek_log_directory: The path to the Zeek current log directory suricata_log_directory: The path to the Suricata log directory Returns: None \"\"\" def write_module ( path , data ): self . logger . debug ( f 'Writing module { path } ' ) with open ( path , 'w' ) as module_yaml : dump ( data , module_yaml , default_flow_style = False ) suricata_module_path = None zeek_module_path = None suricata_module_data = None zeek_module_data = None modules_path = os . path . join ( self . install_directory , 'modules.d' ) self . logger . debug ( f 'Located modules at { modules_path } ' ) if not os . path . exists ( modules_path ): return for module in os . listdir ( modules_path ): if not ( module . endswith ( '.yml' ) or module . endswith ( '.yaml' ) or module . endswith ( '.disabled' )): continue if 'zeek' in module : zeek_module_path = os . path . join ( modules_path , module ) self . logger . debug ( f 'Setting { module } path -> { zeek_module_path } ' ) elif 'suricata' in module : suricata_module_path = os . path . join ( modules_path , module ) self . logger . debug ( f 'Setting { module } path -> { suricata_module_path } ' ) if zeek_module_path : self . logger . debug ( f 'Located Filebeat module { zeek_module_path } ' ) with open ( zeek_module_path , 'r' ) as zeek_module_yaml : zeek_module_data = load ( zeek_module_yaml , Loader = Loader ) for k , v in zeek_module_data [ 0 ] . items (): if isinstance ( v , dict ): if k == 'connection' : k = 'conn' zeek_full_path = os . path . join ( zeek_log_directory , k + '.log' ) v [ 'var.paths' ] = [ zeek_full_path ] self . logger . debug ( f 'Patching path { k } -> { v } ' ) if suricata_module_path : self . logger . debug ( f 'Located Filebeat module { zeek_module_path } ' ) with open ( suricata_module_path , 'r' ) as suricata_module_yaml : suricata_module_data = load ( suricata_module_yaml , Loader = Loader ) for k , v in suricata_module_data [ 0 ] . items (): suricata_full_path = os . path . join ( suricata_log_directory , k + '.json' ) if isinstance ( v , dict ): v [ 'var.paths' ] = [ suricata_full_path ] self . logger . debug ( f 'Patching path { k } -> { v } ' ) patch_file = open ( os . path . join ( modules_path , '.patched' ), 'w' ) if zeek_module_data : write_module ( zeek_module_path , zeek_module_data ) patch_file . write ( str ( datetime . utcnow ()) + ' \\n ' ) if suricata_module_data : write_module ( suricata_module_path , suricata_module_data ) patch_file . write ( str ( datetime . utcnow ()) + ' \\n ' ) patch_file . close () reset ( self , out_file_path = None , default_config_path = None ) Reset a configuration file back to its default Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/filebeat/config.py def reset ( self , out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . install_directory } /filebeat.yml' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /filebeat/filebeat.yml' super ( ConfigManager , self ) . reset ( out_file_path , default_config_path ) self . commit ( out_file_path = out_file_path ) switch_to_elasticsearch_target ( self ) Convenience method that enables ElasticSearch, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_elasticsearch_target ( self ) -> None : \"\"\"Convenience method that enables ElasticSearch, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = True self . kafka_targets . enabled = False self . logstash_targets . enabled = False self . redis_targets . enabled = False switch_to_kafka_target ( self ) Convenience method that enables Kafka, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_kafka_target ( self ) -> None : \"\"\"Convenience method that enables Kafka, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = False self . kafka_targets . enabled = True self . logstash_targets . enabled = False self . redis_targets . enabled = False switch_to_logstash_target ( self ) Convenience method that enables Logstash, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_logstash_target ( self ) -> None : \"\"\"Convenience method that enables Logstash, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = False self . kafka_targets . enabled = False self . logstash_targets . enabled = True self . redis_targets . enabled = False switch_to_redis_target ( self ) Convenience method that enables Redis, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_redis_target ( self ) -> None : \"\"\"Convenience method that enables Redis, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = False self . kafka_targets . enabled = False self . logstash_targets . enabled = False self . redis_targets . enabled = True InvalidAgentTag Thrown when Filebeat agent tag is invalid","title":"config"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager","text":"Manage main Filebeat Configuration","title":"ConfigManager"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.__init__","text":"Configure Filebeat Parameters: Name Type Description Default install_directory str The path to the filebeat installation directory (E.G /opt/dynamite/filebeat) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.__init__--instance-variables","text":"input_logs - A misc.InputLogs instance representing the log paths to be monitored (if ECS is disabled) field_processors - A misc.FieldProcessors instance representing fields to be manipulated at parse time. index_template_settings - A misc.IndexTemplateSettings instance representing the Elasticsearch index template settings to be used elasticsearch_targets - A targets.ElasticsearchTargets instance used when sending logs directly to Elasticsearch. logstash_targets - A targets.LogstashTargets instance used when sending logs to Logstash. kafka_targets - A targets.KafkaTargets instance used when sending logs to Kafka. redis_targets - A targets.RedisTargets instance used when sending logs to Redis. Source code in dynamite_nsm/services/filebeat/config.py def __init__ ( self , install_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure Filebeat Args: install_directory: The path to the filebeat installation directory (E.G /opt/dynamite/filebeat) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables - `input_logs` - A `misc.InputLogs` instance representing the log paths to be monitored (if ECS is disabled) - `field_processors` - A `misc.FieldProcessors` instance representing fields to be manipulated at parse time. - `index_template_settings` - A `misc.IndexTemplateSettings` instance representing the Elasticsearch index template settings to be used - `elasticsearch_targets` - A `targets.ElasticsearchTargets` instance used when sending logs directly to Elasticsearch. - `logstash_targets` - A `targets.LogstashTargets` instance used when sending logs to Logstash. - `kafka_targets` - A `targets.KafkaTargets` instance used when sending logs to Kafka. - `redis_targets` - A `targets.RedisTargets` instance used when sending logs to Redis. \"\"\" extract_tokens = { '_inputs_raw' : ( 'filebeat.inputs' ,), '_elasticsearch_targets_raw' : ( 'output.elasticsearch' ,), '_logstash_targets_raw' : ( 'output.logstash' ,), '_kafka_targets_raw' : ( 'output.kafka' ,), '_redis_targets_raw' : ( 'output.redis' ,), '_index_template_settings_raw' : ( 'setup.template' ,), '_kibana_settings_raw' : ( 'setup.kibana' ,), '_processors_raw' : ( 'processors' ,) } self . _inputs_raw = {} self . _processors_raw = {} self . _index_template_settings_raw = {} self . _kibana_settings_raw = {} self . _elasticsearch_targets_raw = {} self . _logstash_targets_raw = {} self . _kafka_targets_raw = {} self . _redis_targets_raw = {} self . install_directory = install_directory self . filebeat_config_path = f ' { self . install_directory } /filebeat.yml' with open ( self . filebeat_config_path , 'r' ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'filebeat.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () try : agent_tag = self . _processors_raw [ 0 ] . get ( 'add_fields' , {}) . get ( 'fields' , {}) . get ( 'originating_agent_tag' ) except IndexError : agent_tag = None try : log_paths = self . _inputs_raw [ 0 ] . get ( 'paths' , []) except IndexError : log_paths = [] self . input_logs = InputLogs ( monitor_log_paths = log_paths ) self . field_processors = FieldProcessors ( originating_agent_tag = agent_tag ) self . index_template_settings = IndexTemplateSettings ( enabled = self . _index_template_settings_raw . get ( 'enabled' , False ), overwrite = self . _index_template_settings_raw . get ( 'overwrite' , False ), index_name = self . _index_template_settings_raw . get ( 'name' , 'filebeat' ), index_pattern = self . _index_template_settings_raw . get ( 'pattern' , 'filebeat-*' ), ) self . kibana_settings = KibanaSettings ( enabled = self . _kibana_settings_raw . get ( 'enabled' ), kibana_target_str = self . _kibana_settings_raw . get ( 'host' ), kibana_protocol = self . _kibana_settings_raw . get ( 'protocol' ) ) self . elasticsearch_targets = ElasticsearchTargets ( target_strings = self . _elasticsearch_targets_raw . get ( 'hosts' ), index = self . _elasticsearch_targets_raw . get ( 'index' ), ssl_certificate_authorities = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), username = self . _elasticsearch_targets_raw . get ( 'username' ), password = self . _elasticsearch_targets_raw . get ( 'password' ), ssl_enabled = self . _elasticsearch_targets_raw . get ( 'ssl' , False ), ssl_certificate = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _elasticsearch_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _elasticsearch_targets_raw . get ( 'enabled' , False ) ) self . logstash_targets = LogstashTargets ( target_strings = self . _logstash_targets_raw . get ( 'hosts' ), index = self . _logstash_targets_raw . get ( 'index' ), load_balance = self . _logstash_targets_raw . get ( 'loadbalance' ), socks_5_proxy_url = self . _logstash_targets_raw . get ( 'proxy_url' ), pipelines = self . _logstash_targets_raw . get ( 'pipelining' ), max_batch_size = self . _logstash_targets_raw . get ( 'bulk_max_size' ), ssl_enabled = self . _logstash_targets_raw . get ( 'ssl' , False ), ssl_certificate_authorities = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), ssl_certificate = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _logstash_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _logstash_targets_raw . get ( 'enabled' , False ) ) self . kafka_targets = KafkaTargets ( target_strings = self . _kafka_targets_raw . get ( 'hosts' ), topic = self . _kafka_targets_raw . get ( 'topic' ), username = self . _kafka_targets_raw . get ( 'username' ), password = self . _kafka_targets_raw . get ( 'password' ), ssl_enabled = self . _kafka_targets_raw . get ( 'ssl' , False ), ssl_certificate_authorities = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), ssl_certificate = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _kafka_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _kafka_targets_raw . get ( 'enabled' , False ) ) self . redis_targets = RedisTargets ( target_strings = self . _redis_targets_raw . get ( 'hosts' ), index = self . _redis_targets_raw . get ( 'index' ), password = self . _redis_targets_raw . get ( 'password' ), load_balance = self . _redis_targets_raw . get ( 'loadbalance' '' ), db = self . _redis_targets_raw . get ( 'db' ), ssl_enabled = self . _redis_targets_raw . get ( 'ssl' , False ), ssl_certificate_authorities = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'certificate_authorities' ), ssl_certificate = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'certificate' ), ssl_key = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'key' ), ssl_verification_mode = self . _redis_targets_raw . get ( 'ssl' , {}) . get ( 'verification_mode' ), enabled = self . _redis_targets_raw . get ( 'enabled' , False ) )","title":"Instance Variables"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] If specified, the first line of the configuration file will be set to the value of your choosing. None Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: If specified, the first line of the configuration file will be set to the value of your choosing. Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . install_directory } /filebeat.yml' self . _inputs_raw = self . input_logs . get_raw () self . _processors_raw = self . field_processors . get_raw () self . _index_template_settings_raw = self . index_template_settings . get_raw () self . _kibana_settings_raw = self . kibana_settings . get_raw () self . _elasticsearch_targets_raw = self . elasticsearch_targets . get_raw () self . _kafka_targets_raw = self . kafka_targets . get_raw () self . _logstash_targets_raw = self . logstash_targets . get_raw () self . _redis_targets_raw = self . redis_targets . get_raw () super ( ConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.disable_ecs_normalization","text":"Disable ECS normalization for Zeek/Suricata logs Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def disable_ecs_normalization ( self ) -> None : \"\"\" Disable ECS normalization for Zeek/Suricata logs Returns: None \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) if not os . path . exists ( modules_path ): return if os . path . exists ( os . path . join ( modules_path , 'zeek.yml' )): os . rename ( os . path . join ( modules_path , 'zeek.yml' ), os . path . join ( modules_path , 'zeek.yml.disabled' )) if os . path . exists ( os . path . join ( modules_path , 'suricata.yml' )): os . rename ( os . path . join ( modules_path , 'suricata.yml' ), os . path . join ( modules_path , 'suricata.yml.disabled' )) self . input_logs . enabled = True","title":"disable_ecs_normalization()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.enable_ecs_normalization","text":"Enable ECS normalization for Zeek/Suricata logs Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def enable_ecs_normalization ( self ) -> None : \"\"\"Enable ECS normalization for Zeek/Suricata logs Returns: None \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) if not os . path . exists ( modules_path ): return if os . path . exists ( os . path . join ( modules_path , 'zeek.yml.disabled' )): os . rename ( os . path . join ( modules_path , 'zeek.yml.disabled' ), os . path . join ( modules_path , 'zeek.yml' )) if os . path . exists ( os . path . join ( modules_path , 'suricata.yml.disabled' )): os . rename ( os . path . join ( modules_path , 'suricata.yml.disabled' ), os . path . join ( modules_path , 'suricata.yml' )) self . input_logs . enabled = False","title":"enable_ecs_normalization()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.from_raw_text","text":"Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required install_directory Optional[str] The install directory for Filebeat None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/filebeat/config.py @classmethod def from_raw_text ( cls , raw_text : str , install_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file install_directory: The install directory for Filebeat Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp' tmp_config = f ' { tmp_dir } /filebeat.yml' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( install_directory = tmp_dir ) if install_directory : c . install_directory = install_directory return c","title":"from_raw_text()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.is_ecs_normalization_available","text":"Check if the applicable modules (zeek/suricata) have been patched to point to the correct log locations Returns: Type Description bool True, if ECS normalization is available (can be enabled) Source code in dynamite_nsm/services/filebeat/config.py def is_ecs_normalization_available ( self ) -> bool : \"\"\"Check if the applicable modules (zeek/suricata) have been patched to point to the correct log locations Returns: True, if ECS normalization is available (can be enabled) \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) return os . path . exists ( os . path . join ( modules_path , '.patched' ))","title":"is_ecs_normalization_available()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.is_ecs_normalization_enabled","text":"Check if ECS normalization is enabled over generic inputs Returns: Type Description bool True, if ECS normalization is enabled. Source code in dynamite_nsm/services/filebeat/config.py def is_ecs_normalization_enabled ( self ) -> bool : \"\"\"Check if ECS normalization is enabled over generic inputs Returns: True, if ECS normalization is enabled. \"\"\" modules_path = os . path . join ( self . install_directory , 'modules.d' ) zeek_module_exists = os . path . exists ( os . path . join ( modules_path , 'zeek.yml' )) suricata_module_exists = os . path . exists ( os . path . join ( modules_path , 'suricata.yml' )) return zeek_module_exists and suricata_module_exists","title":"is_ecs_normalization_enabled()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.patch_modules","text":"and patch the directory paths to point to the Dynamite configured paths Parameters: Name Type Description Default zeek_log_directory str The path to the Zeek current log directory required suricata_log_directory str The path to the Suricata log directory required Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def patch_modules ( self , zeek_log_directory : str , suricata_log_directory : str ) -> None : \"\"\"and patch the directory paths to point to the Dynamite configured paths Args: zeek_log_directory: The path to the Zeek current log directory suricata_log_directory: The path to the Suricata log directory Returns: None \"\"\" def write_module ( path , data ): self . logger . debug ( f 'Writing module { path } ' ) with open ( path , 'w' ) as module_yaml : dump ( data , module_yaml , default_flow_style = False ) suricata_module_path = None zeek_module_path = None suricata_module_data = None zeek_module_data = None modules_path = os . path . join ( self . install_directory , 'modules.d' ) self . logger . debug ( f 'Located modules at { modules_path } ' ) if not os . path . exists ( modules_path ): return for module in os . listdir ( modules_path ): if not ( module . endswith ( '.yml' ) or module . endswith ( '.yaml' ) or module . endswith ( '.disabled' )): continue if 'zeek' in module : zeek_module_path = os . path . join ( modules_path , module ) self . logger . debug ( f 'Setting { module } path -> { zeek_module_path } ' ) elif 'suricata' in module : suricata_module_path = os . path . join ( modules_path , module ) self . logger . debug ( f 'Setting { module } path -> { suricata_module_path } ' ) if zeek_module_path : self . logger . debug ( f 'Located Filebeat module { zeek_module_path } ' ) with open ( zeek_module_path , 'r' ) as zeek_module_yaml : zeek_module_data = load ( zeek_module_yaml , Loader = Loader ) for k , v in zeek_module_data [ 0 ] . items (): if isinstance ( v , dict ): if k == 'connection' : k = 'conn' zeek_full_path = os . path . join ( zeek_log_directory , k + '.log' ) v [ 'var.paths' ] = [ zeek_full_path ] self . logger . debug ( f 'Patching path { k } -> { v } ' ) if suricata_module_path : self . logger . debug ( f 'Located Filebeat module { zeek_module_path } ' ) with open ( suricata_module_path , 'r' ) as suricata_module_yaml : suricata_module_data = load ( suricata_module_yaml , Loader = Loader ) for k , v in suricata_module_data [ 0 ] . items (): suricata_full_path = os . path . join ( suricata_log_directory , k + '.json' ) if isinstance ( v , dict ): v [ 'var.paths' ] = [ suricata_full_path ] self . logger . debug ( f 'Patching path { k } -> { v } ' ) patch_file = open ( os . path . join ( modules_path , '.patched' ), 'w' ) if zeek_module_data : write_module ( zeek_module_path , zeek_module_data ) patch_file . write ( str ( datetime . utcnow ()) + ' \\n ' ) if suricata_module_data : write_module ( suricata_module_path , suricata_module_data ) patch_file . write ( str ( datetime . utcnow ()) + ' \\n ' ) patch_file . close ()","title":"patch_modules()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.reset","text":"Reset a configuration file back to its default Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/filebeat/config.py def reset ( self , out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . install_directory } /filebeat.yml' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /filebeat/filebeat.yml' super ( ConfigManager , self ) . reset ( out_file_path , default_config_path ) self . commit ( out_file_path = out_file_path )","title":"reset()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.switch_to_elasticsearch_target","text":"Convenience method that enables ElasticSearch, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_elasticsearch_target ( self ) -> None : \"\"\"Convenience method that enables ElasticSearch, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = True self . kafka_targets . enabled = False self . logstash_targets . enabled = False self . redis_targets . enabled = False","title":"switch_to_elasticsearch_target()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.switch_to_kafka_target","text":"Convenience method that enables Kafka, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_kafka_target ( self ) -> None : \"\"\"Convenience method that enables Kafka, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = False self . kafka_targets . enabled = True self . logstash_targets . enabled = False self . redis_targets . enabled = False","title":"switch_to_kafka_target()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.switch_to_logstash_target","text":"Convenience method that enables Logstash, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_logstash_target ( self ) -> None : \"\"\"Convenience method that enables Logstash, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = False self . kafka_targets . enabled = False self . logstash_targets . enabled = True self . redis_targets . enabled = False","title":"switch_to_logstash_target()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.ConfigManager.switch_to_redis_target","text":"Convenience method that enables Redis, and disables all other targets Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/config.py def switch_to_redis_target ( self ) -> None : \"\"\"Convenience method that enables Redis, and disables all other targets Returns: None \"\"\" self . elasticsearch_targets . enabled = False self . kafka_targets . enabled = False self . logstash_targets . enabled = False self . redis_targets . enabled = True","title":"switch_to_redis_target()"},{"location":"guides/developers/SDK/services/filebeat/config/#dynamite_nsm.services.filebeat.config.InvalidAgentTag","text":"Thrown when Filebeat agent tag is invalid","title":"InvalidAgentTag"},{"location":"guides/developers/SDK/services/filebeat/install/","text":"Installation Manager for Filebeat and its dependencies. To import... from dynamite_nsm.services.filebeat import install as filebeat_install InstallManager Manage Filebeat installation process __init__ ( self , install_directory , download_filebeat_archive = True , stdout = False , verbose = False ) special Install Filebeat Parameters: Name Type Description Default install_directory str The installation directory (E.G /opt/dynamite/filebeat/) required download_filebeat_archive Optional[bool] If True, download the Filebeat archive from a mirror True stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/filebeat/install.py def __init__ ( self , install_directory : str , download_filebeat_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Filebeat Args: install_directory: The installation directory (E.G /opt/dynamite/filebeat/) download_filebeat_archive: If True, download the Filebeat archive from a mirror stdout: Print the output to console verbose: Include detailed debug messages \"\"\" self . install_directory = install_directory self . download_filebeat_archive = download_filebeat_archive self . stdout = stdout self . verbose = verbose super () . __init__ ( 'filebeat.install' , verbose , stdout ) if download_filebeat_archive : self . logger . info ( 'Attempting to download Filebeat OSS archive.' ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . FILE_BEAT_MIRRORS ) self . logger . info ( f 'Attempting to extract Filebeat archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . FILE_BEAT_MIRRORS ) create_update_filebeat_environment_variables ( self ) Creates all the required Filebeat environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/install.py def create_update_filebeat_environment_variables ( self ) -> None : \"\"\"Creates all the required Filebeat environmental variables Returns: None \"\"\" self . create_update_env_variable ( 'FILEBEAT_HOME' , self . install_directory ) setup ( self , targets , target_type = 'elasticsearch' , monitor_log_paths = None , agent_tag = None ) Setup Filebeat Parameters: Name Type Description Default targets List[str] A list of Elasticsearch/Kafka/Logstash targets to forward events to (E.G [\"192.168.0.9 5044\", ...]) required target_type Optional[str] The target type; current supported: elasticsearch (default), logstash, kafka, redis 'elasticsearch' monitor_log_paths Optional[List[str]] A tuple of log paths to monitor None agent_tag Optional[str] A friendly name for the agent (defaults to the hostname with no spaces and _agt suffix) None Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/install.py def setup ( self , targets : List [ str ], target_type : Optional [ str ] = 'elasticsearch' , monitor_log_paths : Optional [ List [ str ]] = None , agent_tag : Optional [ str ] = None ) -> None : \"\"\"Setup Filebeat Args: targets: A list of Elasticsearch/Kafka/Logstash targets to forward events to (E.G [\"192.168.0.9 5044\", ...]) target_type: The target type; current supported: elasticsearch (default), logstash, kafka, redis monitor_log_paths: A tuple of log paths to monitor agent_tag: A friendly name for the agent (defaults to the hostname with no spaces and _agt suffix) Returns: None \"\"\" from dynamite_nsm.services.zeek import profile as zeek_profile from dynamite_nsm.services.suricata import profile as suricata_profile sysctl = systemctl . SystemCtl () zeek_log_root , suricata_log_root = None , None # Directory setup self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) utilities . makedirs ( f ' { self . install_directory } /logs' ) self . logger . info ( 'Installing files and directories.' ) self . copy_filebeat_files_and_directories () self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /filebeat/filebeat.yml' , self . install_directory ) # Overwrite with dynamite default configurations self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /filebeat/module/' , self . install_directory ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /filebeat/modules.d/' , self . install_directory ) filebeat_config = config . ConfigManager ( self . install_directory , verbose = self . verbose , stdout = self . stdout ) if target_type == 'elasticsearch' : filebeat_config . switch_to_elasticsearch_target () filebeat_config . elasticsearch_targets . target_strings = targets self . logger . info ( f 'Enabling Elasticsearch connector: ' f ' { filebeat_config . elasticsearch_targets . target_strings } ' ) elif target_type == 'logstash' : filebeat_config . switch_to_logstash_target () filebeat_config . logstash_targets . target_strings = targets elif target_type == 'kafka' : filebeat_config . switch_to_kafka_target () filebeat_config . kafka_targets . target_strings = targets elif target_type == 'redis' : filebeat_config . switch_to_redis_target () filebeat_config . redis_targets . target_strings = targets filebeat_config . input_logs = misc_filebeat_objs . InputLogs ( monitor_log_paths = [] ) filebeat_config . field_processors . originating_agent_tag = agent_tag if not monitor_log_paths : environ = utilities . get_environment_file_dict () zeek_log_root = f ' { environ . get ( \"ZEEK_HOME\" , \"\" ) } /logs/current/' suricata_log_root = environ . get ( 'SURICATA_LOGS' , '' ) zeek_profiler = zeek_profile . ProcessProfiler () suricata_profiler = suricata_profile . ProcessProfiler () if zeek_profiler . is_installed (): self . logger . info ( f 'Zeek installation found; monitoring: { zeek_log_root } *.log' ) filebeat_config . input_logs . monitor_log_paths . append ( f ' { zeek_log_root } *.log' ) if suricata_profiler . is_installed (): self . logger . info ( f 'Suricata installation found; monitoring: { suricata_log_root } /eve.json' ) filebeat_config . input_logs . monitor_log_paths . append ( f ' { suricata_log_root } /eve.json' ) else : filebeat_config . input_logs = misc_filebeat_objs . InputLogs ( monitor_log_paths = monitor_log_paths ) self . logger . info ( f 'Monitoring Paths = { filebeat_config . input_logs . monitor_log_paths } ' ) if not agent_tag : filebeat_config . field_processors . originating_agent_tag = utilities . get_default_agent_tag () self . logger . info ( f 'Agent Tag = { filebeat_config . field_processors . originating_agent_tag } ' ) self . logger . debug ( filebeat_config . elasticsearch_targets . get_raw ()) filebeat_config . commit () self . logger . info ( 'Applying configuration.' ) # Fix Permissions self . logger . info ( 'Installing modules.' ) filebeat_config . patch_modules ( zeek_log_directory = zeek_log_root , suricata_log_directory = suricata_log_root ) # Setting up permissions self . logger . info ( 'Setting up file permissions.' ) config_file = f ' { self . install_directory } /filebeat.yml' utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . install_directory } /modules.d/' , unix_permissions_integer = 'go-w' ) utilities . set_permissions_of_file ( f ' { self . install_directory } /module/' , unix_permissions_integer = 'go-w' ) utilities . set_ownership_of_file ( config_file , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( config_file , unix_permissions_integer = 660 ) filebeat_config . enable_ecs_normalization () # Install and enable service self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/filebeat.service' ) sysctl . install_and_enable ( f ' { const . DEFAULT_CONFIGS } /systemd/filebeat.service' ) # Update environment file self . create_update_filebeat_environment_variables () validate_targets ( targets , stdout = True , verbose = False ) staticmethod Ensures that targets are entered in a valid format (E.G [\"192.168.0.1:5044\", \"myhost2:5044\"]) Parameters: Name Type Description Default targets A list of IP/host port pair required stdout Print the output to console True verbose Include detailed debug messages False Returns: Type Description True if valid Source code in dynamite_nsm/services/filebeat/install.py @staticmethod def validate_targets ( targets , stdout = True , verbose = False ): \"\"\"Ensures that targets are entered in a valid format (E.G [\"192.168.0.1:5044\", \"myhost2:5044\"]) Args: targets: A list of IP/host port pair stdout: Print the output to console verbose: Include detailed debug messages Returns: True if valid \"\"\" log_level = logging . INFO if verbose : log_level = logging . DEBUG logger = get_logger ( 'FILEBEAT' , level = log_level , stdout = stdout ) if isinstance ( targets , list ) or isinstance ( targets , tuple ): protocol_tokens = [ 'http://' , 'https://' , 'plain://' , 'sasl://' , 'redis://' ] for i , target in enumerate ( targets ): target = str ( target ) . lower () for token in protocol_tokens : target = target . replace ( token , '' ) try : host , port = target . split ( ':' ) if not str ( port ) . isdigit (): logger . warning ( 'Target Invalid: {} port must be numeric at position {} ' . format ( target , i )) return False except ValueError : logger . warning ( 'Target Invalid: {} expected host:port at position {} ' . format ( target , i )) return False else : logger . warning ( 'Target Invalid: {} ; must be a enumerable (list, tuple)' . format ( targets )) return False return True UninstallManager Manage Filebeat uninstall process __init__ ( self , stdout = False , verbose = False ) special Uninstall Filebeat Parameters: Name Type Description Default stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/filebeat/install.py def __init__ ( self , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Filebeat Args: stdout: Print output to console verbose: Include detailed debug messages \"\"\" from dynamite_nsm.services.filebeat.process import ProcessManager env_vars = utilities . get_environment_file_dict () fb_directories = [ env_vars . get ( 'FILEBEAT_HOME' ), ] super () . __init__ ( 'filebeat.uninstall' , directories = fb_directories , process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'filebeat.service' , environ_vars = [ 'FILEBEAT_HOME' ], stdout = stdout , verbose = verbose )","title":"install"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.InstallManager","text":"Manage Filebeat installation process","title":"InstallManager"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.InstallManager.__init__","text":"Install Filebeat Parameters: Name Type Description Default install_directory str The installation directory (E.G /opt/dynamite/filebeat/) required download_filebeat_archive Optional[bool] If True, download the Filebeat archive from a mirror True stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/filebeat/install.py def __init__ ( self , install_directory : str , download_filebeat_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Filebeat Args: install_directory: The installation directory (E.G /opt/dynamite/filebeat/) download_filebeat_archive: If True, download the Filebeat archive from a mirror stdout: Print the output to console verbose: Include detailed debug messages \"\"\" self . install_directory = install_directory self . download_filebeat_archive = download_filebeat_archive self . stdout = stdout self . verbose = verbose super () . __init__ ( 'filebeat.install' , verbose , stdout ) if download_filebeat_archive : self . logger . info ( 'Attempting to download Filebeat OSS archive.' ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . FILE_BEAT_MIRRORS ) self . logger . info ( f 'Attempting to extract Filebeat archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . FILE_BEAT_MIRRORS )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.InstallManager.create_update_filebeat_environment_variables","text":"Creates all the required Filebeat environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/install.py def create_update_filebeat_environment_variables ( self ) -> None : \"\"\"Creates all the required Filebeat environmental variables Returns: None \"\"\" self . create_update_env_variable ( 'FILEBEAT_HOME' , self . install_directory )","title":"create_update_filebeat_environment_variables()"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.InstallManager.setup","text":"Setup Filebeat Parameters: Name Type Description Default targets List[str] A list of Elasticsearch/Kafka/Logstash targets to forward events to (E.G [\"192.168.0.9 5044\", ...]) required target_type Optional[str] The target type; current supported: elasticsearch (default), logstash, kafka, redis 'elasticsearch' monitor_log_paths Optional[List[str]] A tuple of log paths to monitor None agent_tag Optional[str] A friendly name for the agent (defaults to the hostname with no spaces and _agt suffix) None Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/install.py def setup ( self , targets : List [ str ], target_type : Optional [ str ] = 'elasticsearch' , monitor_log_paths : Optional [ List [ str ]] = None , agent_tag : Optional [ str ] = None ) -> None : \"\"\"Setup Filebeat Args: targets: A list of Elasticsearch/Kafka/Logstash targets to forward events to (E.G [\"192.168.0.9 5044\", ...]) target_type: The target type; current supported: elasticsearch (default), logstash, kafka, redis monitor_log_paths: A tuple of log paths to monitor agent_tag: A friendly name for the agent (defaults to the hostname with no spaces and _agt suffix) Returns: None \"\"\" from dynamite_nsm.services.zeek import profile as zeek_profile from dynamite_nsm.services.suricata import profile as suricata_profile sysctl = systemctl . SystemCtl () zeek_log_root , suricata_log_root = None , None # Directory setup self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) utilities . makedirs ( f ' { self . install_directory } /logs' ) self . logger . info ( 'Installing files and directories.' ) self . copy_filebeat_files_and_directories () self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /filebeat/filebeat.yml' , self . install_directory ) # Overwrite with dynamite default configurations self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /filebeat/module/' , self . install_directory ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /filebeat/modules.d/' , self . install_directory ) filebeat_config = config . ConfigManager ( self . install_directory , verbose = self . verbose , stdout = self . stdout ) if target_type == 'elasticsearch' : filebeat_config . switch_to_elasticsearch_target () filebeat_config . elasticsearch_targets . target_strings = targets self . logger . info ( f 'Enabling Elasticsearch connector: ' f ' { filebeat_config . elasticsearch_targets . target_strings } ' ) elif target_type == 'logstash' : filebeat_config . switch_to_logstash_target () filebeat_config . logstash_targets . target_strings = targets elif target_type == 'kafka' : filebeat_config . switch_to_kafka_target () filebeat_config . kafka_targets . target_strings = targets elif target_type == 'redis' : filebeat_config . switch_to_redis_target () filebeat_config . redis_targets . target_strings = targets filebeat_config . input_logs = misc_filebeat_objs . InputLogs ( monitor_log_paths = [] ) filebeat_config . field_processors . originating_agent_tag = agent_tag if not monitor_log_paths : environ = utilities . get_environment_file_dict () zeek_log_root = f ' { environ . get ( \"ZEEK_HOME\" , \"\" ) } /logs/current/' suricata_log_root = environ . get ( 'SURICATA_LOGS' , '' ) zeek_profiler = zeek_profile . ProcessProfiler () suricata_profiler = suricata_profile . ProcessProfiler () if zeek_profiler . is_installed (): self . logger . info ( f 'Zeek installation found; monitoring: { zeek_log_root } *.log' ) filebeat_config . input_logs . monitor_log_paths . append ( f ' { zeek_log_root } *.log' ) if suricata_profiler . is_installed (): self . logger . info ( f 'Suricata installation found; monitoring: { suricata_log_root } /eve.json' ) filebeat_config . input_logs . monitor_log_paths . append ( f ' { suricata_log_root } /eve.json' ) else : filebeat_config . input_logs = misc_filebeat_objs . InputLogs ( monitor_log_paths = monitor_log_paths ) self . logger . info ( f 'Monitoring Paths = { filebeat_config . input_logs . monitor_log_paths } ' ) if not agent_tag : filebeat_config . field_processors . originating_agent_tag = utilities . get_default_agent_tag () self . logger . info ( f 'Agent Tag = { filebeat_config . field_processors . originating_agent_tag } ' ) self . logger . debug ( filebeat_config . elasticsearch_targets . get_raw ()) filebeat_config . commit () self . logger . info ( 'Applying configuration.' ) # Fix Permissions self . logger . info ( 'Installing modules.' ) filebeat_config . patch_modules ( zeek_log_directory = zeek_log_root , suricata_log_directory = suricata_log_root ) # Setting up permissions self . logger . info ( 'Setting up file permissions.' ) config_file = f ' { self . install_directory } /filebeat.yml' utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . install_directory } /modules.d/' , unix_permissions_integer = 'go-w' ) utilities . set_permissions_of_file ( f ' { self . install_directory } /module/' , unix_permissions_integer = 'go-w' ) utilities . set_ownership_of_file ( config_file , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( config_file , unix_permissions_integer = 660 ) filebeat_config . enable_ecs_normalization () # Install and enable service self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/filebeat.service' ) sysctl . install_and_enable ( f ' { const . DEFAULT_CONFIGS } /systemd/filebeat.service' ) # Update environment file self . create_update_filebeat_environment_variables ()","title":"setup()"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.InstallManager.validate_targets","text":"Ensures that targets are entered in a valid format (E.G [\"192.168.0.1:5044\", \"myhost2:5044\"]) Parameters: Name Type Description Default targets A list of IP/host port pair required stdout Print the output to console True verbose Include detailed debug messages False Returns: Type Description True if valid Source code in dynamite_nsm/services/filebeat/install.py @staticmethod def validate_targets ( targets , stdout = True , verbose = False ): \"\"\"Ensures that targets are entered in a valid format (E.G [\"192.168.0.1:5044\", \"myhost2:5044\"]) Args: targets: A list of IP/host port pair stdout: Print the output to console verbose: Include detailed debug messages Returns: True if valid \"\"\" log_level = logging . INFO if verbose : log_level = logging . DEBUG logger = get_logger ( 'FILEBEAT' , level = log_level , stdout = stdout ) if isinstance ( targets , list ) or isinstance ( targets , tuple ): protocol_tokens = [ 'http://' , 'https://' , 'plain://' , 'sasl://' , 'redis://' ] for i , target in enumerate ( targets ): target = str ( target ) . lower () for token in protocol_tokens : target = target . replace ( token , '' ) try : host , port = target . split ( ':' ) if not str ( port ) . isdigit (): logger . warning ( 'Target Invalid: {} port must be numeric at position {} ' . format ( target , i )) return False except ValueError : logger . warning ( 'Target Invalid: {} expected host:port at position {} ' . format ( target , i )) return False else : logger . warning ( 'Target Invalid: {} ; must be a enumerable (list, tuple)' . format ( targets )) return False return True","title":"validate_targets()"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.UninstallManager","text":"Manage Filebeat uninstall process","title":"UninstallManager"},{"location":"guides/developers/SDK/services/filebeat/install/#dynamite_nsm.services.filebeat.install.UninstallManager.__init__","text":"Uninstall Filebeat Parameters: Name Type Description Default stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/filebeat/install.py def __init__ ( self , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Filebeat Args: stdout: Print output to console verbose: Include detailed debug messages \"\"\" from dynamite_nsm.services.filebeat.process import ProcessManager env_vars = utilities . get_environment_file_dict () fb_directories = [ env_vars . get ( 'FILEBEAT_HOME' ), ] super () . __init__ ( 'filebeat.uninstall' , directories = fb_directories , process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'filebeat.service' , environ_vars = [ 'FILEBEAT_HOME' ], stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/logs/","text":"Work with a Filebeat's filebeat.log. Currently, supports: filebeat.log To import... from dynamite_nsm.services.filebeat import logs as filebeat_logs InvalidFilebeatStatusLogEntry __init__ ( self , message ) special Thrown when a Filebeat log entry is improperly formatted Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , message ): \"\"\"Thrown when a Filebeat log entry is improperly formatted Args: message: A more specific error message Returns: None \"\"\" msg = \"FileBeat log entry is invalid: {} \" . format ( message ) super ( InvalidFilebeatStatusLogEntry , self ) . __init__ ( msg ) MetricsEntry A single Filebeat metrics entry for a specific time-interval __init__ ( self , monitoring_payload , time ) special Initialize metrics entry object Parameters: Name Type Description Default monitoring_payload Dict The serialized JSON for \"monitoring\" status types required time datetime A datetime object representing when the metrics entry was written required Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , monitoring_payload : Dict , time : datetime ): \"\"\"Initialize metrics entry object Args: monitoring_payload: The serialized JSON for \"monitoring\" status types time: A datetime object representing when the metrics entry was written \"\"\" self . monitoring_payload = monitoring_payload metrics = self . monitoring_payload [ \"monitoring\" ][ \"metrics\" ] self . time = time self . open_file_handles = metrics . get ( \"beat\" , {}) . get ( \"handles\" , {}) . get ( \"open\" , 0 ) self . memory_allocated = metrics . get ( \"beat\" , {}) . get ( \"memstats\" , {}) . get ( \"memory_alloc\" , 0 ) self . harvester_open_files = metrics . get ( \"filebeat\" , {}) . get ( \"harvester\" , {}) . get ( \"open_files\" , 0 ) self . harvester_running_files = metrics . get ( \"filebeat\" , {}) . get ( \"harvester\" , {}) . get ( \"running_files\" , 0 ) self . write_bytes = metrics . get ( \"libbeat\" , {}) . get ( \"output\" , {}) . get ( \"write\" , {}) . get ( \"bytes\" , 0 ) self . read_bytes = metrics . get ( \"libbeat\" , {}) . get ( \"output\" , {}) . get ( \"read\" , {}) . get ( \"bytes\" , 0 ) self . active_events = metrics . get ( \"libbeat\" , {}) . get ( \"pipeline\" , {}) . get ( \"events\" , {}) . get ( \"active\" , 0 ) self . published_events = metrics . get ( \"libbeat\" , {}) . get ( \"pipeline\" , {}) . get ( \"events\" , {}) . get ( \"published\" , 0 ) merge_metric_entry ( self , metric_entry ) Merge another metrics entry into this one Parameters: Name Type Description Default metric_entry MetricsEntry The MetricsEntry you wish to merge in required Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/logs.py def merge_metric_entry ( self , metric_entry : MetricsEntry ) -> None : \"\"\"Merge another metrics entry into this one Args: metric_entry: The MetricsEntry you wish to merge in Returns: None \"\"\" self . open_file_handles = math . ceil (( self . open_file_handles + metric_entry . open_file_handles ) / 2 ) self . memory_allocated = math . ceil (( self . memory_allocated + metric_entry . memory_allocated ) / 2 ) self . harvester_open_files = math . ceil (( self . harvester_open_files + metric_entry . harvester_open_files ) / 2 ) self . harvester_running_files = math . ceil ( ( self . harvester_running_files + metric_entry . harvester_running_files ) / 2 ) self . write_bytes += metric_entry . write_bytes self . read_bytes += metric_entry . read_bytes self . active_events += metric_entry . active_events self . published_events += metric_entry . published_events StatusEntry An entry from Filebeat's main log; automatically parses out MetricsEntries into their own dedicated object __init__ ( self , entry_raw , include_json_payload = False ) special A status entry Parameters: Name Type Description Default entry_raw str A line item representing a single entry within the Filebeat log required include_json_payload Optional[bool] If, True, then the metrics payload will be included in its raw JSON form False Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , entry_raw : str , include_json_payload : Optional [ bool ] = False ): \"\"\"A status entry Args: entry_raw: A line item representing a single entry within the Filebeat log include_json_payload: If, True, then the metrics payload will be included in its raw JSON form \"\"\" self . include_json_payload = include_json_payload self . entry_raw = entry_raw self . json_payload = False self . payload = None self . metrics = None self . message = None self . description = None self . category = None self . timestamp = None self . log_level = None self . _parse_entry () StatusLog Provides an interface for working with Filebeat's main log __init__ ( self , log_sample_size = 500 , include_json_payloads = False ) special Work with Filebeat's filebeat.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries to parse 500 include_json_payloads Optional[bool] If, True, then metrics payloads will be included in their raw JSON form False Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_json_payloads : Optional [ bool ] = False ): \"\"\"Work with Filebeat's filebeat.log Args: log_sample_size: The maximum number of entries to parse include_json_payloads: If, True, then metrics payloads will be included in their raw JSON form \"\"\" self . include_json_payloads = include_json_payloads self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . filebeat_home = self . env_dict . get ( 'FILEBEAT_HOME' ) self . log_path = os . path . join ( self . filebeat_home , 'logs' , 'filebeat' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) iter_aggregated_metrics ( self , start = None , end = None , tolerance_seconds = 60 ) Iterates through metric entries, while aggregating entries together that are within the same tolerance_seconds into a single MetricsEntry Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None tolerance_seconds Optional[int] Specifies the maximum numbers seconds between entries to consider them common, and therefore aggregate. 60 Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/filebeat/logs.py def iter_aggregated_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , tolerance_seconds : Optional [ int ] = 60 ): \"\"\"Iterates through metric entries, while aggregating entries together that are within the same tolerance_seconds into a single MetricsEntry Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum numbers seconds between entries to consider them common, and therefore aggregate. Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . time ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry iter_entries ( self , start = None , end = None , log_level = None , category = None ) Iterate through StatusEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None log_level DEBUG, INFO, WARN, ERROR, CRITICAL None category Defaults to all if none specified; valid categories are beat, cfgwarn, crawler, harvester, monitoring, publisher, registrar, seccomp None Returns: Type Description yields a StatusEntry for every iteration Source code in dynamite_nsm/services/filebeat/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , log_level = None , category = None ): \"\"\"Iterate through StatusEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time log_level: DEBUG, INFO, WARN, ERROR, CRITICAL category: Defaults to all if none specified; valid categories are beat, cfgwarn, crawler, harvester, monitoring, publisher, registrar, seccomp Returns: yields a StatusEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) for en in self . entries : try : en = StatusEntry ( en , include_json_payload = self . include_json_payloads ) except InvalidFilebeatStatusLogEntry : continue if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): if log_level : if log_entry . log_level . lower () != log_level . lower (): continue if category : if log_entry . category . lower () != category . lower (): continue yield log_entry iter_metrics ( self , start = None , end = None ) Iterate through metrics entries individually Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/filebeat/logs.py def iter_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ): \"\"\"Iterate through metrics entries individually Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" for entry in self . iter_entries ( start , end ): if entry . metrics : yield entry . metrics tail_entries ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/filebeat/logs.py def tail_entries ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Log Level' , 'Category' , 'Message' ], [ entry . time , entry . log_level , entry . category , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) tail_metrics ( self , pretty_print = True ) Tail and follow a metrics log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/filebeat/logs.py def tail_metrics ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a metrics log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for metric in self . iter_aggregated_metrics ( start = start , end = end ): if metric . time . timestamp () not in visited : visited . append ( metric . time . timestamp ()) if not pretty_print : print ( json . dumps ( json . loads ( str ( metric )), indent = 1 )) else : status_table = [ [ 'Time' , 'Memory Allocated' , 'Read (Bytes)' , 'Write (Bytes)' , 'Open Files' , 'Active Events' , 'Published Events' ], [ metric . time , metric . memory_allocated , metric . read_bytes , metric . write_bytes , metric . open_file_handles , metric . active_events , metric . published_events ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) parse_filebeat_datetime ( t ) Parse a common filebeat timestamp string Parameters: Name Type Description Default t str A '%Y-%m-%dT%H:%M:%S.%f' formatted string required Returns: Type Description datetime A datetime object Source code in dynamite_nsm/services/filebeat/logs.py def parse_filebeat_datetime ( t : str ) -> datetime : \"\"\"Parse a common filebeat timestamp string Args: t: A '%Y-%m-%dT%H:%M:%S.%f' formatted string Returns: A datetime object \"\"\" ret = datetime . strptime ( t [ 0 : 22 ], '%Y-%m- %d T%H:%M:%S. %f ' ) if t [ 23 ] == '+' : ret -= timedelta ( hours = int ( t [ 24 : 26 ]), minutes = int ( t [ 27 :])) elif t [ 23 ] == '-' : ret += timedelta ( hours = int ( t [ 24 : 26 ]), minutes = int ( t [ 27 :])) return ret","title":"logs"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.InvalidFilebeatStatusLogEntry","text":"","title":"InvalidFilebeatStatusLogEntry"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.InvalidFilebeatStatusLogEntry.__init__","text":"Thrown when a Filebeat log entry is improperly formatted Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , message ): \"\"\"Thrown when a Filebeat log entry is improperly formatted Args: message: A more specific error message Returns: None \"\"\" msg = \"FileBeat log entry is invalid: {} \" . format ( message ) super ( InvalidFilebeatStatusLogEntry , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.MetricsEntry","text":"A single Filebeat metrics entry for a specific time-interval","title":"MetricsEntry"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.MetricsEntry.__init__","text":"Initialize metrics entry object Parameters: Name Type Description Default monitoring_payload Dict The serialized JSON for \"monitoring\" status types required time datetime A datetime object representing when the metrics entry was written required Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , monitoring_payload : Dict , time : datetime ): \"\"\"Initialize metrics entry object Args: monitoring_payload: The serialized JSON for \"monitoring\" status types time: A datetime object representing when the metrics entry was written \"\"\" self . monitoring_payload = monitoring_payload metrics = self . monitoring_payload [ \"monitoring\" ][ \"metrics\" ] self . time = time self . open_file_handles = metrics . get ( \"beat\" , {}) . get ( \"handles\" , {}) . get ( \"open\" , 0 ) self . memory_allocated = metrics . get ( \"beat\" , {}) . get ( \"memstats\" , {}) . get ( \"memory_alloc\" , 0 ) self . harvester_open_files = metrics . get ( \"filebeat\" , {}) . get ( \"harvester\" , {}) . get ( \"open_files\" , 0 ) self . harvester_running_files = metrics . get ( \"filebeat\" , {}) . get ( \"harvester\" , {}) . get ( \"running_files\" , 0 ) self . write_bytes = metrics . get ( \"libbeat\" , {}) . get ( \"output\" , {}) . get ( \"write\" , {}) . get ( \"bytes\" , 0 ) self . read_bytes = metrics . get ( \"libbeat\" , {}) . get ( \"output\" , {}) . get ( \"read\" , {}) . get ( \"bytes\" , 0 ) self . active_events = metrics . get ( \"libbeat\" , {}) . get ( \"pipeline\" , {}) . get ( \"events\" , {}) . get ( \"active\" , 0 ) self . published_events = metrics . get ( \"libbeat\" , {}) . get ( \"pipeline\" , {}) . get ( \"events\" , {}) . get ( \"published\" , 0 )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.MetricsEntry.merge_metric_entry","text":"Merge another metrics entry into this one Parameters: Name Type Description Default metric_entry MetricsEntry The MetricsEntry you wish to merge in required Returns: Type Description None None Source code in dynamite_nsm/services/filebeat/logs.py def merge_metric_entry ( self , metric_entry : MetricsEntry ) -> None : \"\"\"Merge another metrics entry into this one Args: metric_entry: The MetricsEntry you wish to merge in Returns: None \"\"\" self . open_file_handles = math . ceil (( self . open_file_handles + metric_entry . open_file_handles ) / 2 ) self . memory_allocated = math . ceil (( self . memory_allocated + metric_entry . memory_allocated ) / 2 ) self . harvester_open_files = math . ceil (( self . harvester_open_files + metric_entry . harvester_open_files ) / 2 ) self . harvester_running_files = math . ceil ( ( self . harvester_running_files + metric_entry . harvester_running_files ) / 2 ) self . write_bytes += metric_entry . write_bytes self . read_bytes += metric_entry . read_bytes self . active_events += metric_entry . active_events self . published_events += metric_entry . published_events","title":"merge_metric_entry()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusEntry","text":"An entry from Filebeat's main log; automatically parses out MetricsEntries into their own dedicated object","title":"StatusEntry"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusEntry.__init__","text":"A status entry Parameters: Name Type Description Default entry_raw str A line item representing a single entry within the Filebeat log required include_json_payload Optional[bool] If, True, then the metrics payload will be included in its raw JSON form False Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , entry_raw : str , include_json_payload : Optional [ bool ] = False ): \"\"\"A status entry Args: entry_raw: A line item representing a single entry within the Filebeat log include_json_payload: If, True, then the metrics payload will be included in its raw JSON form \"\"\" self . include_json_payload = include_json_payload self . entry_raw = entry_raw self . json_payload = False self . payload = None self . metrics = None self . message = None self . description = None self . category = None self . timestamp = None self . log_level = None self . _parse_entry ()","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog","text":"Provides an interface for working with Filebeat's main log","title":"StatusLog"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog.__init__","text":"Work with Filebeat's filebeat.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries to parse 500 include_json_payloads Optional[bool] If, True, then metrics payloads will be included in their raw JSON form False Source code in dynamite_nsm/services/filebeat/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_json_payloads : Optional [ bool ] = False ): \"\"\"Work with Filebeat's filebeat.log Args: log_sample_size: The maximum number of entries to parse include_json_payloads: If, True, then metrics payloads will be included in their raw JSON form \"\"\" self . include_json_payloads = include_json_payloads self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . filebeat_home = self . env_dict . get ( 'FILEBEAT_HOME' ) self . log_path = os . path . join ( self . filebeat_home , 'logs' , 'filebeat' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog.iter_aggregated_metrics","text":"Iterates through metric entries, while aggregating entries together that are within the same tolerance_seconds into a single MetricsEntry Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None tolerance_seconds Optional[int] Specifies the maximum numbers seconds between entries to consider them common, and therefore aggregate. 60 Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/filebeat/logs.py def iter_aggregated_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , tolerance_seconds : Optional [ int ] = 60 ): \"\"\"Iterates through metric entries, while aggregating entries together that are within the same tolerance_seconds into a single MetricsEntry Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum numbers seconds between entries to consider them common, and therefore aggregate. Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . time ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry","title":"iter_aggregated_metrics()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog.iter_entries","text":"Iterate through StatusEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None log_level DEBUG, INFO, WARN, ERROR, CRITICAL None category Defaults to all if none specified; valid categories are beat, cfgwarn, crawler, harvester, monitoring, publisher, registrar, seccomp None Returns: Type Description yields a StatusEntry for every iteration Source code in dynamite_nsm/services/filebeat/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , log_level = None , category = None ): \"\"\"Iterate through StatusEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time log_level: DEBUG, INFO, WARN, ERROR, CRITICAL category: Defaults to all if none specified; valid categories are beat, cfgwarn, crawler, harvester, monitoring, publisher, registrar, seccomp Returns: yields a StatusEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) for en in self . entries : try : en = StatusEntry ( en , include_json_payload = self . include_json_payloads ) except InvalidFilebeatStatusLogEntry : continue if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): if log_level : if log_entry . log_level . lower () != log_level . lower (): continue if category : if log_entry . category . lower () != category . lower (): continue yield log_entry","title":"iter_entries()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog.iter_metrics","text":"Iterate through metrics entries individually Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/filebeat/logs.py def iter_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ): \"\"\"Iterate through metrics entries individually Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" for entry in self . iter_entries ( start , end ): if entry . metrics : yield entry . metrics","title":"iter_metrics()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog.tail_entries","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/filebeat/logs.py def tail_entries ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Log Level' , 'Category' , 'Message' ], [ entry . time , entry . log_level , entry . category , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail_entries()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.StatusLog.tail_metrics","text":"Tail and follow a metrics log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/filebeat/logs.py def tail_metrics ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a metrics log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for metric in self . iter_aggregated_metrics ( start = start , end = end ): if metric . time . timestamp () not in visited : visited . append ( metric . time . timestamp ()) if not pretty_print : print ( json . dumps ( json . loads ( str ( metric )), indent = 1 )) else : status_table = [ [ 'Time' , 'Memory Allocated' , 'Read (Bytes)' , 'Write (Bytes)' , 'Open Files' , 'Active Events' , 'Published Events' ], [ metric . time , metric . memory_allocated , metric . read_bytes , metric . write_bytes , metric . open_file_handles , metric . active_events , metric . published_events ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail_metrics()"},{"location":"guides/developers/SDK/services/filebeat/logs/#dynamite_nsm.services.filebeat.logs.parse_filebeat_datetime","text":"Parse a common filebeat timestamp string Parameters: Name Type Description Default t str A '%Y-%m-%dT%H:%M:%S.%f' formatted string required Returns: Type Description datetime A datetime object Source code in dynamite_nsm/services/filebeat/logs.py def parse_filebeat_datetime ( t : str ) -> datetime : \"\"\"Parse a common filebeat timestamp string Args: t: A '%Y-%m-%dT%H:%M:%S.%f' formatted string Returns: A datetime object \"\"\" ret = datetime . strptime ( t [ 0 : 22 ], '%Y-%m- %d T%H:%M:%S. %f ' ) if t [ 23 ] == '+' : ret -= timedelta ( hours = int ( t [ 24 : 26 ]), minutes = int ( t [ 27 :])) elif t [ 23 ] == '-' : ret += timedelta ( hours = int ( t [ 24 : 26 ]), minutes = int ( t [ 27 :])) return ret","title":"parse_filebeat_datetime()"},{"location":"guides/developers/SDK/services/filebeat/process/","text":"Process Manager for Filebeat processes and sub-processes. To import... from dynamite_nsm.services.filebeat import process as filebeat_process CallFilebeatProcessError __init__ ( self , message ) special Thrown when filebeat process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/filebeat/process.py def __init__ ( self , message ): \"\"\"Thrown when filebeat process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling filebeat process: {} \" . format ( message ) super ( CallFilebeatProcessError , self ) . __init__ ( msg ) ProcessManager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Filebeat Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/filebeat/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Filebeat Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" process . BaseProcessManager . __init__ ( self , 'filebeat.service' , 'filebeat.process' , log_path = None , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not profile . ProcessProfiler () . is_installed (): self . logger . error ( \"Filebeat is not installed. Install it with 'dynamite filebeat install -h'\" ) raise CallFilebeatProcessError ( 'Filebeat is not installed.' )","title":"process"},{"location":"guides/developers/SDK/services/filebeat/process/#dynamite_nsm.services.filebeat.process.CallFilebeatProcessError","text":"","title":"CallFilebeatProcessError"},{"location":"guides/developers/SDK/services/filebeat/process/#dynamite_nsm.services.filebeat.process.CallFilebeatProcessError.__init__","text":"Thrown when filebeat process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/filebeat/process.py def __init__ ( self , message ): \"\"\"Thrown when filebeat process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling filebeat process: {} \" . format ( message ) super ( CallFilebeatProcessError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/process/#dynamite_nsm.services.filebeat.process.ProcessManager","text":"","title":"ProcessManager"},{"location":"guides/developers/SDK/services/filebeat/process/#dynamite_nsm.services.filebeat.process.ProcessManager.__init__","text":"Manage Filebeat Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/filebeat/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Filebeat Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" process . BaseProcessManager . __init__ ( self , 'filebeat.service' , 'filebeat.process' , log_path = None , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not profile . ProcessProfiler () . is_installed (): self . logger . error ( \"Filebeat is not installed. Install it with 'dynamite filebeat install -h'\" ) raise CallFilebeatProcessError ( 'Filebeat is not installed.' )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/profile/","text":"Profile Filebeat processes to ensure they are running and installed properly. To import... from dynamite_nsm.services.filebeat import profile as filebeat_profile ProcessProfiler __init__ ( self ) special Get information about the Filebeat service Source code in dynamite_nsm/services/filebeat/profile.py def __init__ ( self ): \"\"\" Get information about the Filebeat service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . filebeat_home = self . env_dict . get ( 'FILEBEAT_HOME' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . filebeat_home , config_directory = self . filebeat_home , required_install_files = [ 'filebeat' , 'filebeat.yml' ] ) is_running ( self ) Determine of Filebeat is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/filebeat/profile.py def is_running ( self ) -> bool : \"\"\" Determine of Filebeat is running Returns: True, if running \"\"\" if self . filebeat_home : try : return filebeat_process . ProcessManager () . status ()[ 'running' ] except KeyError : return filebeat_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"profile"},{"location":"guides/developers/SDK/services/filebeat/profile/#dynamite_nsm.services.filebeat.profile.ProcessProfiler","text":"","title":"ProcessProfiler"},{"location":"guides/developers/SDK/services/filebeat/profile/#dynamite_nsm.services.filebeat.profile.ProcessProfiler.__init__","text":"Get information about the Filebeat service Source code in dynamite_nsm/services/filebeat/profile.py def __init__ ( self ): \"\"\" Get information about the Filebeat service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . filebeat_home = self . env_dict . get ( 'FILEBEAT_HOME' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . filebeat_home , config_directory = self . filebeat_home , required_install_files = [ 'filebeat' , 'filebeat.yml' ] )","title":"__init__()"},{"location":"guides/developers/SDK/services/filebeat/profile/#dynamite_nsm.services.filebeat.profile.ProcessProfiler.is_running","text":"Determine of Filebeat is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/filebeat/profile.py def is_running ( self ) -> bool : \"\"\" Determine of Filebeat is running Returns: True, if running \"\"\" if self . filebeat_home : try : return filebeat_process . ProcessManager () . status ()[ 'running' ] except KeyError : return filebeat_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"is_running()"},{"location":"guides/developers/SDK/services/kibana/config/","text":"Configuration wrappers for the main Kibana configuration file. To import... from dynamite_nsm.services.kibana import config as kibana_config ConfigManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Manage Kibana Configuration Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/kibana/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/kibana/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage Kibana Configuration Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/kibana/) stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" extract_tokens = { 'host' : ( 'server.host' ,), 'port' : ( 'server.port' ,), 'elasticsearch_targets' : ( 'elasticsearch.hosts' ,), 'elasticsearch_username' : ( 'elasticsearch.username' ,), 'elasticsearch_password' : ( 'elasticsearch.password' ,), } self . host = None self . port = None self . elasticsearch_targets = None self . elasticsearch_username = None self . elasticsearch_password = None self . configuration_directory = configuration_directory self . kibana_config_path = f ' { self . configuration_directory } /kibana.yml' with open ( self . kibana_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'KIBANACFG' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The text to be appended at the top of the config file (typically used for YAML version header) None Returns: Type Description None None Source code in dynamite_nsm/services/kibana/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The text to be appended at the top of the config file (typically used for YAML version header) Returns: None \"\"\" if not out_file_path : out_file_path = self . kibana_config_path super ( ConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"config"},{"location":"guides/developers/SDK/services/kibana/config/#dynamite_nsm.services.kibana.config.ConfigManager","text":"","title":"ConfigManager"},{"location":"guides/developers/SDK/services/kibana/config/#dynamite_nsm.services.kibana.config.ConfigManager.__init__","text":"Manage Kibana Configuration Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/kibana/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/kibana/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage Kibana Configuration Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/kibana/) stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" extract_tokens = { 'host' : ( 'server.host' ,), 'port' : ( 'server.port' ,), 'elasticsearch_targets' : ( 'elasticsearch.hosts' ,), 'elasticsearch_username' : ( 'elasticsearch.username' ,), 'elasticsearch_password' : ( 'elasticsearch.password' ,), } self . host = None self . port = None self . elasticsearch_targets = None self . elasticsearch_username = None self . elasticsearch_password = None self . configuration_directory = configuration_directory self . kibana_config_path = f ' { self . configuration_directory } /kibana.yml' with open ( self . kibana_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'KIBANACFG' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file ()","title":"__init__()"},{"location":"guides/developers/SDK/services/kibana/config/#dynamite_nsm.services.kibana.config.ConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] The text to be appended at the top of the config file (typically used for YAML version header) None Returns: Type Description None None Source code in dynamite_nsm/services/kibana/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: The text to be appended at the top of the config file (typically used for YAML version header) Returns: None \"\"\" if not out_file_path : out_file_path = self . kibana_config_path super ( ConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/kibana/install/","text":"Installation Manager for Kibana and its dependencies. To import... from dynamite_nsm.services.kibana import install as kibana_install InstallManager __init__ ( self , configuration_directory , install_directory , log_directory , download_kibana_archive = True , stdout = False , verbose = False ) special Install Kibana Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/kibana/) required install_directory str Path to the install directory (E.G /opt/dynamite/kibana/) required log_directory str Path to the log directory (E.G /var/log/dynamite/kibana/) required download_kibana_archive Optional[bool] If True, download the Kibana archive from a mirror True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/kibana/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_kibana_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Kibana Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/kibana/) install_directory: Path to the install directory (E.G /opt/dynamite/kibana/) log_directory: Path to the log directory (E.G /var/log/dynamite/kibana/) download_kibana_archive: If True, download the Kibana archive from a mirror stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . stdout = stdout self . verbose = verbose super () . __init__ ( 'kibana.install' , verbose , stdout ) if download_kibana_archive : self . logger . info ( \"Attempting to download Kibana (OpenDistro) archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . KIBANA_MIRRORS ) self . logger . info ( f 'Attempting to extract Kibana archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . KIBANA_MIRRORS ) copy_kibana_files_and_directories ( self ) Copy the required Kibana files from the install cache to their respective directories Returns: Type Description None None Source code in dynamite_nsm/services/kibana/install.py def copy_kibana_files_and_directories ( self ) -> None : \"\"\"Copy the required Kibana files from the install cache to their respective directories Returns: None \"\"\" kibana_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'config/kibana.yml' , 'config/node.options' ] install_paths = [ 'bin/' , 'data/' , 'node/' , 'node_modules/' , 'plugins/' , 'src/' , 'package.json' ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { kibana_tarball_extracted } / { conf } ' , self . configuration_directory ) for inst in install_paths : self . copy_file_or_directory_to_destination ( f ' { kibana_tarball_extracted } / { inst } ' , self . install_directory ) create_update_kibana_environment_variables ( self ) Creates all the required Kibana environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/kibana/install.py def create_update_kibana_environment_variables ( self ) -> None : \"\"\"Creates all the required Kibana environmental variables Returns: None \"\"\" self . create_update_env_variable ( 'KIBANA_PATH_CONF' , self . configuration_directory ) self . create_update_env_variable ( 'KIBANA_HOME' , self . install_directory ) self . create_update_env_variable ( 'KIBANA_LOGS' , self . log_directory ) setup ( self , host = None , port = None , elasticsearch_targets = None ) Setup Kibana Parameters: Name Type Description Default host Optional[str] The IP or hostname to listen on None port Optional[int] The port to listen on None elasticsearch_targets Optional[List[str]] A list of Elasticsearch urls None Returns: Type Description None None Source code in dynamite_nsm/services/kibana/install.py def setup ( self , host : Optional [ str ] = None , port : Optional [ int ] = None , elasticsearch_targets : Optional [ List [ str ]] = None ) -> None : \"\"\"Setup Kibana Args: host: The IP or hostname to listen on port: The port to listen on elasticsearch_targets: A list of Elasticsearch urls Returns: None \"\"\" sysctl = systemctl . SystemCtl () # Directory setup self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . debug ( f 'Creating directory: { self . log_directory } ' ) utilities . makedirs ( self . log_directory ) self . copy_kibana_files_and_directories () self . create_update_kibana_environment_variables () self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /kibana/kibana.yml' , self . configuration_directory ) # Optimize Configurations kb_main_config = config . ConfigManager ( self . configuration_directory ) if not host : host = utilities . get_primary_ip_address () if not port : port = 5601 if not elasticsearch_targets : elasticsearch_targets = [ f 'https:// { utilities . get_primary_ip_address () } :9200' ] self . logger . debug ( f 'Elasticsearch Targets = { elasticsearch_targets } ' ) kb_main_config . host = host kb_main_config . port = port self . logger . debug ( f 'Kibana will listen on { kb_main_config . host } : { kb_main_config . port } ' ) kb_main_config . elasticsearch_targets = elasticsearch_targets self . logger . info ( 'Applying configuration.' ) kb_main_config . commit () # Fix Permissions utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . log_directory , user = 'dynamite' , group = 'dynamite' ) # Install and enable service self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/kibana.service' ) sysctl . install_and_enable ( f ' { const . DEFAULT_CONFIGS } /systemd/kibana.service' ) self . logger . info ( 'Installing \"BaseViews\" Kibana package' ) task = install_dynamite_base_views . InstallKibanaDynamiteBaseViewsPackage ( username = 'admin' , password = 'admin' , target = f \"http:// { host } : { port } \" ) task . download_and_install () UninstallManager __init__ ( self , purge_config = True , stdout = False , verbose = False ) special Uninstall Kibana Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/kibana/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Kibana Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.kibana.process import ProcessManager env_vars = utilities . get_environment_file_dict () kb_directories = [ env_vars . get ( 'KIBANA_HOME' ), env_vars . get ( 'KIBANA_LOGS' )] if purge_config : kb_directories . append ( env_vars . get ( 'KIBANA_PATH_CONF' )) super () . __init__ ( 'kibana.uninstall' , directories = kb_directories , process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'kibana.service' , environ_vars = [ 'KIBANA_HOME' , 'KIBANA_LOGS' , 'KIBANA_PATH_CONF' ], stdout = stdout , verbose = verbose )","title":"install"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.InstallManager","text":"","title":"InstallManager"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.InstallManager.__init__","text":"Install Kibana Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/kibana/) required install_directory str Path to the install directory (E.G /opt/dynamite/kibana/) required log_directory str Path to the log directory (E.G /var/log/dynamite/kibana/) required download_kibana_archive Optional[bool] If True, download the Kibana archive from a mirror True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/kibana/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_kibana_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Kibana Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/kibana/) install_directory: Path to the install directory (E.G /opt/dynamite/kibana/) log_directory: Path to the log directory (E.G /var/log/dynamite/kibana/) download_kibana_archive: If True, download the Kibana archive from a mirror stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . stdout = stdout self . verbose = verbose super () . __init__ ( 'kibana.install' , verbose , stdout ) if download_kibana_archive : self . logger . info ( \"Attempting to download Kibana (OpenDistro) archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . KIBANA_MIRRORS ) self . logger . info ( f 'Attempting to extract Kibana archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . KIBANA_MIRRORS )","title":"__init__()"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.InstallManager.copy_kibana_files_and_directories","text":"Copy the required Kibana files from the install cache to their respective directories Returns: Type Description None None Source code in dynamite_nsm/services/kibana/install.py def copy_kibana_files_and_directories ( self ) -> None : \"\"\"Copy the required Kibana files from the install cache to their respective directories Returns: None \"\"\" kibana_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'config/kibana.yml' , 'config/node.options' ] install_paths = [ 'bin/' , 'data/' , 'node/' , 'node_modules/' , 'plugins/' , 'src/' , 'package.json' ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { kibana_tarball_extracted } / { conf } ' , self . configuration_directory ) for inst in install_paths : self . copy_file_or_directory_to_destination ( f ' { kibana_tarball_extracted } / { inst } ' , self . install_directory )","title":"copy_kibana_files_and_directories()"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.InstallManager.create_update_kibana_environment_variables","text":"Creates all the required Kibana environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/kibana/install.py def create_update_kibana_environment_variables ( self ) -> None : \"\"\"Creates all the required Kibana environmental variables Returns: None \"\"\" self . create_update_env_variable ( 'KIBANA_PATH_CONF' , self . configuration_directory ) self . create_update_env_variable ( 'KIBANA_HOME' , self . install_directory ) self . create_update_env_variable ( 'KIBANA_LOGS' , self . log_directory )","title":"create_update_kibana_environment_variables()"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.InstallManager.setup","text":"Setup Kibana Parameters: Name Type Description Default host Optional[str] The IP or hostname to listen on None port Optional[int] The port to listen on None elasticsearch_targets Optional[List[str]] A list of Elasticsearch urls None Returns: Type Description None None Source code in dynamite_nsm/services/kibana/install.py def setup ( self , host : Optional [ str ] = None , port : Optional [ int ] = None , elasticsearch_targets : Optional [ List [ str ]] = None ) -> None : \"\"\"Setup Kibana Args: host: The IP or hostname to listen on port: The port to listen on elasticsearch_targets: A list of Elasticsearch urls Returns: None \"\"\" sysctl = systemctl . SystemCtl () # Directory setup self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . debug ( f 'Creating directory: { self . log_directory } ' ) utilities . makedirs ( self . log_directory ) self . copy_kibana_files_and_directories () self . create_update_kibana_environment_variables () self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /kibana/kibana.yml' , self . configuration_directory ) # Optimize Configurations kb_main_config = config . ConfigManager ( self . configuration_directory ) if not host : host = utilities . get_primary_ip_address () if not port : port = 5601 if not elasticsearch_targets : elasticsearch_targets = [ f 'https:// { utilities . get_primary_ip_address () } :9200' ] self . logger . debug ( f 'Elasticsearch Targets = { elasticsearch_targets } ' ) kb_main_config . host = host kb_main_config . port = port self . logger . debug ( f 'Kibana will listen on { kb_main_config . host } : { kb_main_config . port } ' ) kb_main_config . elasticsearch_targets = elasticsearch_targets self . logger . info ( 'Applying configuration.' ) kb_main_config . commit () # Fix Permissions utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . log_directory , user = 'dynamite' , group = 'dynamite' ) # Install and enable service self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/kibana.service' ) sysctl . install_and_enable ( f ' { const . DEFAULT_CONFIGS } /systemd/kibana.service' ) self . logger . info ( 'Installing \"BaseViews\" Kibana package' ) task = install_dynamite_base_views . InstallKibanaDynamiteBaseViewsPackage ( username = 'admin' , password = 'admin' , target = f \"http:// { host } : { port } \" ) task . download_and_install ()","title":"setup()"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.UninstallManager","text":"","title":"UninstallManager"},{"location":"guides/developers/SDK/services/kibana/install/#dynamite_nsm.services.kibana.install.UninstallManager.__init__","text":"Uninstall Kibana Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/kibana/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Kibana Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.kibana.process import ProcessManager env_vars = utilities . get_environment_file_dict () kb_directories = [ env_vars . get ( 'KIBANA_HOME' ), env_vars . get ( 'KIBANA_LOGS' )] if purge_config : kb_directories . append ( env_vars . get ( 'KIBANA_PATH_CONF' )) super () . __init__ ( 'kibana.uninstall' , directories = kb_directories , process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'kibana.service' , environ_vars = [ 'KIBANA_HOME' , 'KIBANA_LOGS' , 'KIBANA_PATH_CONF' ], stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/kibana/process/","text":"Process Manager for Kibana process To import... from dynamite_nsm.services.kibana import process as kibana_process CallKibanaProcessError __init__ ( self , message ) special Thrown when kibana process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/kibana/process.py def __init__ ( self , message ): \"\"\"Thrown when kibana process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = f 'An error occurred while calling kibana process: { message } ' super ( CallKibanaProcessError , self ) . __init__ ( msg ) ProcessManager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Kibana Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/kibana/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Kibana Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" environ = utilities . get_environment_file_dict () process . BaseProcessManager . __init__ ( self , 'kibana.service' , 'kibana.process' , log_path = environ . get ( 'KIBANA_LOGS' ), stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not kibana_profile . ProcessProfiler () . is_installed (): self . logger . error ( \"Kibana is not installed. Install it with 'dynamite kibana install -h'\" ) raise CallKibanaProcessError ( \"Kibana is not installed.\" ) optimize ( self ) Runs Kibana webpack optimizer among other things. Returns: Type Description None None Source code in dynamite_nsm/services/kibana/process.py def optimize ( self ) -> None : \"\"\"Runs Kibana webpack optimizer among other things. Returns: None \"\"\" environ = utilities . get_environment_file_dict () if not os . path . exists ( PID_DIRECTORY ): utilities . makedirs ( PID_DIRECTORY ) utilities . set_ownership_of_file ( PID_DIRECTORY , user = 'dynamite' , group = 'dynamite' ) self . logger . info ( 'Optimizing Kibana Libraries.' ) # Kibana initially has to be called as root due to a process forking issue when using runuser # builtin subprocess . call ( ' {} /bin/kibana --optimize --allow-root' . format ( environ [ 'KIBANA_HOME' ], ), shell = True , env = utilities . get_environment_file_dict (), stderr = subprocess . PIPE , stdout = subprocess . PIPE ) # Pass permissions back to dynamite user utilities . set_ownership_of_file ( environ [ 'KIBANA_LOGS' ], user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( environ [ 'KIBANA_HOME' ], user = 'dynamite' , group = 'dynamite' )","title":"process"},{"location":"guides/developers/SDK/services/kibana/process/#dynamite_nsm.services.kibana.process.CallKibanaProcessError","text":"","title":"CallKibanaProcessError"},{"location":"guides/developers/SDK/services/kibana/process/#dynamite_nsm.services.kibana.process.CallKibanaProcessError.__init__","text":"Thrown when kibana process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/kibana/process.py def __init__ ( self , message ): \"\"\"Thrown when kibana process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = f 'An error occurred while calling kibana process: { message } ' super ( CallKibanaProcessError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/kibana/process/#dynamite_nsm.services.kibana.process.ProcessManager","text":"","title":"ProcessManager"},{"location":"guides/developers/SDK/services/kibana/process/#dynamite_nsm.services.kibana.process.ProcessManager.__init__","text":"Manage Kibana Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/kibana/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Kibana Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" environ = utilities . get_environment_file_dict () process . BaseProcessManager . __init__ ( self , 'kibana.service' , 'kibana.process' , log_path = environ . get ( 'KIBANA_LOGS' ), stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not kibana_profile . ProcessProfiler () . is_installed (): self . logger . error ( \"Kibana is not installed. Install it with 'dynamite kibana install -h'\" ) raise CallKibanaProcessError ( \"Kibana is not installed.\" )","title":"__init__()"},{"location":"guides/developers/SDK/services/kibana/process/#dynamite_nsm.services.kibana.process.ProcessManager.optimize","text":"Runs Kibana webpack optimizer among other things. Returns: Type Description None None Source code in dynamite_nsm/services/kibana/process.py def optimize ( self ) -> None : \"\"\"Runs Kibana webpack optimizer among other things. Returns: None \"\"\" environ = utilities . get_environment_file_dict () if not os . path . exists ( PID_DIRECTORY ): utilities . makedirs ( PID_DIRECTORY ) utilities . set_ownership_of_file ( PID_DIRECTORY , user = 'dynamite' , group = 'dynamite' ) self . logger . info ( 'Optimizing Kibana Libraries.' ) # Kibana initially has to be called as root due to a process forking issue when using runuser # builtin subprocess . call ( ' {} /bin/kibana --optimize --allow-root' . format ( environ [ 'KIBANA_HOME' ], ), shell = True , env = utilities . get_environment_file_dict (), stderr = subprocess . PIPE , stdout = subprocess . PIPE ) # Pass permissions back to dynamite user utilities . set_ownership_of_file ( environ [ 'KIBANA_LOGS' ], user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( environ [ 'KIBANA_HOME' ], user = 'dynamite' , group = 'dynamite' )","title":"optimize()"},{"location":"guides/developers/SDK/services/kibana/profile/","text":"Profile Kibana processes to ensure they are running and installed properly. To import... from dynamite_nsm.services.kibana import profile as kibana_profile ProcessProfiler __init__ ( self ) special Get information about the Kibana service Source code in dynamite_nsm/services/kibana/profile.py def __init__ ( self ): \"\"\" Get information about the Kibana service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . kibana_home = self . env_dict . get ( 'KIBANA_HOME' ) self . kibana_config = self . env_dict . get ( 'KIBANA_PATH_CONF' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . kibana_home , config_directory = self . kibana_config , required_install_files = [ 'bin' , 'data' , 'node' ], required_config_files = [ 'kibana.yml' ] ) is_listening ( self ) Check if Kibana is listening Returns: Type Description True, if listening Source code in dynamite_nsm/services/kibana/profile.py def is_listening ( self ): \"\"\"Check if Kibana is listening Returns: True, if listening \"\"\" if not self . kibana_config : return False if not os . path . exists ( self . kibana_config ): return False kb_config_obj = kibana_config . ConfigManager ( configuration_directory = self . kibana_config ) host = kb_config_obj . host port = kb_config_obj . port if host . strip () == '0.0.0.0' : host = 'localhost' return utilities . check_socket ( host , port ) is_running ( self ) Check if Kibana is running Returns: Type Description True, if running Source code in dynamite_nsm/services/kibana/profile.py def is_running ( self ): \"\"\"Check if Kibana is running Returns: True, if running \"\"\" if self . kibana_home : try : return kibana_process . ProcessManager () . status ()[ 'running' ] except KeyError : return kibana_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"profile"},{"location":"guides/developers/SDK/services/kibana/profile/#dynamite_nsm.services.kibana.profile.ProcessProfiler","text":"","title":"ProcessProfiler"},{"location":"guides/developers/SDK/services/kibana/profile/#dynamite_nsm.services.kibana.profile.ProcessProfiler.__init__","text":"Get information about the Kibana service Source code in dynamite_nsm/services/kibana/profile.py def __init__ ( self ): \"\"\" Get information about the Kibana service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . kibana_home = self . env_dict . get ( 'KIBANA_HOME' ) self . kibana_config = self . env_dict . get ( 'KIBANA_PATH_CONF' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . kibana_home , config_directory = self . kibana_config , required_install_files = [ 'bin' , 'data' , 'node' ], required_config_files = [ 'kibana.yml' ] )","title":"__init__()"},{"location":"guides/developers/SDK/services/kibana/profile/#dynamite_nsm.services.kibana.profile.ProcessProfiler.is_listening","text":"Check if Kibana is listening Returns: Type Description True, if listening Source code in dynamite_nsm/services/kibana/profile.py def is_listening ( self ): \"\"\"Check if Kibana is listening Returns: True, if listening \"\"\" if not self . kibana_config : return False if not os . path . exists ( self . kibana_config ): return False kb_config_obj = kibana_config . ConfigManager ( configuration_directory = self . kibana_config ) host = kb_config_obj . host port = kb_config_obj . port if host . strip () == '0.0.0.0' : host = 'localhost' return utilities . check_socket ( host , port )","title":"is_listening()"},{"location":"guides/developers/SDK/services/kibana/profile/#dynamite_nsm.services.kibana.profile.ProcessProfiler.is_running","text":"Check if Kibana is running Returns: Type Description True, if running Source code in dynamite_nsm/services/kibana/profile.py def is_running ( self ): \"\"\"Check if Kibana is running Returns: True, if running \"\"\" if self . kibana_home : try : return kibana_process . ProcessManager () . status ()[ 'running' ] except KeyError : return kibana_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"is_running()"},{"location":"guides/developers/SDK/services/logstash/config/","text":"Configuration wrappers for the main Logstash configuration file. To import... from dynamite_nsm.services.logstash import config as logstash_config ConfigManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Manage Logstash Configuration Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/logstash/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/logstash/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage Logstash Configuration Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" extract_tokens = { 'node_name' : ( 'node.name' ,), 'path_data' : ( 'path.data' ,), 'path_logs' : ( 'path.logs' ,), 'pipeline_batch_size' : ( 'pipeline.batch.size' ,), 'pipeline_batch_delay' : ( 'pipeline.batch.delay' ,) } self . node_name = None self . path_data = None self . path_logs = None self . pipeline_batch_size = None self . pipeline_batch_delay = None self . configuration_directory = configuration_directory self . logstash_config_path = f ' { self . configuration_directory } /logstash.yml' with open ( self . logstash_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'logstash.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory Source code in dynamite_nsm/services/logstash/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\" Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory \"\"\" if not out_file_path : out_file_path = self . logstash_config_path super ( ConfigManager , self ) . commit ( out_file_path , backup_directory ) JavaHeapOptionsConfigManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special :param configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) :param stdout: Print output to console :param verbose: Include detailed debug messages Source code in dynamite_nsm/services/logstash/config.py def __init__ ( self , configuration_directory , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" :param configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) :param stdout: Print output to console :param verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . logstash_jvm_config_path = f ' { self . configuration_directory } /jvm.options' with open ( self . logstash_jvm_config_path ) as jvm_config : data = { 'data' : jvm_config . readlines ()} super () . __init__ ( data , name = 'logstash.java' , verbose = verbose , stdout = stdout ) commit ( self , out_file_path = None , backup_directory = None ) Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory Source code in dynamite_nsm/services/logstash/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\" Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory \"\"\" if not out_file_path : out_file_path = self . logstash_jvm_config_path super ( JavaHeapOptionsConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"config"},{"location":"guides/developers/SDK/services/logstash/config/#dynamite_nsm.services.logstash.config.ConfigManager","text":"","title":"ConfigManager"},{"location":"guides/developers/SDK/services/logstash/config/#dynamite_nsm.services.logstash.config.ConfigManager.__init__","text":"Manage Logstash Configuration Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/logstash/) required stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/logstash/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Manage Logstash Configuration Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" extract_tokens = { 'node_name' : ( 'node.name' ,), 'path_data' : ( 'path.data' ,), 'path_logs' : ( 'path.logs' ,), 'pipeline_batch_size' : ( 'pipeline.batch.size' ,), 'pipeline_batch_delay' : ( 'pipeline.batch.delay' ,) } self . node_name = None self . path_data = None self . path_logs = None self . pipeline_batch_size = None self . pipeline_batch_delay = None self . configuration_directory = configuration_directory self . logstash_config_path = f ' { self . configuration_directory } /logstash.yml' with open ( self . logstash_config_path ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) super () . __init__ ( self . config_data_raw , name = 'logstash.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file ()","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/config/#dynamite_nsm.services.logstash.config.ConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory Source code in dynamite_nsm/services/logstash/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\" Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory \"\"\" if not out_file_path : out_file_path = self . logstash_config_path super ( ConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/logstash/config/#dynamite_nsm.services.logstash.config.JavaHeapOptionsConfigManager","text":"","title":"JavaHeapOptionsConfigManager"},{"location":"guides/developers/SDK/services/logstash/config/#dynamite_nsm.services.logstash.config.JavaHeapOptionsConfigManager.__init__","text":":param configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) :param stdout: Print output to console :param verbose: Include detailed debug messages Source code in dynamite_nsm/services/logstash/config.py def __init__ ( self , configuration_directory , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" :param configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) :param stdout: Print output to console :param verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . logstash_jvm_config_path = f ' { self . configuration_directory } /jvm.options' with open ( self . logstash_jvm_config_path ) as jvm_config : data = { 'data' : jvm_config . readlines ()} super () . __init__ ( data , name = 'logstash.java' , verbose = verbose , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/config/#dynamite_nsm.services.logstash.config.JavaHeapOptionsConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory Source code in dynamite_nsm/services/logstash/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\" Write out an updated configuration file, and optionally backup the old one. :param out_file_path: The path to the output file; if none given overwrites existing :param backup_directory: The path to the backup directory \"\"\" if not out_file_path : out_file_path = self . logstash_jvm_config_path super ( JavaHeapOptionsConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/logstash/install/","text":"Installation Manager for Logstash and its dependencies. To import... from dynamite_nsm.services.logstash import install as logstash_install InstallManager __init__ ( self , configuration_directory , install_directory , log_directory , download_logstash_archive = True , stdout = False , verbose = False ) special Install Logstash Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/logstash/) required install_directory str Path to the install directory (E.G /opt/dynamite/logstash/) required log_directory str Path to the log directory (E.G /var/log/dynamite/logstash/) required download_logstash_archive Optional[bool] If True, download the Logstash archive from a mirror True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/logstash/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_logstash_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Logstash Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) install_directory: Path to the install directory (E.G /opt/dynamite/logstash/) log_directory: Path to the log directory (E.G /var/log/dynamite/logstash/) download_logstash_archive: If True, download the Logstash archive from a mirror stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . stdout = stdout self . verbose = verbose super () . __init__ ( 'logstash.process' , stdout = self . stdout , verbose = self . verbose ) java_home = self . dynamite_environ . get ( 'JAVA_HOME' ) if not java_home : self . logger . info ( 'Installing compatible version of Java.' ) from dynamite_nsm.services.java import install as java_install java_install . InstallManager ( const . JVM_ROOT , stdout = stdout , verbose = verbose ) . setup () if download_logstash_archive : self . logger . info ( \"Attempting to download Logstash (OSS) archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . LOGSTASH_MIRRORS ) self . logger . info ( f 'Attempting to extract Logstash archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . LOGSTASH_MIRRORS ) copy_logstash_fills_and_directories ( self ) Copy the required Logstash files from the install cache to their respective directories Source code in dynamite_nsm/services/logstash/install.py def copy_logstash_fills_and_directories ( self ) -> None : \"\"\" Copy the required Logstash files from the install cache to their respective directories \"\"\" logstash_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'config/logstash.yml' , 'config/pipelines.yml' , 'config/jvm.options' , 'config/log4j2.properties' ] install_paths = [ 'Gemfile' , 'Gemfile.lock' , 'bin/' , 'data/' , 'lib/' , 'logstash-core/' , 'logstash-core-plugin-api/' , 'modules/' , 'tools/' , 'vendor/' , ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { logstash_tarball_extracted } / { conf } ' , self . configuration_directory ) for inst in install_paths : self . copy_file_or_directory_to_destination ( f ' { logstash_tarball_extracted } / { inst } ' , self . install_directory ) create_update_logstash_environment_variables ( self ) Creates all the required Logstash environmental variables Source code in dynamite_nsm/services/logstash/install.py def create_update_logstash_environment_variables ( self ) -> None : \"\"\" Creates all the required Logstash environmental variables \"\"\" self . create_update_env_variable ( 'LS_PATH_CONF' , self . configuration_directory ) self . create_update_env_variable ( 'LS_HOME' , self . install_directory ) self . create_update_env_variable ( 'LS_LOGS' , self . log_directory ) UninstallManager Uninstall Logstash __init__ ( self , purge_config = True , stdout = False , verbose = False ) special :param purge_config: If enabled, remove all the configuration files associated with this installation :param stdout: Print output to console :param verbose: Include detailed debug messages Source code in dynamite_nsm/services/logstash/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\" :param purge_config: If enabled, remove all the configuration files associated with this installation :param stdout: Print output to console :param verbose: Include detailed debug messages \"\"\" from dynamite_nsm.services.logstash.config import ConfigManager from dynamite_nsm.services.logstash.process import ProcessManager env_vars = utilities . get_environment_file_dict () ls_config = ConfigManager ( configuration_directory = env_vars . get ( 'LS_PATH_CONF' )) ls_directories = [ env_vars . get ( 'LS_HOME' ), ls_config . path_logs ] if purge_config : ls_directories . append ( env_vars . get ( 'LS_PATH_CONF' )) super () . __init__ ( 'logstash.process' , directories = ls_directories , process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'logstash.service' , environ_vars = [ 'LS_HOME' , 'LS_PATH_CONF' ], stdout = stdout , verbose = verbose )","title":"install"},{"location":"guides/developers/SDK/services/logstash/install/#dynamite_nsm.services.logstash.install.InstallManager","text":"","title":"InstallManager"},{"location":"guides/developers/SDK/services/logstash/install/#dynamite_nsm.services.logstash.install.InstallManager.__init__","text":"Install Logstash Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/logstash/) required install_directory str Path to the install directory (E.G /opt/dynamite/logstash/) required log_directory str Path to the log directory (E.G /var/log/dynamite/logstash/) required download_logstash_archive Optional[bool] If True, download the Logstash archive from a mirror True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/logstash/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_logstash_archive : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Logstash Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/logstash/) install_directory: Path to the install directory (E.G /opt/dynamite/logstash/) log_directory: Path to the log directory (E.G /var/log/dynamite/logstash/) download_logstash_archive: If True, download the Logstash archive from a mirror stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . stdout = stdout self . verbose = verbose super () . __init__ ( 'logstash.process' , stdout = self . stdout , verbose = self . verbose ) java_home = self . dynamite_environ . get ( 'JAVA_HOME' ) if not java_home : self . logger . info ( 'Installing compatible version of Java.' ) from dynamite_nsm.services.java import install as java_install java_install . InstallManager ( const . JVM_ROOT , stdout = stdout , verbose = verbose ) . setup () if download_logstash_archive : self . logger . info ( \"Attempting to download Logstash (OSS) archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . LOGSTASH_MIRRORS ) self . logger . info ( f 'Attempting to extract Logstash archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . LOGSTASH_MIRRORS )","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/install/#dynamite_nsm.services.logstash.install.InstallManager.copy_logstash_fills_and_directories","text":"Copy the required Logstash files from the install cache to their respective directories Source code in dynamite_nsm/services/logstash/install.py def copy_logstash_fills_and_directories ( self ) -> None : \"\"\" Copy the required Logstash files from the install cache to their respective directories \"\"\" logstash_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'config/logstash.yml' , 'config/pipelines.yml' , 'config/jvm.options' , 'config/log4j2.properties' ] install_paths = [ 'Gemfile' , 'Gemfile.lock' , 'bin/' , 'data/' , 'lib/' , 'logstash-core/' , 'logstash-core-plugin-api/' , 'modules/' , 'tools/' , 'vendor/' , ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { logstash_tarball_extracted } / { conf } ' , self . configuration_directory ) for inst in install_paths : self . copy_file_or_directory_to_destination ( f ' { logstash_tarball_extracted } / { inst } ' , self . install_directory )","title":"copy_logstash_fills_and_directories()"},{"location":"guides/developers/SDK/services/logstash/install/#dynamite_nsm.services.logstash.install.InstallManager.create_update_logstash_environment_variables","text":"Creates all the required Logstash environmental variables Source code in dynamite_nsm/services/logstash/install.py def create_update_logstash_environment_variables ( self ) -> None : \"\"\" Creates all the required Logstash environmental variables \"\"\" self . create_update_env_variable ( 'LS_PATH_CONF' , self . configuration_directory ) self . create_update_env_variable ( 'LS_HOME' , self . install_directory ) self . create_update_env_variable ( 'LS_LOGS' , self . log_directory )","title":"create_update_logstash_environment_variables()"},{"location":"guides/developers/SDK/services/logstash/install/#dynamite_nsm.services.logstash.install.UninstallManager","text":"Uninstall Logstash","title":"UninstallManager"},{"location":"guides/developers/SDK/services/logstash/install/#dynamite_nsm.services.logstash.install.UninstallManager.__init__","text":":param purge_config: If enabled, remove all the configuration files associated with this installation :param stdout: Print output to console :param verbose: Include detailed debug messages Source code in dynamite_nsm/services/logstash/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\" :param purge_config: If enabled, remove all the configuration files associated with this installation :param stdout: Print output to console :param verbose: Include detailed debug messages \"\"\" from dynamite_nsm.services.logstash.config import ConfigManager from dynamite_nsm.services.logstash.process import ProcessManager env_vars = utilities . get_environment_file_dict () ls_config = ConfigManager ( configuration_directory = env_vars . get ( 'LS_PATH_CONF' )) ls_directories = [ env_vars . get ( 'LS_HOME' ), ls_config . path_logs ] if purge_config : ls_directories . append ( env_vars . get ( 'LS_PATH_CONF' )) super () . __init__ ( 'logstash.process' , directories = ls_directories , process = ProcessManager ( stdout = stdout , verbose = verbose ), sysctl_service_name = 'logstash.service' , environ_vars = [ 'LS_HOME' , 'LS_PATH_CONF' ], stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/process/","text":"Process Manager for Logstash process To import... from dynamite_nsm.services.logstash import process as logstash_process CallLogstashProcessError __init__ ( self , message ) special Thrown when logstash process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/logstash/process.py def __init__ ( self , message ): \"\"\"Thrown when logstash process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling logstash process: {} \" . format ( message ) super ( CallLogstashProcessError , self ) . __init__ ( msg ) ProcessManager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Logstash Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/logstash/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Logstash Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" environ = utilities . get_environment_file_dict () process . BaseProcessManager . __init__ ( self , 'logstash.service' , 'logstash.process' , log_path = environ . get ( 'LS_LOGS' ), stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not logstash_profile . ProcessProfiler () . is_installed (): self . logger . error ( \"LogStash is not installed. Install it with 'dynamite logstash install -h'\" ) raise CallLogstashProcessError ( \"LogStash is not installed.\" )","title":"process"},{"location":"guides/developers/SDK/services/logstash/process/#dynamite_nsm.services.logstash.process.CallLogstashProcessError","text":"","title":"CallLogstashProcessError"},{"location":"guides/developers/SDK/services/logstash/process/#dynamite_nsm.services.logstash.process.CallLogstashProcessError.__init__","text":"Thrown when logstash process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/logstash/process.py def __init__ ( self , message ): \"\"\"Thrown when logstash process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling logstash process: {} \" . format ( message ) super ( CallLogstashProcessError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/process/#dynamite_nsm.services.logstash.process.ProcessManager","text":"","title":"ProcessManager"},{"location":"guides/developers/SDK/services/logstash/process/#dynamite_nsm.services.logstash.process.ProcessManager.__init__","text":"Manage Logstash Process Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/logstash/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Logstash Process Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" environ = utilities . get_environment_file_dict () process . BaseProcessManager . __init__ ( self , 'logstash.service' , 'logstash.process' , log_path = environ . get ( 'LS_LOGS' ), stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not logstash_profile . ProcessProfiler () . is_installed (): self . logger . error ( \"LogStash is not installed. Install it with 'dynamite logstash install -h'\" ) raise CallLogstashProcessError ( \"LogStash is not installed.\" )","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/profile/","text":"Profile Logstash processes to ensure they are running and installed properly. To import... from dynamite_nsm.services.logstash import profile as logstash_profile ProcessProfiler __init__ ( self ) special Get information about the Logstash service Source code in dynamite_nsm/services/logstash/profile.py def __init__ ( self ): \"\"\" Get information about the Logstash service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . logstash_home = self . env_dict . get ( 'LS_HOME' ) self . logstash_config = self . env_dict . get ( 'LS_PATH_CONF' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . logstash_home , config_directory = self . logstash_config , required_install_files = [ 'bin' , 'data' , 'lib' , 'logstash-core' ], required_config_files = [ 'logstash.yml' , 'jvm.options' ] ) is_running ( self ) Check if Logstash is running Returns: Type Description True, if running Source code in dynamite_nsm/services/logstash/profile.py def is_running ( self ): \"\"\"Check if Logstash is running Returns: True, if running \"\"\" if self . logstash_home : try : return logstash_process . ProcessManager () . status ()[ 'running' ] except KeyError : return logstash_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"profile"},{"location":"guides/developers/SDK/services/logstash/profile/#dynamite_nsm.services.logstash.profile.ProcessProfiler","text":"","title":"ProcessProfiler"},{"location":"guides/developers/SDK/services/logstash/profile/#dynamite_nsm.services.logstash.profile.ProcessProfiler.__init__","text":"Get information about the Logstash service Source code in dynamite_nsm/services/logstash/profile.py def __init__ ( self ): \"\"\" Get information about the Logstash service \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . logstash_home = self . env_dict . get ( 'LS_HOME' ) self . logstash_config = self . env_dict . get ( 'LS_PATH_CONF' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . logstash_home , config_directory = self . logstash_config , required_install_files = [ 'bin' , 'data' , 'lib' , 'logstash-core' ], required_config_files = [ 'logstash.yml' , 'jvm.options' ] )","title":"__init__()"},{"location":"guides/developers/SDK/services/logstash/profile/#dynamite_nsm.services.logstash.profile.ProcessProfiler.is_running","text":"Check if Logstash is running Returns: Type Description True, if running Source code in dynamite_nsm/services/logstash/profile.py def is_running ( self ): \"\"\"Check if Logstash is running Returns: True, if running \"\"\" if self . logstash_home : try : return logstash_process . ProcessManager () . status ()[ 'running' ] except KeyError : return logstash_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"is_running()"},{"location":"guides/developers/SDK/services/monitor/install/","text":"Installation Manager that will install Elasticsearch and Kibana on the same physical instance. To import... from dynamite_nsm.services.monitor import install as monitor_install InstallManager __init__ ( self , elasticsearch_install_directory , elasticsearch_configuration_directory = None , elasticsearch_log_directory = None , logstash_install_directory = None , logstash_configuration_directory = None , logstash_log_directory = None , kibana_install_directory = None , kibana_configuration_directory = None , kibana_log_directory = None , stdout = False , verbose = False ) special Install Elaticsearch, Logstash, and Kibana Parameters: Name Type Description Default elasticsearch_install_directory str Path to the elasticsearch install directory (E.G /opt/dynamite/elasticsearch/) required elasticsearch_configuration_directory Optional[str] Path to the elasticsearch configuration directory (E.G /etc/dynamite/elasticsearch/) None elasticsearch_log_directory Optional[str] Path to the elasticsearch log directory (E.G /var/log/dynamite/elasticsearch/) None logstash_install_directory Optional[str] Path to the logstash install directory (E.G /opt/dynamite/logstash/) None logstash_configuration_directory Optional[str] Path to the logstash configuration directory (E.G /etc/dynamite/logstash/) None logstash_log_directory Optional[str] Path to the log directory (E.G /var/log/dynamite/logstash/) None kibana_install_directory Optional[str] Path to the kibana install directory (E.G /opt/dynamite/kibana/) None kibana_configuration_directory Optional[str] Path to the kibana configuration directory (E.G /etc/dynamite/kibana/) None kibana_log_directory Optional[str] Path to the kibana configuration directory (E.G /var/log/dynamite/kibana/) None stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/monitor/install.py def __init__ ( self , elasticsearch_install_directory : str , elasticsearch_configuration_directory : Optional [ str ] = None , elasticsearch_log_directory : Optional [ str ] = None , logstash_install_directory : Optional [ str ] = None , logstash_configuration_directory : Optional [ str ] = None , logstash_log_directory : Optional [ str ] = None , kibana_install_directory : Optional [ str ] = None , kibana_configuration_directory : Optional [ str ] = None , kibana_log_directory : Optional [ str ] = None , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Elaticsearch, Logstash, and Kibana Args: elasticsearch_install_directory: Path to the elasticsearch install directory (E.G /opt/dynamite/elasticsearch/) elasticsearch_configuration_directory: Path to the elasticsearch configuration directory (E.G /etc/dynamite/elasticsearch/) elasticsearch_log_directory: Path to the elasticsearch log directory (E.G /var/log/dynamite/elasticsearch/) logstash_install_directory: Path to the logstash install directory (E.G /opt/dynamite/logstash/) logstash_configuration_directory: Path to the logstash configuration directory (E.G /etc/dynamite/logstash/) logstash_log_directory: Path to the log directory (E.G /var/log/dynamite/logstash/) kibana_install_directory: Path to the kibana install directory (E.G /opt/dynamite/kibana/) kibana_configuration_directory: Path to the kibana configuration directory (E.G /etc/dynamite/kibana/) kibana_log_directory: Path to the kibana configuration directory (E.G /var/log/dynamite/kibana/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( 'monitor.install' , stdout = stdout , verbose = verbose ) self . elasticsearch_install_directory = elasticsearch_install_directory self . elasticsearch_configuration_directory = elasticsearch_configuration_directory self . elasticsearch_log_directory = elasticsearch_log_directory self . logstash_install_directory = logstash_install_directory self . logstash_configuration_directory = logstash_configuration_directory self . logstash_log_directory = logstash_log_directory self . kibana_install_directory = kibana_install_directory self . kibana_configuration_directory = kibana_configuration_directory self . kibana_log_directory = kibana_log_directory setup ( self ) Setup Elasticsearch, Logstash, and Kibana Returns: Type Description None Source code in dynamite_nsm/services/monitor/install.py def setup ( self ): \"\"\"Setup Elasticsearch, Logstash, and Kibana Returns: None \"\"\" es_install = self . elasticsearch_install_directory or self . elasticsearch_configuration_directory or \\ self . elasticsearch_log_directory ls_install = self . logstash_install_directory or self . logstash_configuration_directory or \\ self . logstash_log_directory kb_install = self . kibana_install_directory or self . kibana_configuration_directory or self . kibana_log_directory # Determine how much heap space to pre-allocate to Elasticsearch and Logstash one or both are # specified for installation reserved_memory = utilities . get_memory_available_bytes () * .75 heap_size_gigs = int (( reserved_memory / 10 ** 9 ) / 2 ) ls_heap_size_gigs , es_heap_size_gigs = heap_size_gigs , heap_size_gigs if es_install and ls_install : ls_reserved_memory = utilities . get_memory_available_bytes () * .25 ls_heap_size_gigs = int (( ls_reserved_memory / 10 ** 9 ) / 2 ) es_reserved_memory = utilities . get_memory_available_bytes () * .50 es_heap_size_gigs = int (( es_reserved_memory / 10 ** 9 ) / 2 ) if es_install : if not ( self . elasticsearch_install_directory and self . elasticsearch_configuration_directory and self . elasticsearch_log_directory ): self . logger . error ( 'You must specify elasticsearch-configuration-directory, elasticsearch-install-directory, ' 'and elasticsearch-log-directory.' ) return None elasticsearch_install . InstallManager ( configuration_directory = self . elasticsearch_configuration_directory , install_directory = self . elasticsearch_install_directory , log_directory = self . elasticsearch_log_directory , stdout = self . stdout , verbose = self . verbose ) . setup ( node_name = utilities . get_default_es_node_name (), network_host = utilities . get_primary_ip_address (), port = 9200 , heap_size_gigs = es_heap_size_gigs ) if ls_install : if not ( self . logstash_install_directory and self . logstash_configuration_directory and self . logstash_log_directory ): self . logger . error ( 'You must specify logstash-configuration-directory, logstash-install-directory, ' 'and logstash-log-directory.' ) return None logstash_install . InstallManager ( configuration_directory = self . logstash_configuration_directory , install_directory = self . logstash_install_directory , log_directory = self . logstash_log_directory , stdout = self . stdout , verbose = self . verbose ) . setup ( node_name = utilities . get_default_es_node_name () . replace ( 'es' , 'ls' ), elasticsearch_host = utilities . get_primary_ip_address (), elasticsearch_port = 9200 , heap_size_gigs = es_heap_size_gigs ) if kb_install : from dynamite_nsm.services.elasticsearch import process as elasticsearch_process if not ( self . kibana_install_directory and self . kibana_configuration_directory and self . kibana_log_directory ): self . logger . error ( 'You must specify kibana-configuration-directory, kibana-install-directory, ' 'and kibana-log-directory.' ) return None self . logger . info ( 'Starting Elasticsearch.' ) elasticsearch_process . ProcessManager () . start () kibana_install . InstallManager ( configuration_directory = self . kibana_configuration_directory , install_directory = self . kibana_install_directory , log_directory = self . kibana_log_directory , stdout = self . stdout , verbose = self . verbose ) . setup ( host = utilities . get_primary_ip_address (), port = 5601 , elasticsearch_targets = [ f 'https:// { utilities . get_primary_ip_address () } :9200' ]) UninstallManager __init__ ( self , stdout = False , verbose = False ) special Uninstall Elasticsearch, Logstash, and Kibana Parameters: Name Type Description Default stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/monitor/install.py def __init__ ( self , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Elasticsearch, Logstash, and Kibana Args: stdout: Print output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( directories = [], name = 'monitor.uninstall' , stdout = stdout , verbose = verbose ) uninstall ( self ) Stop and uninstall the service Returns: Type Description None Source code in dynamite_nsm/services/monitor/install.py def uninstall ( self ): from dynamite_nsm.services.elasticsearch import profile as elasticsearch_profile from dynamite_nsm.services.logstash import profile as logstash_profile from dynamite_nsm.services.kibana import profile as kibana_profile if elasticsearch_profile . ProcessProfiler () . is_installed (): elasticsearch_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall () if logstash_profile . ProcessProfiler () . is_installed (): logstash_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall () if kibana_profile . ProcessProfiler () . is_installed (): kibana_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall ()","title":"install"},{"location":"guides/developers/SDK/services/monitor/install/#dynamite_nsm.services.monitor.install.InstallManager","text":"","title":"InstallManager"},{"location":"guides/developers/SDK/services/monitor/install/#dynamite_nsm.services.monitor.install.InstallManager.__init__","text":"Install Elaticsearch, Logstash, and Kibana Parameters: Name Type Description Default elasticsearch_install_directory str Path to the elasticsearch install directory (E.G /opt/dynamite/elasticsearch/) required elasticsearch_configuration_directory Optional[str] Path to the elasticsearch configuration directory (E.G /etc/dynamite/elasticsearch/) None elasticsearch_log_directory Optional[str] Path to the elasticsearch log directory (E.G /var/log/dynamite/elasticsearch/) None logstash_install_directory Optional[str] Path to the logstash install directory (E.G /opt/dynamite/logstash/) None logstash_configuration_directory Optional[str] Path to the logstash configuration directory (E.G /etc/dynamite/logstash/) None logstash_log_directory Optional[str] Path to the log directory (E.G /var/log/dynamite/logstash/) None kibana_install_directory Optional[str] Path to the kibana install directory (E.G /opt/dynamite/kibana/) None kibana_configuration_directory Optional[str] Path to the kibana configuration directory (E.G /etc/dynamite/kibana/) None kibana_log_directory Optional[str] Path to the kibana configuration directory (E.G /var/log/dynamite/kibana/) None stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/monitor/install.py def __init__ ( self , elasticsearch_install_directory : str , elasticsearch_configuration_directory : Optional [ str ] = None , elasticsearch_log_directory : Optional [ str ] = None , logstash_install_directory : Optional [ str ] = None , logstash_configuration_directory : Optional [ str ] = None , logstash_log_directory : Optional [ str ] = None , kibana_install_directory : Optional [ str ] = None , kibana_configuration_directory : Optional [ str ] = None , kibana_log_directory : Optional [ str ] = None , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Elaticsearch, Logstash, and Kibana Args: elasticsearch_install_directory: Path to the elasticsearch install directory (E.G /opt/dynamite/elasticsearch/) elasticsearch_configuration_directory: Path to the elasticsearch configuration directory (E.G /etc/dynamite/elasticsearch/) elasticsearch_log_directory: Path to the elasticsearch log directory (E.G /var/log/dynamite/elasticsearch/) logstash_install_directory: Path to the logstash install directory (E.G /opt/dynamite/logstash/) logstash_configuration_directory: Path to the logstash configuration directory (E.G /etc/dynamite/logstash/) logstash_log_directory: Path to the log directory (E.G /var/log/dynamite/logstash/) kibana_install_directory: Path to the kibana install directory (E.G /opt/dynamite/kibana/) kibana_configuration_directory: Path to the kibana configuration directory (E.G /etc/dynamite/kibana/) kibana_log_directory: Path to the kibana configuration directory (E.G /var/log/dynamite/kibana/) stdout: Print output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( 'monitor.install' , stdout = stdout , verbose = verbose ) self . elasticsearch_install_directory = elasticsearch_install_directory self . elasticsearch_configuration_directory = elasticsearch_configuration_directory self . elasticsearch_log_directory = elasticsearch_log_directory self . logstash_install_directory = logstash_install_directory self . logstash_configuration_directory = logstash_configuration_directory self . logstash_log_directory = logstash_log_directory self . kibana_install_directory = kibana_install_directory self . kibana_configuration_directory = kibana_configuration_directory self . kibana_log_directory = kibana_log_directory","title":"__init__()"},{"location":"guides/developers/SDK/services/monitor/install/#dynamite_nsm.services.monitor.install.InstallManager.setup","text":"Setup Elasticsearch, Logstash, and Kibana Returns: Type Description None Source code in dynamite_nsm/services/monitor/install.py def setup ( self ): \"\"\"Setup Elasticsearch, Logstash, and Kibana Returns: None \"\"\" es_install = self . elasticsearch_install_directory or self . elasticsearch_configuration_directory or \\ self . elasticsearch_log_directory ls_install = self . logstash_install_directory or self . logstash_configuration_directory or \\ self . logstash_log_directory kb_install = self . kibana_install_directory or self . kibana_configuration_directory or self . kibana_log_directory # Determine how much heap space to pre-allocate to Elasticsearch and Logstash one or both are # specified for installation reserved_memory = utilities . get_memory_available_bytes () * .75 heap_size_gigs = int (( reserved_memory / 10 ** 9 ) / 2 ) ls_heap_size_gigs , es_heap_size_gigs = heap_size_gigs , heap_size_gigs if es_install and ls_install : ls_reserved_memory = utilities . get_memory_available_bytes () * .25 ls_heap_size_gigs = int (( ls_reserved_memory / 10 ** 9 ) / 2 ) es_reserved_memory = utilities . get_memory_available_bytes () * .50 es_heap_size_gigs = int (( es_reserved_memory / 10 ** 9 ) / 2 ) if es_install : if not ( self . elasticsearch_install_directory and self . elasticsearch_configuration_directory and self . elasticsearch_log_directory ): self . logger . error ( 'You must specify elasticsearch-configuration-directory, elasticsearch-install-directory, ' 'and elasticsearch-log-directory.' ) return None elasticsearch_install . InstallManager ( configuration_directory = self . elasticsearch_configuration_directory , install_directory = self . elasticsearch_install_directory , log_directory = self . elasticsearch_log_directory , stdout = self . stdout , verbose = self . verbose ) . setup ( node_name = utilities . get_default_es_node_name (), network_host = utilities . get_primary_ip_address (), port = 9200 , heap_size_gigs = es_heap_size_gigs ) if ls_install : if not ( self . logstash_install_directory and self . logstash_configuration_directory and self . logstash_log_directory ): self . logger . error ( 'You must specify logstash-configuration-directory, logstash-install-directory, ' 'and logstash-log-directory.' ) return None logstash_install . InstallManager ( configuration_directory = self . logstash_configuration_directory , install_directory = self . logstash_install_directory , log_directory = self . logstash_log_directory , stdout = self . stdout , verbose = self . verbose ) . setup ( node_name = utilities . get_default_es_node_name () . replace ( 'es' , 'ls' ), elasticsearch_host = utilities . get_primary_ip_address (), elasticsearch_port = 9200 , heap_size_gigs = es_heap_size_gigs ) if kb_install : from dynamite_nsm.services.elasticsearch import process as elasticsearch_process if not ( self . kibana_install_directory and self . kibana_configuration_directory and self . kibana_log_directory ): self . logger . error ( 'You must specify kibana-configuration-directory, kibana-install-directory, ' 'and kibana-log-directory.' ) return None self . logger . info ( 'Starting Elasticsearch.' ) elasticsearch_process . ProcessManager () . start () kibana_install . InstallManager ( configuration_directory = self . kibana_configuration_directory , install_directory = self . kibana_install_directory , log_directory = self . kibana_log_directory , stdout = self . stdout , verbose = self . verbose ) . setup ( host = utilities . get_primary_ip_address (), port = 5601 , elasticsearch_targets = [ f 'https:// { utilities . get_primary_ip_address () } :9200' ])","title":"setup()"},{"location":"guides/developers/SDK/services/monitor/install/#dynamite_nsm.services.monitor.install.UninstallManager","text":"","title":"UninstallManager"},{"location":"guides/developers/SDK/services/monitor/install/#dynamite_nsm.services.monitor.install.UninstallManager.__init__","text":"Uninstall Elasticsearch, Logstash, and Kibana Parameters: Name Type Description Default stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/monitor/install.py def __init__ ( self , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Elasticsearch, Logstash, and Kibana Args: stdout: Print output to console verbose: Include detailed debug messages \"\"\" super () . __init__ ( directories = [], name = 'monitor.uninstall' , stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/monitor/install/#dynamite_nsm.services.monitor.install.UninstallManager.uninstall","text":"Stop and uninstall the service Returns: Type Description None Source code in dynamite_nsm/services/monitor/install.py def uninstall ( self ): from dynamite_nsm.services.elasticsearch import profile as elasticsearch_profile from dynamite_nsm.services.logstash import profile as logstash_profile from dynamite_nsm.services.kibana import profile as kibana_profile if elasticsearch_profile . ProcessProfiler () . is_installed (): elasticsearch_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall () if logstash_profile . ProcessProfiler () . is_installed (): logstash_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall () if kibana_profile . ProcessProfiler () . is_installed (): kibana_install . UninstallManager ( purge_config = True , stdout = self . stdout , verbose = self . verbose ) . uninstall ()","title":"uninstall()"},{"location":"guides/developers/SDK/services/monitor/process/","text":"A simple process manager wrapping Elasticsearch, Kibana, and Logstash (if installed). To import... from dynamite_nsm.services.agent import process as agent_process ProcessManager Agent Process Manager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Agent Processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/agent/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Agent Processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" self . stdout = stdout self . verbose = verbose self . pretty_print_status = pretty_print_status log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( 'agent.process' , level = log_level , stdout = stdout ) start ( self ) Start agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def start ( self ) -> bool : \"\"\"Start agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . start () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . start () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . start () return filebeat_res and zeek_res and suricata_res status ( self ) Get the status of a processes Returns: Type Description Union[Dict, str] A dictionary containing process status or a tabulated string if pretty_print is True. Source code in dynamite_nsm/services/agent/process.py def status ( self ) -> Optional [ Union [ Dict , str ]]: \"\"\"Get the status of a processes Returns: A dictionary containing process status or a tabulated string if `pretty_print` is True. \"\"\" if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install filebeat to run this command.' ) return None agent_status = {} filebeat_status , zeek_status , suricata_status = {}, {}, {} filebeat_status = filebeat_process . ProcessManager () . status () agent_status . update ({ 'filebeat' : { 'running' : filebeat_status . get ( 'running' ), 'enabled_on_startup' : filebeat_status . get ( 'enabled_on_startup' )}}) if zeek_profile . ProcessProfiler () . is_installed (): zeek_status = zeek_process . ProcessManager () . status () agent_status . update ({ 'zeek' : { 'running' : zeek_status . get ( 'running' ), 'enabled_on_startup' : zeek_status . get ( 'enabled_on_startup' )}}) if suricata_profile . ProcessProfiler () . is_installed (): suricata_status = suricata_process . ProcessManager () . status () agent_status . update ({ 'suricata' : { 'running' : suricata_status . get ( 'running' ), 'enabled_on_startup' : suricata_status . get ( 'enabled_on_startup' )}}) if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize child_services = [ [ 'Service' , 'Running' , 'Enabled on Startup' ], [ 'filebeat' , colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' ) ] ] if zeek_status : child_services . append ( [ 'zeek' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) if suricata_status : child_services . append ( [ 'suricata' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) return tabulate . tabulate ( child_services , tablefmt = 'fancy_grid' ) return agent_status stop ( self ) Stop agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def stop ( self ) -> bool : \"\"\"Stop agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . stop () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . stop () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . stop () return filebeat_res and zeek_res and suricata_res","title":"process"},{"location":"guides/developers/SDK/services/monitor/process/#dynamite_nsm.services.agent.process.ProcessManager","text":"Agent Process Manager","title":"ProcessManager"},{"location":"guides/developers/SDK/services/monitor/process/#dynamite_nsm.services.agent.process.ProcessManager.__init__","text":"Manage Agent Processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If enabled, status will be printed in a tabulated style False Returns: Type Description None Source code in dynamite_nsm/services/agent/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Agent Processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If enabled, status will be printed in a tabulated style Returns: None \"\"\" self . stdout = stdout self . verbose = verbose self . pretty_print_status = pretty_print_status log_level = logging . INFO if verbose : log_level = logging . DEBUG self . logger = get_logger ( 'agent.process' , level = log_level , stdout = stdout )","title":"__init__()"},{"location":"guides/developers/SDK/services/monitor/process/#dynamite_nsm.services.agent.process.ProcessManager.start","text":"Start agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def start ( self ) -> bool : \"\"\"Start agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . start () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . start () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . start () return filebeat_res and zeek_res and suricata_res","title":"start()"},{"location":"guides/developers/SDK/services/monitor/process/#dynamite_nsm.services.agent.process.ProcessManager.status","text":"Get the status of a processes Returns: Type Description Union[Dict, str] A dictionary containing process status or a tabulated string if pretty_print is True. Source code in dynamite_nsm/services/agent/process.py def status ( self ) -> Optional [ Union [ Dict , str ]]: \"\"\"Get the status of a processes Returns: A dictionary containing process status or a tabulated string if `pretty_print` is True. \"\"\" if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install filebeat to run this command.' ) return None agent_status = {} filebeat_status , zeek_status , suricata_status = {}, {}, {} filebeat_status = filebeat_process . ProcessManager () . status () agent_status . update ({ 'filebeat' : { 'running' : filebeat_status . get ( 'running' ), 'enabled_on_startup' : filebeat_status . get ( 'enabled_on_startup' )}}) if zeek_profile . ProcessProfiler () . is_installed (): zeek_status = zeek_process . ProcessManager () . status () agent_status . update ({ 'zeek' : { 'running' : zeek_status . get ( 'running' ), 'enabled_on_startup' : zeek_status . get ( 'enabled_on_startup' )}}) if suricata_profile . ProcessProfiler () . is_installed (): suricata_status = suricata_process . ProcessManager () . status () agent_status . update ({ 'suricata' : { 'running' : suricata_status . get ( 'running' ), 'enabled_on_startup' : suricata_status . get ( 'enabled_on_startup' )}}) if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize child_services = [ [ 'Service' , 'Running' , 'Enabled on Startup' ], [ 'filebeat' , colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if filebeat_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' ) ] ] if zeek_status : child_services . append ( [ 'zeek' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) if suricata_status : child_services . append ( [ 'suricata' , colorize ( 'yes' , 'green' ) if zeek_status . get ( 'running' ) else colorize ( 'no' , 'red' ), colorize ( 'yes' , 'green' ) if zeek_status . get ( 'enabled_on_startup' ) else colorize ( 'no' , 'red' )] ) return tabulate . tabulate ( child_services , tablefmt = 'fancy_grid' ) return agent_status","title":"status()"},{"location":"guides/developers/SDK/services/monitor/process/#dynamite_nsm.services.agent.process.ProcessManager.stop","text":"Stop agent processes Returns: Type Description bool True, if successful Source code in dynamite_nsm/services/agent/process.py def stop ( self ) -> bool : \"\"\"Stop agent processes Returns: True, if successful \"\"\" filebeat_res , suricata_res , zeek_res = True , True , True if not filebeat_profile . ProcessProfiler () . is_installed (): self . logger . error ( 'You must install Filebeat to run this command.' ) return False filebeat_res = filebeat_process . ProcessManager () . stop () if suricata_profile . ProcessProfiler () . is_installed (): suricata_res = suricata_process . ProcessManager () . stop () if zeek_profile . ProcessProfiler () . is_installed (): zeek_res = zeek_process . ProcessManager () . stop () return filebeat_res and zeek_res and suricata_res","title":"stop()"},{"location":"guides/developers/SDK/services/suricata/config/","text":"Configuration wrappers for the main Suricata configuration file. To import... from dynamite_nsm.services.suricata import config as suricata_config ConfigManager Manage Suricata.yaml configuration __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Configuration Manager for suricata.yaml file Parameters: Name Type Description Default configuration_directory str The path to the Suricata configuration directory (E.G /etc/dynamite/suricata) required Instance Variables: Directories and Files: suricata_log_output_file - Directory where logs are written default_rules_directory - Directory where rules live classification_file - The file (path) that maps severity to various [class]types (classification.config) reference_config_file - The file (path) to the reference.config file Network Interface Setup: af_packet_interfaces - A list of misc.AfPacketInterfaces representing suricata monitored interfaces pcap_interfaces - A list of misc.PcapInterfaces (libpcap support if af_packet isn't possible) Rules: rule_files - A list of suricata rules.Rules (rulesets) Address Groups: See syntax. home_net external_net http_servers sql_servers dns_servers telnet_servers aim_servers dc_servers dnp3_servers modbus_servers enip_server Port Groups: See syntax. http_ports shellcode_ports oracle_ports ssh_ports dnp3_ports modbus_ports file_data_ports ftp_ports Source code in dynamite_nsm/services/suricata/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configuration Manager for suricata.yaml file Args: configuration_directory: The path to the Suricata configuration directory (E.G /etc/dynamite/suricata) ___ # Instance Variables: ## Directories and Files: - `suricata_log_output_file` - Directory where logs are written - `default_rules_directory` - Directory where rules live - `classification_file` - The file (path) that maps severity to various [class]types (classification.config) - `reference_config_file` - The file (path) to the reference.config file ## Network Interface Setup: - `af_packet_interfaces` - A `list` of `misc.AfPacketInterfaces` representing suricata monitored interfaces - `pcap_interfaces` - A `list` of `misc.PcapInterfaces` (libpcap support if af_packet isn't possible) ## Rules: - `rule_files` - A `list` of suricata `rules.Rules` (rulesets) ## Address Groups: > <sup>[See syntax.](https://suricata.readthedocs.io/en/suricata-6.0.0/configuration/suricata-yaml.html#rule-vars)</sup> - `home_net` - `external_net` - `http_servers` - `sql_servers` - `dns_servers` - `telnet_servers` - `aim_servers` - `dc_servers` - `dnp3_servers` - `modbus_servers` - `enip_server` ## Port Groups: > <sup>[See syntax.](https://suricata.readthedocs.io/en/suricata-6.0.0/configuration/suricata-yaml.html#rule-vars)</sup> - `http_ports` - `shellcode_ports` - `oracle_ports` - `ssh_ports` - `dnp3_ports` - `modbus_ports` - `file_data_ports` - `ftp_ports` \"\"\" extract_tokens = { 'runmode' : ( 'runmode' ,), 'home_net' : ( 'vars' , 'address-groups' , 'HOME_NET' ), 'external_net' : ( 'vars' , 'address-groups' , 'EXTERNAL_NET' ), 'http_servers' : ( 'vars' , 'address-groups' , 'HTTP_SERVERS' ), 'sql_servers' : ( 'vars' , 'address-groups' , 'SQL_SERVERS' ), 'dns_servers' : ( 'vars' , 'address-groups' , 'DNS_SERVERS' ), 'telnet_servers' : ( 'vars' , 'address-groups' , 'TELNET_SERVERS' ), 'aim_servers' : ( 'vars' , 'address-groups' , 'AIM_SERVERS' ), 'dc_servers' : ( 'vars' , 'address-groups' , 'DC_SERVERS' ), 'dnp3_servers' : ( 'vars' , 'address-groups' , 'DNP3_SERVERS' ), 'modbus_client' : ( 'vars' , 'address-groups' , 'MODBUS_CLIENT' ), 'modbus_server' : ( 'vars' , 'address-groups' , 'MODBUS_SERVER' ), 'enip_client' : ( 'vars' , 'address-groups' , 'ENIP_CLIENT' ), 'enip_server' : ( 'vars' , 'address-groups' , 'ENIP_SERVER' ), 'http_ports' : ( 'vars' , 'port-groups' , 'HTTP_PORTS' ), 'shellcode_ports' : ( 'vars' , 'port-groups' , 'SHELLCODE_PORTS' ), 'oracle_ports' : ( 'vars' , 'port-groups' , 'ORACLE_PORTS' ), 'ssh_ports' : ( 'vars' , 'port-groups' , 'SSH_PORTS' ), 'dnp3_ports' : ( 'vars' , 'port-groups' , 'DNP3_PORTS' ), 'modbus_ports' : ( 'vars' , 'port-groups' , 'MODBUS_PORTS' ), 'file_data_ports' : ( 'vars' , 'port-groups' , 'FILE_DATA_PORTS' ), 'ftp_ports' : ( 'vars' , 'port-groups' , 'FTP_PORTS' ), 'default_log_directory' : ( 'default-log-dir' ,), 'suricata_log_output_file' : ( 'logging' , 'outputs' , 'file' , 'filename' ), 'default_rules_directory' : ( 'default-rule-path' ,), 'classification_file' : ( 'classification-file' ,), 'reference_config_file' : ( 'reference-config-file' ,), '_af_packet_interfaces_raw' : ( 'af-packet' ,), '_rule_files_raw' : ( 'rule-files' ,), '_threading_raw' : ( 'threading' ,) } self . configuration_directory = configuration_directory self . config_data = None self . runmode = None self . home_net = None self . external_net = None self . http_servers = None self . sql_servers = None self . dns_servers = None self . telnet_servers = None self . aim_servers = None self . dc_servers = None self . modbus_client = None self . modbus_server = None self . enip_client = None self . enip_server = None self . http_ports = None self . shellcode_ports = None self . oracle_ports = None self . ssh_ports = None self . dnp3_ports = None self . modbus_ports = None self . ftp_ports = None self . file_data_ports = None self . default_log_directory = None self . suricata_log_output_file = None self . default_rules_directory = None self . classification_file = None self . reference_config_file = None self . _af_packet_interfaces_raw = [] self . _rule_files_raw = [] self . _threading_raw = {} self . suricata_config_file = os . path . join ( self . configuration_directory , 'suricata.yaml' ) try : with open ( self . suricata_config_file , 'r' ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) except ( IOError , ValueError ): raise general_exceptions . ReadConfigError ( f 'Failed to read or parse { self . suricata_config_file } .' ) super () . __init__ ( self . config_data_raw , name = 'suricata.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () self . rules = rules . Rules () for rule_name in self . list_available_rule_names (): if rule_name in self . _rule_files_raw : self . rules . add ( rules . Rule ( rule_name , enabled = True )) else : self . rules . add ( rules . Rule ( rule_name , enabled = False )) self . af_packet_interfaces = misc . AfPacketInterfaces ( [ misc . AfPacketInterface ( cluster_id = af_packet_interface_raw . get ( 'cluster-id' ), cluster_type = af_packet_interface_raw . get ( 'cluster-type' ), interface_name = af_packet_interface_raw . get ( 'interface' ), bpf_filter = af_packet_interface_raw . get ( 'bpf-filter' ), threads = af_packet_interface_raw . get ( 'threads' ) ) for af_packet_interface_raw in self . _af_packet_interfaces_raw ] ) thread_families = self . _threading_raw . get ( 'cpu-affinity' , []) management_cpu_set , receive_cpu_set , worker_cpu_set = None , None , None for thread_family in thread_families : if 'management-cpu-set' in thread_family . keys (): management_cpu_set = thread_family . get ( 'management-cpu-set' , {}) . get ( 'cpu' , []) elif 'receive-cpu-set' in thread_family . keys (): receive_cpu_set = thread_family . get ( 'receive-cpu-set' , {}) . get ( 'cpu' , []) elif 'worker-cpu-set' in thread_family . keys (): worker_cpu_set = thread_family . get ( 'worker-cpu-set' , {}) . get ( 'cpu' , []) self . threading = misc . Threading ( management_cpu_set , receive_cpu_set , worker_cpu_set ) commit ( self , out_file_path = None , backup_directory = None , top_text = None ) Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] If specified, the first line of the configuration file will be set to the value of your choosing. None Returns: Type Description None None Source code in dynamite_nsm/services/suricata/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: If specified, the first line of the configuration file will be set to the value of your choosing. Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /suricata.yaml' if not top_text : top_text = '%YAML 1.1 \\n ---' self . _rule_files_raw = self . rules . get_raw () self . _af_packet_interfaces_raw = self . af_packet_interfaces . get_raw () self . _threading_raw = self . threading . get_raw () super ( ConfigManager , self ) . commit ( out_file_path , backup_directory , top_text = top_text ) from_raw_text ( raw_text , configuration_directory = None ) classmethod Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required configuration_directory Optional[str] The configuration directory for Suricata None Returns: Type Description ConfigManager An instance of ConfigManager Source code in dynamite_nsm/services/suricata/config.py @classmethod def from_raw_text ( cls , raw_text : str , configuration_directory : Optional [ str ] = None ) -> ConfigManager : \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file configuration_directory: The configuration directory for Suricata Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp' tmp_config = f ' { tmp_dir } /suricata.yaml' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( configuration_directory = tmp_dir ) if configuration_directory : c . configuration_directory = configuration_directory return c list_available_rule_names ( self ) List the names of all available Suricata rules. Returns: Type Description List[str] A list of Suricata rule names that can be enabled Source code in dynamite_nsm/services/suricata/config.py def list_available_rule_names ( self ) -> List [ str ]: \"\"\"List the names of all available Suricata rules. Returns: A list of Suricata rule names that can be enabled \"\"\" return [ rule for rule in os . listdir ( f ' { self . configuration_directory } /rules' ) if rule . endswith ( '.rules' )] reset ( self , inspect_interfaces , out_file_path = None , default_config_path = None ) Reset a configuration file back to its default Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/suricata/config.py def reset ( self , inspect_interfaces : List [ str ], out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not install . BaseInstallManager . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) if not out_file_path : out_file_path = f ' { self . configuration_directory } /suricata.yaml' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /suricata/suricata.yaml' super ( ConfigManager , self ) . reset ( out_file_path , default_config_path ) self . af_packet_interfaces = misc . AfPacketInterfaces () for interface in inspect_interfaces : self . af_packet_interfaces . add ( misc . AfPacketInterface ( interface_name = interface , threads = 'auto' , cluster_id = random . randint ( 1 , 50000 ), cluster_type = 'cluster_qm' ) ) self . commit ( out_file_path = out_file_path ) lookup_rule_definition ( rule_id ) Return the definition, categories, and friendly_name of a given script Parameters: Name Type Description Default rule_id str A unique identifier representing a Suricata rule. required Returns: Type Description Dict A dictionary of the format {\"friendly_name\" , \"description\" , \"categories\" } Source code in dynamite_nsm/services/suricata/config.py def lookup_rule_definition ( rule_id : str ) -> Dict : \"\"\"Return the definition, categories, and friendly_name of a given script Args: rule_id: A unique identifier representing a Suricata rule. Returns: A dictionary of the format {\"friendly_name\" <str>, \"description\" <str>, \"categories\" <list>} \"\"\" try : suricata_rule_defs = os . path . join ( const . DEFAULT_CONFIGS , 'suricata' , 'suricata_rule_definitions.json' ) with open ( suricata_rule_defs ) as f : suricata_defs = json . load ( f ) except FileNotFoundError : suricata_defs = {} definition = suricata_defs . get ( str ( rule_id )) return definition","title":"config"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager","text":"Manage Suricata.yaml configuration","title":"ConfigManager"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__","text":"Configuration Manager for suricata.yaml file Parameters: Name Type Description Default configuration_directory str The path to the Suricata configuration directory (E.G /etc/dynamite/suricata) required","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__--instance-variables","text":"","title":"Instance Variables:"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__--directories-and-files","text":"suricata_log_output_file - Directory where logs are written default_rules_directory - Directory where rules live classification_file - The file (path) that maps severity to various [class]types (classification.config) reference_config_file - The file (path) to the reference.config file","title":"Directories and Files:"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__--network-interface-setup","text":"af_packet_interfaces - A list of misc.AfPacketInterfaces representing suricata monitored interfaces pcap_interfaces - A list of misc.PcapInterfaces (libpcap support if af_packet isn't possible)","title":"Network Interface Setup:"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__--rules","text":"rule_files - A list of suricata rules.Rules (rulesets)","title":"Rules:"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__--address-groups","text":"See syntax. home_net external_net http_servers sql_servers dns_servers telnet_servers aim_servers dc_servers dnp3_servers modbus_servers enip_server","title":"Address Groups:"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.__init__--port-groups","text":"See syntax. http_ports shellcode_ports oracle_ports ssh_ports dnp3_ports modbus_ports file_data_ports ftp_ports Source code in dynamite_nsm/services/suricata/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configuration Manager for suricata.yaml file Args: configuration_directory: The path to the Suricata configuration directory (E.G /etc/dynamite/suricata) ___ # Instance Variables: ## Directories and Files: - `suricata_log_output_file` - Directory where logs are written - `default_rules_directory` - Directory where rules live - `classification_file` - The file (path) that maps severity to various [class]types (classification.config) - `reference_config_file` - The file (path) to the reference.config file ## Network Interface Setup: - `af_packet_interfaces` - A `list` of `misc.AfPacketInterfaces` representing suricata monitored interfaces - `pcap_interfaces` - A `list` of `misc.PcapInterfaces` (libpcap support if af_packet isn't possible) ## Rules: - `rule_files` - A `list` of suricata `rules.Rules` (rulesets) ## Address Groups: > <sup>[See syntax.](https://suricata.readthedocs.io/en/suricata-6.0.0/configuration/suricata-yaml.html#rule-vars)</sup> - `home_net` - `external_net` - `http_servers` - `sql_servers` - `dns_servers` - `telnet_servers` - `aim_servers` - `dc_servers` - `dnp3_servers` - `modbus_servers` - `enip_server` ## Port Groups: > <sup>[See syntax.](https://suricata.readthedocs.io/en/suricata-6.0.0/configuration/suricata-yaml.html#rule-vars)</sup> - `http_ports` - `shellcode_ports` - `oracle_ports` - `ssh_ports` - `dnp3_ports` - `modbus_ports` - `file_data_ports` - `ftp_ports` \"\"\" extract_tokens = { 'runmode' : ( 'runmode' ,), 'home_net' : ( 'vars' , 'address-groups' , 'HOME_NET' ), 'external_net' : ( 'vars' , 'address-groups' , 'EXTERNAL_NET' ), 'http_servers' : ( 'vars' , 'address-groups' , 'HTTP_SERVERS' ), 'sql_servers' : ( 'vars' , 'address-groups' , 'SQL_SERVERS' ), 'dns_servers' : ( 'vars' , 'address-groups' , 'DNS_SERVERS' ), 'telnet_servers' : ( 'vars' , 'address-groups' , 'TELNET_SERVERS' ), 'aim_servers' : ( 'vars' , 'address-groups' , 'AIM_SERVERS' ), 'dc_servers' : ( 'vars' , 'address-groups' , 'DC_SERVERS' ), 'dnp3_servers' : ( 'vars' , 'address-groups' , 'DNP3_SERVERS' ), 'modbus_client' : ( 'vars' , 'address-groups' , 'MODBUS_CLIENT' ), 'modbus_server' : ( 'vars' , 'address-groups' , 'MODBUS_SERVER' ), 'enip_client' : ( 'vars' , 'address-groups' , 'ENIP_CLIENT' ), 'enip_server' : ( 'vars' , 'address-groups' , 'ENIP_SERVER' ), 'http_ports' : ( 'vars' , 'port-groups' , 'HTTP_PORTS' ), 'shellcode_ports' : ( 'vars' , 'port-groups' , 'SHELLCODE_PORTS' ), 'oracle_ports' : ( 'vars' , 'port-groups' , 'ORACLE_PORTS' ), 'ssh_ports' : ( 'vars' , 'port-groups' , 'SSH_PORTS' ), 'dnp3_ports' : ( 'vars' , 'port-groups' , 'DNP3_PORTS' ), 'modbus_ports' : ( 'vars' , 'port-groups' , 'MODBUS_PORTS' ), 'file_data_ports' : ( 'vars' , 'port-groups' , 'FILE_DATA_PORTS' ), 'ftp_ports' : ( 'vars' , 'port-groups' , 'FTP_PORTS' ), 'default_log_directory' : ( 'default-log-dir' ,), 'suricata_log_output_file' : ( 'logging' , 'outputs' , 'file' , 'filename' ), 'default_rules_directory' : ( 'default-rule-path' ,), 'classification_file' : ( 'classification-file' ,), 'reference_config_file' : ( 'reference-config-file' ,), '_af_packet_interfaces_raw' : ( 'af-packet' ,), '_rule_files_raw' : ( 'rule-files' ,), '_threading_raw' : ( 'threading' ,) } self . configuration_directory = configuration_directory self . config_data = None self . runmode = None self . home_net = None self . external_net = None self . http_servers = None self . sql_servers = None self . dns_servers = None self . telnet_servers = None self . aim_servers = None self . dc_servers = None self . modbus_client = None self . modbus_server = None self . enip_client = None self . enip_server = None self . http_ports = None self . shellcode_ports = None self . oracle_ports = None self . ssh_ports = None self . dnp3_ports = None self . modbus_ports = None self . ftp_ports = None self . file_data_ports = None self . default_log_directory = None self . suricata_log_output_file = None self . default_rules_directory = None self . classification_file = None self . reference_config_file = None self . _af_packet_interfaces_raw = [] self . _rule_files_raw = [] self . _threading_raw = {} self . suricata_config_file = os . path . join ( self . configuration_directory , 'suricata.yaml' ) try : with open ( self . suricata_config_file , 'r' ) as configyaml : self . config_data_raw = load ( configyaml , Loader = Loader ) except ( IOError , ValueError ): raise general_exceptions . ReadConfigError ( f 'Failed to read or parse { self . suricata_config_file } .' ) super () . __init__ ( self . config_data_raw , name = 'suricata.config' , verbose = verbose , stdout = stdout , ** extract_tokens ) self . parse_yaml_file () self . rules = rules . Rules () for rule_name in self . list_available_rule_names (): if rule_name in self . _rule_files_raw : self . rules . add ( rules . Rule ( rule_name , enabled = True )) else : self . rules . add ( rules . Rule ( rule_name , enabled = False )) self . af_packet_interfaces = misc . AfPacketInterfaces ( [ misc . AfPacketInterface ( cluster_id = af_packet_interface_raw . get ( 'cluster-id' ), cluster_type = af_packet_interface_raw . get ( 'cluster-type' ), interface_name = af_packet_interface_raw . get ( 'interface' ), bpf_filter = af_packet_interface_raw . get ( 'bpf-filter' ), threads = af_packet_interface_raw . get ( 'threads' ) ) for af_packet_interface_raw in self . _af_packet_interfaces_raw ] ) thread_families = self . _threading_raw . get ( 'cpu-affinity' , []) management_cpu_set , receive_cpu_set , worker_cpu_set = None , None , None for thread_family in thread_families : if 'management-cpu-set' in thread_family . keys (): management_cpu_set = thread_family . get ( 'management-cpu-set' , {}) . get ( 'cpu' , []) elif 'receive-cpu-set' in thread_family . keys (): receive_cpu_set = thread_family . get ( 'receive-cpu-set' , {}) . get ( 'cpu' , []) elif 'worker-cpu-set' in thread_family . keys (): worker_cpu_set = thread_family . get ( 'worker-cpu-set' , {}) . get ( 'cpu' , []) self . threading = misc . Threading ( management_cpu_set , receive_cpu_set , worker_cpu_set )","title":"Port Groups:"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.commit","text":"Write out an updated configuration file, and optionally backup the old one. Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file; if none given overwrites existing None backup_directory Optional[str] The path to the backup directory None top_text Optional[str] If specified, the first line of the configuration file will be set to the value of your choosing. None Returns: Type Description None None Source code in dynamite_nsm/services/suricata/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None , top_text : Optional [ str ] = None ) -> None : \"\"\"Write out an updated configuration file, and optionally backup the old one. Args: out_file_path: The path to the output file; if none given overwrites existing backup_directory: The path to the backup directory top_text: If specified, the first line of the configuration file will be set to the value of your choosing. Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /suricata.yaml' if not top_text : top_text = '%YAML 1.1 \\n ---' self . _rule_files_raw = self . rules . get_raw () self . _af_packet_interfaces_raw = self . af_packet_interfaces . get_raw () self . _threading_raw = self . threading . get_raw () super ( ConfigManager , self ) . commit ( out_file_path , backup_directory , top_text = top_text )","title":"commit()"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.from_raw_text","text":"Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required configuration_directory Optional[str] The configuration directory for Suricata None Returns: Type Description ConfigManager An instance of ConfigManager Source code in dynamite_nsm/services/suricata/config.py @classmethod def from_raw_text ( cls , raw_text : str , configuration_directory : Optional [ str ] = None ) -> ConfigManager : \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file configuration_directory: The configuration directory for Suricata Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp' tmp_config = f ' { tmp_dir } /suricata.yaml' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( configuration_directory = tmp_dir ) if configuration_directory : c . configuration_directory = configuration_directory return c","title":"from_raw_text()"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.list_available_rule_names","text":"List the names of all available Suricata rules. Returns: Type Description List[str] A list of Suricata rule names that can be enabled Source code in dynamite_nsm/services/suricata/config.py def list_available_rule_names ( self ) -> List [ str ]: \"\"\"List the names of all available Suricata rules. Returns: A list of Suricata rule names that can be enabled \"\"\" return [ rule for rule in os . listdir ( f ' { self . configuration_directory } /rules' ) if rule . endswith ( '.rules' )]","title":"list_available_rule_names()"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.ConfigManager.reset","text":"Reset a configuration file back to its default Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/suricata/config.py def reset ( self , inspect_interfaces : List [ str ], out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not install . BaseInstallManager . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) if not out_file_path : out_file_path = f ' { self . configuration_directory } /suricata.yaml' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /suricata/suricata.yaml' super ( ConfigManager , self ) . reset ( out_file_path , default_config_path ) self . af_packet_interfaces = misc . AfPacketInterfaces () for interface in inspect_interfaces : self . af_packet_interfaces . add ( misc . AfPacketInterface ( interface_name = interface , threads = 'auto' , cluster_id = random . randint ( 1 , 50000 ), cluster_type = 'cluster_qm' ) ) self . commit ( out_file_path = out_file_path )","title":"reset()"},{"location":"guides/developers/SDK/services/suricata/config/#dynamite_nsm.services.suricata.config.lookup_rule_definition","text":"Return the definition, categories, and friendly_name of a given script Parameters: Name Type Description Default rule_id str A unique identifier representing a Suricata rule. required Returns: Type Description Dict A dictionary of the format {\"friendly_name\" , \"description\" , \"categories\" } Source code in dynamite_nsm/services/suricata/config.py def lookup_rule_definition ( rule_id : str ) -> Dict : \"\"\"Return the definition, categories, and friendly_name of a given script Args: rule_id: A unique identifier representing a Suricata rule. Returns: A dictionary of the format {\"friendly_name\" <str>, \"description\" <str>, \"categories\" <list>} \"\"\" try : suricata_rule_defs = os . path . join ( const . DEFAULT_CONFIGS , 'suricata' , 'suricata_rule_definitions.json' ) with open ( suricata_rule_defs ) as f : suricata_defs = json . load ( f ) except FileNotFoundError : suricata_defs = {} definition = suricata_defs . get ( str ( rule_id )) return definition","title":"lookup_rule_definition()"},{"location":"guides/developers/SDK/services/suricata/install/","text":"Installation Manager for Suricata and its dependencies. To import... from dynamite_nsm.services.suricata import install as suricata_install InstallManager Manage Suricata installation process __init__ ( self , configuration_directory , install_directory , log_directory , download_suricata_archive = True , skip_interface_validation = False , stdout = False , verbose = False ) special Install Suricata Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/suricata) required install_directory str Path to the install directory (E.G /opt/dynamite/suricata/) required log_directory str Path to the log directory (E.G /var/log/dynamite/suricata/) required download_suricata_archive Optional[bool] If True, download the Suricata archive from a mirror True skip_interface_validation Optional[bool] If included we don't check if the interface is available on the system False stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/suricata/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_suricata_archive : Optional [ bool ] = True , skip_interface_validation : Optional [ bool ] = False , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Suricata Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/suricata) install_directory: Path to the install directory (E.G /opt/dynamite/suricata/) log_directory: Path to the log directory (E.G /var/log/dynamite/suricata/) download_suricata_archive: If True, download the Suricata archive from a mirror skip_interface_validation: If included we don't check if the interface is available on the system stdout: Print the output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . download_suricata_archive = download_suricata_archive self . skip_interface_validation = skip_interface_validation self . stdout = stdout self . verbose = verbose install . BaseInstallManager . __init__ ( self , 'suricata.install' , verbose = self . verbose , stdout = stdout ) if download_suricata_archive : self . logger . info ( \"Attempting to download Suricata archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . SURICATA_MIRRORS ) self . logger . info ( f 'Attempting to extract Suricata archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . SURICATA_MIRRORS ) configure_compile_suricata ( self , parallel_threads = None ) Configure and build Suricata from source Parameters: Name Type Description Default parallel_threads Optional[int] Number of parallel threads to use during the compiling process None Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def configure_compile_suricata ( self , parallel_threads : Optional [ int ] = None ) -> None : \"\"\"Configure and build Suricata from source Args: parallel_threads: Number of parallel threads to use during the compiling process Returns: None \"\"\" suricata_source_install_cache = os . path . join ( const . INSTALL_CACHE , self . local_mirror_root ) suricata_config_parent_directory = '/' . join ( self . configuration_directory . split ( '/' )[: - 1 ]) if self . configuration_directory . endswith ( '/' ): suricata_config_parent_directory = '/' . join ( self . configuration_directory . split ( '/' )[: - 2 ]) configure_args = [ f '--prefix= { self . install_directory } ' , f '--sysconfdir= { suricata_config_parent_directory } ' , f '--localstatedir= { const . STATE_PATH } /suricata' ] self . logger . debug ( f 'Configuring with: { configure_args } ' ) self . configure_source_package ( suricata_source_install_cache , configure_args = configure_args ) time . sleep ( 1 ) self . compile_source_package ( suricata_source_install_cache , parallel_threads = parallel_threads , expected_lines_printed = COMPILE_PROCESS_EXPECTED_LINE_COUNT ) copy_suricata_files_and_directories ( self ) Copy the required Suricata files from the install cache to their respective directories Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def copy_suricata_files_and_directories ( self ) -> None : \"\"\"Copy the required Suricata files from the install cache to their respective directories Returns: None \"\"\" suricata_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'reference.config' , 'threshold.config' , 'rules/' ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { suricata_tarball_extracted } / { conf } ' , self . configuration_directory ) create_update_suricata_environment_variables ( self ) Creates all the required Suricata environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def create_update_suricata_environment_variables ( self ) -> None : \"\"\"Creates all the required Suricata environmental variables Returns: None \"\"\" self . create_update_env_variable ( 'SURICATA_HOME' , self . install_directory ) self . create_update_env_variable ( 'SURICATA_CONFIG' , self . configuration_directory ) self . create_update_env_variable ( 'SURICATA_LOGS' , self . log_directory ) install_suricata_dependencies ( self ) Install Suricata dependencies Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def install_suricata_dependencies ( self ) -> None : \"\"\"Install Suricata dependencies Returns: None \"\"\" apt_get_packages = [ 'automake' , 'bison' , 'cargo' , 'cmake' , 'flex' , 'g++' , 'gcc' , 'libcap-ng-dev' , 'libjansson-dev' , 'libjemalloc-dev' , 'liblz4-dev' , 'libmagic-dev' , 'libnspr4-dev' , 'libnss3-dev' , 'libpcap-dev' , 'libpcre3-dev' , 'libtool' , 'libyaml-dev' , 'make' , 'pkg-config' , 'rustc' , 'tar' , 'wget' , 'zlib1g-dev' ] yum_packages = [ 'automake' , 'bison' , 'cargo' , 'cmake' , 'file-devel' , 'flex' , 'gcc' , 'gcc-c++' , 'jansson-devel' , 'jemalloc-devel' , 'libcap-ng-devel' , 'libpcap-devel' , 'libtool' , 'libyaml-devel' , 'lz4-devel' , 'make' , 'nspr-devel' , 'nss-devel' , 'pcre-devel' , 'pkgconfig' , 'rustc' , 'tar' , 'wget' , 'zlib-devel' ] def install_powertools_rhel ( pacman_type ): \"\"\"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Args: Returns: None \"\"\" if pacman_type != 'yum' : self . logger . info ( 'Skipping RHEL PowerTools install, as it is not needed on this distribution.' ) return self . install_dependencies ( yum_packages = [ 'dnf-plugins-core' , 'epel-release' ]) enable_powertools_p = subprocess . Popen ([ 'yum' , 'config-manager' , '--set-enabled' , 'powertools' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) enable_powertools_p . communicate () if enable_powertools_p . returncode == 0 : self . logger . info ( \"Installed PowerTools.\" ) super ( InstallManager , self ) . install_dependencies ( apt_get_packages = apt_get_packages , yum_packages = yum_packages , pre_install_function = install_powertools_rhel ) setup ( self , inspect_interfaces ) Install Suricata Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required Returns: Type Description None Source code in dynamite_nsm/services/suricata/install.py def setup ( self , inspect_interfaces : List [ str ]): \"\"\"Install Suricata Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) Returns: None \"\"\" if not self . skip_interface_validation : if not self . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) sysctl = systemctl . SystemCtl () self . install_suricata_dependencies () self . create_update_suricata_environment_variables () self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . debug ( f 'Creating directory: { self . log_directory } ' ) utilities . makedirs ( self . log_directory ) self . copy_suricata_files_and_directories () self . logger . info ( 'Setting up Suricata from source. This can take a few minutes.' ) if self . stdout : utilities . print_coffee_art () self . configure_compile_suricata () self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /suricata/suricata.yaml' , self . configuration_directory ) suricata_config = config . ConfigManager ( self . configuration_directory , stdout = self . stdout , verbose = self . verbose ) suricata_config . default_log_directory = self . log_directory suricata_config . suricata_log_output_file = os . path . join ( self . log_directory , 'suricata.log' ) suricata_config . default_rules_directory = os . path . join ( self . configuration_directory , 'rules' ) suricata_config . reference_config_file = os . path . join ( self . configuration_directory , 'reference.config' ) suricata_config . classification_file = os . path . join ( self . configuration_directory , 'rules' , 'classification.config' ) suricata_config . af_packet_interfaces = misc . AfPacketInterfaces () for interface in inspect_interfaces : suricata_config . af_packet_interfaces . add ( misc . AfPacketInterface ( interface_name = interface , threads = 'auto' , cluster_id = random . randint ( 1 , 50000 ), cluster_type = 'cluster_qm' ) ) suricata_config . threading = suricata_config . get_optimal_suricata_threading_config ( tuple ([ i for i in range ( 0 , utilities . get_cpu_core_count () - 1 )])) suricata_config . commit () self . logger . info ( 'Applying Suricata configuration.' ) self . logger . debug ( suricata_config . af_packet_interfaces ) suricata_config . commit () # Fix Permissions self . logger . info ( 'Setting up file permissions.' ) utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . log_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . configuration_directory } /suricata.yaml' , 660 ) post_install_bootstrap_updater ( self . install_directory , stdout = self . stdout , verbose = self . verbose ) self . logger . info ( 'Setting up Suricata capture rules for dynamite user.' ) set_caps . SetCapturePermissions ( self . install_directory ) . invoke ( shell = True ) self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/suricata.service' ) sysctl . install_and_enable ( os . path . join ( const . DEFAULT_CONFIGS , 'systemd' , 'suricata.service' )) UninstallManager Uninstall Suricata process manager __init__ ( self , purge_config = True , stdout = False , verbose = False ) special Uninstall Suricata Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/suricata/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Suricata Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.suricata.process import ProcessManager env_vars = utilities . get_environment_file_dict () suricata_directories = [ env_vars . get ( 'SURICATA_HOME' ), env_vars . get ( 'SURICATA_LOGS' )] if purge_config : suricata_directories . append ( env_vars . get ( 'SURICATA_CONFIG' )) super () . __init__ ( 'suricata.uninstall' , directories = suricata_directories , sysctl_service_name = 'suricata.service' , environ_vars = [ 'SURICATA_HOME' , 'SURICATA_CONFIG' , 'SURICATA_LOGS' , 'OINKMASTER_HOME' ], process = ProcessManager ( stdout = stdout , verbose = verbose ), stdout = stdout , verbose = verbose ) post_install_bootstrap_updater ( suricata_install_directory , stdout = False , verbose = False ) Perform Suricata rule setup and updating with Oinkmaster Parameters: Name Type Description Default suricata_install_directory str The location of the suricata root install directory (E.G /opt/dynamite/suricata) required stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def post_install_bootstrap_updater ( suricata_install_directory : str , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ) -> None : \"\"\"Perform Suricata rule setup and updating with Oinkmaster Args: suricata_install_directory: The location of the suricata root install directory (E.G /opt/dynamite/suricata) stdout: Print the output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.suricata import oinkmaster as suricata_rule_updater from dynamite_nsm.services.suricata.oinkmaster import install as suricata_rule_updater_install suricata_rule_updater_install . InstallManager ( install_directory = f ' { suricata_install_directory } /rule_updater' , download_oinkmaster_archive = True , stdout = stdout , verbose = verbose ) . setup () suricata_rule_updater . update_suricata_rules ()","title":"install"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager","text":"Manage Suricata installation process","title":"InstallManager"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager.__init__","text":"Install Suricata Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/suricata) required install_directory str Path to the install directory (E.G /opt/dynamite/suricata/) required log_directory str Path to the log directory (E.G /var/log/dynamite/suricata/) required download_suricata_archive Optional[bool] If True, download the Suricata archive from a mirror True skip_interface_validation Optional[bool] If included we don't check if the interface is available on the system False stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/suricata/install.py def __init__ ( self , configuration_directory : str , install_directory : str , log_directory : str , download_suricata_archive : Optional [ bool ] = True , skip_interface_validation : Optional [ bool ] = False , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Suricata Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/suricata) install_directory: Path to the install directory (E.G /opt/dynamite/suricata/) log_directory: Path to the log directory (E.G /var/log/dynamite/suricata/) download_suricata_archive: If True, download the Suricata archive from a mirror skip_interface_validation: If included we don't check if the interface is available on the system stdout: Print the output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . log_directory = log_directory self . download_suricata_archive = download_suricata_archive self . skip_interface_validation = skip_interface_validation self . stdout = stdout self . verbose = verbose install . BaseInstallManager . __init__ ( self , 'suricata.install' , verbose = self . verbose , stdout = stdout ) if download_suricata_archive : self . logger . info ( \"Attempting to download Suricata archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . SURICATA_MIRRORS ) self . logger . info ( f 'Attempting to extract Suricata archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . SURICATA_MIRRORS )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager.configure_compile_suricata","text":"Configure and build Suricata from source Parameters: Name Type Description Default parallel_threads Optional[int] Number of parallel threads to use during the compiling process None Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def configure_compile_suricata ( self , parallel_threads : Optional [ int ] = None ) -> None : \"\"\"Configure and build Suricata from source Args: parallel_threads: Number of parallel threads to use during the compiling process Returns: None \"\"\" suricata_source_install_cache = os . path . join ( const . INSTALL_CACHE , self . local_mirror_root ) suricata_config_parent_directory = '/' . join ( self . configuration_directory . split ( '/' )[: - 1 ]) if self . configuration_directory . endswith ( '/' ): suricata_config_parent_directory = '/' . join ( self . configuration_directory . split ( '/' )[: - 2 ]) configure_args = [ f '--prefix= { self . install_directory } ' , f '--sysconfdir= { suricata_config_parent_directory } ' , f '--localstatedir= { const . STATE_PATH } /suricata' ] self . logger . debug ( f 'Configuring with: { configure_args } ' ) self . configure_source_package ( suricata_source_install_cache , configure_args = configure_args ) time . sleep ( 1 ) self . compile_source_package ( suricata_source_install_cache , parallel_threads = parallel_threads , expected_lines_printed = COMPILE_PROCESS_EXPECTED_LINE_COUNT )","title":"configure_compile_suricata()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager.copy_suricata_files_and_directories","text":"Copy the required Suricata files from the install cache to their respective directories Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def copy_suricata_files_and_directories ( self ) -> None : \"\"\"Copy the required Suricata files from the install cache to their respective directories Returns: None \"\"\" suricata_tarball_extracted = f ' { const . INSTALL_CACHE } / { self . local_mirror_root } ' config_paths = [ 'reference.config' , 'threshold.config' , 'rules/' ] for conf in config_paths : self . copy_file_or_directory_to_destination ( f ' { suricata_tarball_extracted } / { conf } ' , self . configuration_directory )","title":"copy_suricata_files_and_directories()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager.create_update_suricata_environment_variables","text":"Creates all the required Suricata environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def create_update_suricata_environment_variables ( self ) -> None : \"\"\"Creates all the required Suricata environmental variables Returns: None \"\"\" self . create_update_env_variable ( 'SURICATA_HOME' , self . install_directory ) self . create_update_env_variable ( 'SURICATA_CONFIG' , self . configuration_directory ) self . create_update_env_variable ( 'SURICATA_LOGS' , self . log_directory )","title":"create_update_suricata_environment_variables()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager.install_suricata_dependencies","text":"Install Suricata dependencies Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def install_suricata_dependencies ( self ) -> None : \"\"\"Install Suricata dependencies Returns: None \"\"\" apt_get_packages = [ 'automake' , 'bison' , 'cargo' , 'cmake' , 'flex' , 'g++' , 'gcc' , 'libcap-ng-dev' , 'libjansson-dev' , 'libjemalloc-dev' , 'liblz4-dev' , 'libmagic-dev' , 'libnspr4-dev' , 'libnss3-dev' , 'libpcap-dev' , 'libpcre3-dev' , 'libtool' , 'libyaml-dev' , 'make' , 'pkg-config' , 'rustc' , 'tar' , 'wget' , 'zlib1g-dev' ] yum_packages = [ 'automake' , 'bison' , 'cargo' , 'cmake' , 'file-devel' , 'flex' , 'gcc' , 'gcc-c++' , 'jansson-devel' , 'jemalloc-devel' , 'libcap-ng-devel' , 'libpcap-devel' , 'libtool' , 'libyaml-devel' , 'lz4-devel' , 'make' , 'nspr-devel' , 'nss-devel' , 'pcre-devel' , 'pkgconfig' , 'rustc' , 'tar' , 'wget' , 'zlib-devel' ] def install_powertools_rhel ( pacman_type ): \"\"\"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Args: Returns: None \"\"\" if pacman_type != 'yum' : self . logger . info ( 'Skipping RHEL PowerTools install, as it is not needed on this distribution.' ) return self . install_dependencies ( yum_packages = [ 'dnf-plugins-core' , 'epel-release' ]) enable_powertools_p = subprocess . Popen ([ 'yum' , 'config-manager' , '--set-enabled' , 'powertools' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) enable_powertools_p . communicate () if enable_powertools_p . returncode == 0 : self . logger . info ( \"Installed PowerTools.\" ) super ( InstallManager , self ) . install_dependencies ( apt_get_packages = apt_get_packages , yum_packages = yum_packages , pre_install_function = install_powertools_rhel )","title":"install_suricata_dependencies()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.InstallManager.setup","text":"Install Suricata Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required Returns: Type Description None Source code in dynamite_nsm/services/suricata/install.py def setup ( self , inspect_interfaces : List [ str ]): \"\"\"Install Suricata Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) Returns: None \"\"\" if not self . skip_interface_validation : if not self . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) sysctl = systemctl . SystemCtl () self . install_suricata_dependencies () self . create_update_suricata_environment_variables () self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . debug ( f 'Creating directory: { self . log_directory } ' ) utilities . makedirs ( self . log_directory ) self . copy_suricata_files_and_directories () self . logger . info ( 'Setting up Suricata from source. This can take a few minutes.' ) if self . stdout : utilities . print_coffee_art () self . configure_compile_suricata () self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /suricata/suricata.yaml' , self . configuration_directory ) suricata_config = config . ConfigManager ( self . configuration_directory , stdout = self . stdout , verbose = self . verbose ) suricata_config . default_log_directory = self . log_directory suricata_config . suricata_log_output_file = os . path . join ( self . log_directory , 'suricata.log' ) suricata_config . default_rules_directory = os . path . join ( self . configuration_directory , 'rules' ) suricata_config . reference_config_file = os . path . join ( self . configuration_directory , 'reference.config' ) suricata_config . classification_file = os . path . join ( self . configuration_directory , 'rules' , 'classification.config' ) suricata_config . af_packet_interfaces = misc . AfPacketInterfaces () for interface in inspect_interfaces : suricata_config . af_packet_interfaces . add ( misc . AfPacketInterface ( interface_name = interface , threads = 'auto' , cluster_id = random . randint ( 1 , 50000 ), cluster_type = 'cluster_qm' ) ) suricata_config . threading = suricata_config . get_optimal_suricata_threading_config ( tuple ([ i for i in range ( 0 , utilities . get_cpu_core_count () - 1 )])) suricata_config . commit () self . logger . info ( 'Applying Suricata configuration.' ) self . logger . debug ( suricata_config . af_packet_interfaces ) suricata_config . commit () # Fix Permissions self . logger . info ( 'Setting up file permissions.' ) utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_ownership_of_file ( self . log_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . configuration_directory } /suricata.yaml' , 660 ) post_install_bootstrap_updater ( self . install_directory , stdout = self . stdout , verbose = self . verbose ) self . logger . info ( 'Setting up Suricata capture rules for dynamite user.' ) set_caps . SetCapturePermissions ( self . install_directory ) . invoke ( shell = True ) self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/suricata.service' ) sysctl . install_and_enable ( os . path . join ( const . DEFAULT_CONFIGS , 'systemd' , 'suricata.service' ))","title":"setup()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.UninstallManager","text":"Uninstall Suricata process manager","title":"UninstallManager"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.UninstallManager.__init__","text":"Uninstall Suricata Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None Source code in dynamite_nsm/services/suricata/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Suricata Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.suricata.process import ProcessManager env_vars = utilities . get_environment_file_dict () suricata_directories = [ env_vars . get ( 'SURICATA_HOME' ), env_vars . get ( 'SURICATA_LOGS' )] if purge_config : suricata_directories . append ( env_vars . get ( 'SURICATA_CONFIG' )) super () . __init__ ( 'suricata.uninstall' , directories = suricata_directories , sysctl_service_name = 'suricata.service' , environ_vars = [ 'SURICATA_HOME' , 'SURICATA_CONFIG' , 'SURICATA_LOGS' , 'OINKMASTER_HOME' ], process = ProcessManager ( stdout = stdout , verbose = verbose ), stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/install/#dynamite_nsm.services.suricata.install.post_install_bootstrap_updater","text":"Perform Suricata rule setup and updating with Oinkmaster Parameters: Name Type Description Default suricata_install_directory str The location of the suricata root install directory (E.G /opt/dynamite/suricata) required stdout Optional[bool] Print the output to console False verbose Optional[bool] Include detailed debug messages False Returns: Type Description None None Source code in dynamite_nsm/services/suricata/install.py def post_install_bootstrap_updater ( suricata_install_directory : str , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ) -> None : \"\"\"Perform Suricata rule setup and updating with Oinkmaster Args: suricata_install_directory: The location of the suricata root install directory (E.G /opt/dynamite/suricata) stdout: Print the output to console verbose: Include detailed debug messages Returns: None \"\"\" from dynamite_nsm.services.suricata import oinkmaster as suricata_rule_updater from dynamite_nsm.services.suricata.oinkmaster import install as suricata_rule_updater_install suricata_rule_updater_install . InstallManager ( install_directory = f ' { suricata_install_directory } /rule_updater' , download_oinkmaster_archive = True , stdout = stdout , verbose = verbose ) . setup () suricata_rule_updater . update_suricata_rules ()","title":"post_install_bootstrap_updater()"},{"location":"guides/developers/SDK/services/suricata/logs/","text":"Work with a variety of Suricata logs useful for troubleshooting and performance analysis. Currently, supports: suricata.log stats.log eve.json (not recommended) To import... from dynamite_nsm.services.suricata import logs as suricata_logs InvalidSuricataStatusLogEntry Thrown when a Suricata suricata.log entry is improperly formatted __init__ ( self , message ) special :param message: A more specific error message Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , message ): \"\"\" :param message: A more specific error message \"\"\" msg = \"Suricata log entry is invalid: {} \" . format ( message ) super ( InvalidSuricataStatusLogEntry , self ) . __init__ ( msg ) MainEntry A single line item entry in suricata.log __init__ ( self , entry_raw ) special A single line item entry in the suricata.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the suricata.log required Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the suricata.log Args: entry_raw: A JSON serializable string representing a single line item entry in the suricata.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . log_level = None self . category = None self . error_code = None self . error = None self . message = None self . _parse_entry () MainLog Provides an interface for working with suricata.log __init__ ( self , log_sample_size = 10000 ) special Work with Suricata's suricata.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries (or lines) to parse 10000 Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 10000 ): \"\"\"Work with Suricata's suricata.log Args: log_sample_size: The maximum number of entries (or lines) to parse Returns: None \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . suricata_logs = self . env_dict . get ( 'SURICATA_LOGS' ) self . log_path = os . path . join ( self . suricata_logs , 'suricata.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) iter_entries ( self , start = None , end = None , log_level = None , category = None ) Iterate through MainEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime.datetime] UTC start time None end Optional[datetime.datetime] UTC end time None log_level Optional[str] DEBUG, INFO, WARN, ERROR, CRITICAL None category Optional[str] The log entry category None Returns: Type Description yields a MainEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , log_level : Optional [ str ] = None , category : Optional [ str ] = None ): \"\"\"Iterate through MainEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time log_level: DEBUG, INFO, WARN, ERROR, CRITICAL category: The log entry category Returns: yields a MainEntry for every iteration \"\"\" def filter_entries ( s = None , e = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) for en in self . entries : en = MainEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): if log_level : if log_entry . log_level . lower () != log_level . lower (): continue if category : if log_entry . category . lower () != category . lower (): continue yield log_entry tail ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Log Level' , 'Category' , 'Error' , 'Error Code' , 'Message' ], [ entry . time , entry . log_level , entry . category , entry . error , entry . error_code , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) MetricsEntry A single Suricata stats.log (or eve.json \"stats\") metric entry __init__ ( self , time , uptime , capture_kernel_packets , capture_kernel_drops , capture_errors , flow_memory , tcp_memory , tcp_reassembly_memory , dns_memory , http_memory , ftp_memory , http_events , tls_events , ssh_events , imap_events , msn_events , smb_events , dcerpc_tcp_events , dns_tcp_events , nfs_tcp_events , ntp_events , ftp_data_events , tftp_events , ikev2_data_events , krb5_tcp_events , dhcp_events , failed_tcp_events , dcerpc_udp_events , dns_udp_events , krb5_udp_events , failed_udp_events ) special A metrics entry Parameters: Name Type Description Default time str A string representing the time the metric was generated by Suricata required uptime str The amount of time suricata has been up required capture_kernel_packets int The number of packets the kernel has seen required capture_kernel_drops int The number of packets the kernel has dropped required capture_errors int Errors seen while acquiring packet required flow_memory int Memory (bytes) utilized for parsing flows required tcp_memory int Memory (bytes) utilized for TCP parsing required tcp_reassembly_memory int Memory (bytes) utilized for TCP stream reassembly required dns_memory int Memory (bytes) utilized for parsing DNS related traffic required http_memory int Memory (bytes) utilized for parsing HTTP related traffic required ftp_memory int Memory (bytes) utilized for parsing FTP related traffic required http_events int An internal Suricata event metric (useful for debugging) required tls_events int An internal Suricata event metric (useful for debugging) required ssh_events int An internal Suricata event metric (useful for debugging) required imap_events int An internal Suricata event metric (useful for debugging) required msn_events int An internal Suricata event metric (useful for debugging) required smb_events int An internal Suricata event metric (useful for debugging) required dcerpc_tcp_events int An internal Suricata event metric (useful for debugging) required dns_tcp_events int An internal Suricata event metric (useful for debugging) required nfs_tcp_events int An internal Suricata event metric (useful for debugging) required ntp_events int An internal Suricata event metric (useful for debugging) required ftp_data_events int An internal Suricata event metric (useful for debugging) required tftp_events int An internal Suricata event metric (useful for debugging) required ikev2_data_events int An internal Suricata event metric (useful for debugging) required krb5_tcp_events int An internal Suricata event metric (useful for debugging) required dhcp_events int An internal Suricata event metric (useful for debugging) required failed_tcp_events int An internal Suricata event metric (useful for debugging) required dcerpc_udp_events int An internal Suricata event metric (useful for debugging) required dns_udp_events int An internal Suricata event metric (useful for debugging) required krb5_udp_events int An internal Suricata event metric (useful for debugging) required failed_udp_events int An internal Suricata event metric (useful for debugging) required Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , time : str , uptime : str , capture_kernel_packets : int , capture_kernel_drops : int , capture_errors : int , flow_memory : int , tcp_memory : int , tcp_reassembly_memory : int , dns_memory : int , http_memory : int , ftp_memory : int , http_events : int , tls_events : int , ssh_events : int , imap_events : int , msn_events : int , smb_events : int , dcerpc_tcp_events : int , dns_tcp_events : int , nfs_tcp_events : int , ntp_events : int , ftp_data_events : int , tftp_events : int , ikev2_data_events : int , krb5_tcp_events : int , dhcp_events : int , failed_tcp_events : int , dcerpc_udp_events : int , dns_udp_events : int , krb5_udp_events : int , failed_udp_events : int ): \"\"\" A metrics entry Args: time: A string representing the time the metric was generated by Suricata uptime: The amount of time suricata has been up capture_kernel_packets: The number of packets the kernel has seen capture_kernel_drops: The number of packets the kernel has dropped capture_errors: Errors seen while acquiring packet flow_memory: Memory (bytes) utilized for parsing flows tcp_memory: Memory (bytes) utilized for TCP parsing tcp_reassembly_memory: Memory (bytes) utilized for TCP stream reassembly dns_memory: Memory (bytes) utilized for parsing DNS related traffic http_memory: Memory (bytes) utilized for parsing HTTP related traffic ftp_memory: Memory (bytes) utilized for parsing FTP related traffic http_events: An internal Suricata event metric (useful for debugging) tls_events: An internal Suricata event metric (useful for debugging) ssh_events: An internal Suricata event metric (useful for debugging) imap_events: An internal Suricata event metric (useful for debugging) msn_events: An internal Suricata event metric (useful for debugging) smb_events: An internal Suricata event metric (useful for debugging) dcerpc_tcp_events: An internal Suricata event metric (useful for debugging) dns_tcp_events: An internal Suricata event metric (useful for debugging) nfs_tcp_events: An internal Suricata event metric (useful for debugging) ntp_events: An internal Suricata event metric (useful for debugging) ftp_data_events: An internal Suricata event metric (useful for debugging) tftp_events: An internal Suricata event metric (useful for debugging) ikev2_data_events: An internal Suricata event metric (useful for debugging) krb5_tcp_events: An internal Suricata event metric (useful for debugging) dhcp_events: An internal Suricata event metric (useful for debugging) failed_tcp_events: An internal Suricata event metric (useful for debugging) dcerpc_udp_events: An internal Suricata event metric (useful for debugging) dns_udp_events: An internal Suricata event metric (useful for debugging) krb5_udp_events: An internal Suricata event metric (useful for debugging) failed_udp_events: An internal Suricata event metric (useful for debugging) \"\"\" self . timestamp = str ( time ) self . time = time self . uptime = uptime self . capture_kernel_packets = capture_kernel_packets self . capture_kernel_drops = capture_kernel_drops self . capture_errors = capture_errors self . flow_memory = flow_memory self . tcp_memory = tcp_memory self . tcp_reassembly_memory = tcp_reassembly_memory self . dns_memory = dns_memory self . http_memory = http_memory self . ftp_memory = ftp_memory self . http_events = http_events self . tls_events = tls_events self . ssh_events = ssh_events self . imap_events = imap_events self . msn_events = msn_events self . smb_events = smb_events self . dcerpc_tcp_events = dcerpc_tcp_events self . dns_tcp_events = dns_tcp_events self . nfs_tcp_events = nfs_tcp_events self . ntp_events = ntp_events self . ftp_data_events = ftp_data_events self . tftp_events = tftp_events self . ikev2_data_events = ikev2_data_events self . krb5_tcp_events = krb5_tcp_events self . dhcp_events = dhcp_events self . failed_tcp_events = failed_tcp_events self . dcerpc_udp_events = dcerpc_udp_events self . dns_udp_events = dns_udp_events self . krb5_udp_events = krb5_udp_events self . failed_udp_events = failed_udp_events self . capture_kernel_drops_percentage = 0 if self . capture_kernel_packets > 0 : self . capture_kernel_drops_percentage = round ( self . capture_kernel_drops / self . capture_kernel_packets , 2 ) create_from_eve_raw_stats ( entry_raw_eve ) classmethod Create a metrics entry from stats.log serialized entry. Parameters: Name Type Description Default entry_raw_eve A dictionary containing the various metrics fields found in eve.json stats entry required Returns: An instance of MetricsEntry class Source code in dynamite_nsm/services/suricata/logs.py @classmethod def create_from_eve_raw_stats ( cls , entry_raw_eve ): \"\"\" Create a metrics entry from stats.log serialized entry. Args: entry_raw_eve: A dictionary containing the various metrics fields found in eve.json `stats` entry Returns: An instance of MetricsEntry class \"\"\" entry_raw = entry_raw_eve stats = entry_raw [ 'stats' ] timestamp = entry_raw . get ( 'timestamp' ) time = parse_suricata_datetime ( timestamp ) uptime = stats . get ( 'uptime' ) capture_kernel_packets = stats . get ( 'capture' , {}) . get ( 'kernel_packets' , 0 ) capture_kernel_drops = stats . get ( 'capture' , {}) . get ( 'kernel_drops' , 0 ) capture_errors = stats . get ( 'capture' , {}) . get ( 'errors' , 0 ) flow_memory = stats . get ( 'flow' , {}) . get ( 'memuse' , 0 ) tcp_memory = stats . get ( 'tcp' , {}) . get ( 'memuse' , 0 ) tcp_reassembly_memory = stats . get ( 'tcp' , {}) . get ( 'reassembly_memuse' , 0 ) dns_memory = stats . get ( 'dns' , {}) . get ( 'memuse' , 0 ) http_memory = stats . get ( 'http' , {}) . get ( 'memuse' , 0 ) ftp_memory = stats . get ( 'ftp' , {}) . get ( 'memuse' , 0 ) http_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'http' , 0 ) tls_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'tls' , 0 ) ssh_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ssh' , 0 ) imap_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'imap' , 0 ) msn_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'msn' , 0 ) smb_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'smb' , 0 ) dcerpc_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dcerpc_tcp' , 0 ) dns_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dns_tcp' , 0 ) nfs_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'nfs_tcp' , 0 ) ntp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ntp' , 0 ) ftp_data_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ftp-data' , 0 ) tftp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'tftp' , 0 ) ikev2_data_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ikev2' , 0 ) krb5_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'krb5_tcp' , 0 ) dhcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dhcp' , 0 ) failed_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'failed_tcp' , 0 ) dcerpc_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dcerpc_udp' , 0 ) dns_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dns_udp' , 0 ) krb5_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'krb5_udp' , 0 ) failed_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'failed_udp' , 0 ) return cls ( time , uptime , capture_kernel_packets , capture_kernel_drops , capture_errors , flow_memory , tcp_memory , tcp_reassembly_memory , dns_memory , http_memory , ftp_memory , http_events , tls_events , ssh_events , imap_events , msn_events , smb_events , dcerpc_tcp_events , dns_tcp_events , nfs_tcp_events , ntp_events , ftp_data_events , tftp_events , ikev2_data_events , krb5_tcp_events , dhcp_events , failed_tcp_events , dcerpc_udp_events , dns_udp_events , krb5_udp_events , failed_udp_events ) create_from_raw_stats_entry ( entry_raw_stats ) classmethod Create a metrics entry from stats.log serialized entry. Parameters: Name Type Description Default entry_raw_stats Dict A dictionary containing the various metrics fields found in stats.log. required Returns: An instance of MetricsEntry class Source code in dynamite_nsm/services/suricata/logs.py @classmethod def create_from_raw_stats_entry ( cls , entry_raw_stats : Dict ): \"\"\" Create a metrics entry from stats.log serialized entry. Args: entry_raw_stats: A dictionary containing the various metrics fields found in stats.log. Returns: An instance of MetricsEntry class \"\"\" entry_raw = entry_raw_stats time = entry_raw . get ( 'time' ) uptime = entry_raw . get ( 'uptime' ) capture_kernel_packets = entry_raw . get ( 'capture.kernel_packets' , 0 ) capture_kernel_drops = entry_raw . get ( 'capture.kernel_drops' , 0 ) capture_errors = entry_raw . get ( 'capture.errors' , 0 ) flow_memory = entry_raw . get ( 'flow.memuse' , 0 ) tcp_memory = entry_raw . get ( 'tcp.memuse' , 0 ) tcp_reassembly_memory = entry_raw . get ( 'tcp.reassembly_memuse' , 0 ) dns_memory = entry_raw . get ( 'dns.memuse' , 0 ) http_memory = entry_raw . get ( 'http.memuse' , 0 ) ftp_memory = entry_raw . get ( 'ftp.memuse' , 0 ) http_events = entry_raw . get ( 'app_layer.flow.http' , 0 ) tls_events = entry_raw . get ( 'app_layer.flow.tls' , 0 ) ssh_events = entry_raw . get ( 'app_layer.flow.ssh' , 0 ) imap_events = entry_raw . get ( 'app_layer.flow.imap' , 0 ) msn_events = entry_raw . get ( 'app_layer.flow.msn' , 0 ) smb_events = entry_raw . get ( 'app_layer.flow.smb' , 0 ) dcerpc_tcp_events = entry_raw . get ( 'app_layer.flow.dcerpc_tcp' , 0 ) dns_tcp_events = entry_raw . get ( 'app_layer.flow.dns_tcp' , 0 ) nfs_tcp_events = entry_raw . get ( 'app_layer.flow.nfs_tcp' , 0 ) ntp_events = entry_raw . get ( 'app_layer.flow.dcerpc_tcp' , 0 ) ftp_data_events = entry_raw . get ( 'app_layer.flow.ftp-data' , 0 ) tftp_events = entry_raw . get ( 'app_layer.flow.tftp' , 0 ) ikev2_data_events = entry_raw . get ( 'app_layer.flow.ikev2' , 0 ) krb5_tcp_events = entry_raw . get ( 'app_layer.flow.krb5_tcp' , 0 ) dhcp_events = entry_raw . get ( 'app_layer.flow.dhcp' , 0 ) failed_tcp_events = entry_raw . get ( 'app_layer.flow.failed_tcp' , 0 ) dcerpc_udp_events = entry_raw . get ( 'app_layer.flow.failed_udp' , 0 ) dns_udp_events = entry_raw . get ( 'app_layer.flow.dns_udp' , 0 ) krb5_udp_events = entry_raw . get ( 'app_layer.flow.krb5_udp' , 0 ) failed_udp_events = entry_raw . get ( 'app_layer.flow.failed_udp' , 0 ) return cls ( time , uptime , capture_kernel_packets , capture_kernel_drops , capture_errors , flow_memory , tcp_memory , tcp_reassembly_memory , dns_memory , http_memory , ftp_memory , http_events , tls_events , ssh_events , imap_events , msn_events , smb_events , dcerpc_tcp_events , dns_tcp_events , nfs_tcp_events , ntp_events , ftp_data_events , tftp_events , ikev2_data_events , krb5_tcp_events , dhcp_events , failed_tcp_events , dcerpc_udp_events , dns_udp_events , krb5_udp_events , failed_udp_events ) get_total_memory ( self ) Get the total amount of memory being used (in bytes) Warning Testing has proven this number to be unreliable. Returns: Type Description The total amount of memory being used by Suricata processes Source code in dynamite_nsm/services/suricata/logs.py def get_total_memory ( self ): \"\"\" Get the total amount of memory being used (in bytes) > **Warning** Testing has proven this number to be unreliable. Returns: The total amount of memory being used by Suricata processes \"\"\" return self . ftp_memory + self . http_memory + self . dns_memory + self . tcp_reassembly_memory + self . tcp_memory + \\ self . flow_memory merge_metric_entry ( self , metric_entry ) Merge another metrics entry into this one Parameters: Name Type Description Default metric_entry The MetricsEntry you wish to merge in required Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def merge_metric_entry ( self , metric_entry ): \"\"\"Merge another metrics entry into this one Args: metric_entry: The MetricsEntry you wish to merge in Returns: None \"\"\" if not isinstance ( metric_entry , MetricsEntry ): return self . capture_kernel_packets += metric_entry . capture_kernel_packets self . capture_kernel_drops += metric_entry . capture_kernel_drops self . capture_errors += metric_entry . capture_errors self . ftp_memory = math . ceil (( self . ftp_memory + metric_entry . ftp_memory ) / 2 ) self . flow_memory = math . ceil (( self . flow_memory + metric_entry . flow_memory ) / 2 ) self . http_memory = math . ceil (( self . http_memory + metric_entry . http_memory ) / 2 ) self . dns_memory = math . ceil (( self . dns_memory + metric_entry . dns_memory ) / 2 ) self . tcp_memory = math . ceil (( self . tcp_memory + metric_entry . tcp_memory ) / 2 ) self . tcp_reassembly_memory = \\ math . ceil (( self . tcp_reassembly_memory + metric_entry . tcp_reassembly_memory ) / 2 ) self . http_events += metric_entry . http_events self . tls_events += metric_entry . tls_events self . ssh_events += metric_entry . ssh_events self . imap_events += metric_entry . imap_events self . msn_events += metric_entry . msn_events self . smb_events += metric_entry . smb_events self . dcerpc_tcp_events += metric_entry . dcerpc_tcp_events self . dns_tcp_events += metric_entry . dns_tcp_events self . nfs_tcp_events += metric_entry . nfs_tcp_events self . ntp_events += metric_entry . ntp_events self . ftp_data_events += metric_entry . ftp_data_events self . tftp_events += metric_entry . tftp_events self . ikev2_data_events += metric_entry . ikev2_data_events self . krb5_tcp_events += metric_entry . krb5_tcp_events self . dhcp_events += metric_entry . dhcp_events self . failed_tcp_events += metric_entry . failed_tcp_events self . dcerpc_udp_events += metric_entry . dcerpc_udp_events self . dns_udp_events += metric_entry . dns_udp_events self . krb5_udp_events += metric_entry . krb5_udp_events self . failed_udp_events += metric_entry . failed_udp_events if self . capture_kernel_packets > 0 : self . capture_kernel_drops_percentage = round ( self . capture_kernel_drops / self . capture_kernel_packets , 6 ) StatsLog Provides an interface for working with Suricata's stats.log __init__ ( self , log_sample_size = 10000 ) special Work with Suricata's stats.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries (or lines) to parse. 10000 The size of log_sample_size is set significantly higher as log entries can span multiple lines. Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 10000 ): \"\"\"Work with Suricata's stats.log Args: log_sample_size: The maximum number of entries (or lines) to parse. --- > The size of `log_sample_size` is set significantly higher as log entries can span multiple lines. \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . suricata_logs = self . env_dict . get ( 'SURICATA_LOGS' ) self . log_path = os . path . join ( self . suricata_logs , 'stats.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) self . log_path = os . path . join ( self . suricata_logs , 'stats.log' ) iter_aggregated_metrics ( self , start = None , end = None , tolerance_seconds = 60 ) Aggregate events within tolerance_seconds into the same entry. Parameters: Name Type Description Default start Optional[datetime.datetime] UTC start time None end Optional[datetime.datetime] UTC end time None tolerance_seconds Optional[int] Specifies the maximum time distance between entries to combine them 60 Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_aggregated_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , tolerance_seconds : Optional [ int ] = 60 ): \"\"\"Aggregate events within tolerance_seconds into the same entry. Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum time distance between entries to combine them Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . time ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry iter_metrics ( self , start = None , end = None ) Iterate through metrics entries individually. Parameters: Name Type Description Default start Optional[datetime.datetime] UTC start time None end Optional[datetime.datetime] UTC end time None Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ): \"\"\"Iterate through metrics entries individually. Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" self . _state_machine_parser () def filter_metrics ( s = None , e = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) prev_en = None for en_dict in self . entries : try : en = MetricsEntry . create_from_raw_stats_entry ( en_dict ) except ValueError : continue en_corrected = MetricsEntry . create_from_raw_stats_entry ( en_dict ) if s < en . time < e : if not prev_en : prev_en = en continue # A lot of Suricata stats are counters, meaning that they increment from zero starting at # process start. We address this by applying the difference between the current and previous # entries so that each entry represents an increment en_corrected . capture_kernel_packets = \\ max ( 0 , en . capture_kernel_packets - prev_en . capture_kernel_packets ) en_corrected . capture_errors = max ( 0 , en . capture_errors - prev_en . capture_errors ) en_corrected . capture_kernel_drops = \\ max ( 0 , en . capture_kernel_drops - prev_en . capture_kernel_drops ) en_corrected . http_events = max ( 0 , en . http_events - prev_en . http_events ) en_corrected . tls_events = max ( 0 , en . tls_events - prev_en . tls_events ) en_corrected . ssh_events = max ( 0 , en . ssh_events - prev_en . ssh_events ) en_corrected . imap_events = max ( 0 , en . imap_events - prev_en . imap_events ) en_corrected . msn_events = max ( 0 , en . msn_events - prev_en . msn_events ) en_corrected . smb_events = max ( 0 , en . smb_events - prev_en . smb_events ) en_corrected . dcerpc_tcp_events = max ( 0 , en . dcerpc_tcp_events - prev_en . dcerpc_tcp_events ) en_corrected . dns_tcp_events = max ( 0 , en . dns_tcp_events - prev_en . dns_tcp_events ) en_corrected . nfs_tcp_events = max ( 0 , en . nfs_tcp_events - prev_en . nfs_tcp_events ) en_corrected . ntp_events = max ( 0 , en . ntp_events - prev_en . ntp_events ) en_corrected . ftp_data_events = max ( 0 , en . ftp_data_events - prev_en . ftp_data_events ) en_corrected . tftp_events = max ( 0 , en . tftp_events - prev_en . tftp_events ) en_corrected . ikev2_data_events = max ( 0 , en . ikev2_data_events - prev_en . ikev2_data_events ) en_corrected . krb5_tcp_events = max ( 0 , en . krb5_tcp_events - prev_en . krb5_tcp_events ) en_corrected . dhcp_events = max ( 0 , en . dhcp_events - prev_en . dhcp_events ) en_corrected . failed_tcp_events = max ( 0 , en . failed_tcp_events - prev_en . failed_tcp_events ) en_corrected . dcerpc_udp_events = max ( 0 , en . dcerpc_udp_events - prev_en . dcerpc_udp_events ) en_corrected . dns_udp_events = max ( 0 , en . dns_udp_events - prev_en . dns_udp_events ) en_corrected . krb5_udp_events = max ( 0 , en . krb5_udp_events - prev_en . krb5_udp_events ) en_corrected . failed_udp_events = max ( 0 , en . failed_udp_events - prev_en . failed_udp_events ) prev_en = en yield en_corrected for log_entry in filter_metrics ( start , end ): yield log_entry tail ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for metric in self . iter_aggregated_metrics ( start = start , end = end ): if metric . timestamp not in visited : visited . append ( metric . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( metric )), indent = 1 )) else : status_table = [ [ 'Time' , 'Memory' , 'Packets Captured' , 'Packets Dropped' , 'Errors During Capture' ], [ metric . time , metric . get_total_memory (), metric . capture_kernel_packets , metric . capture_kernel_drops , metric . capture_errors ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) StatusLogEve A status entry from Suricata's eve.json __init__ ( self , log_sample_size = 10000 ) special A status entry from eve.json log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries (or lines) to parse 10000 Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 10000 ): \"\"\"A status entry from eve.json log Args: log_sample_size: The maximum number of entries (or lines) to parse \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . suricata_logs = self . env_dict . get ( 'SURICATA_LOGS' ) self . log_path = os . path . join ( self . suricata_logs , 'eve.json' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) iter_aggregated_metrics ( self , start = None , end = None , tolerance_seconds = 60 ) Aggregate events within tolerance_seconds into the same entry. Parameters: Name Type Description Default start UTC start time None end UTC end time None tolerance_seconds Specifies the maximum time distance between entries to combine them 60 Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_aggregated_metrics ( self , start = None , end = None , tolerance_seconds = 60 ): \"\"\"Aggregate events within tolerance_seconds into the same entry. Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum time distance between entries to combine them Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . time ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry iter_metrics ( self , start = None , end = None ) Iterate through metrics entries individually. Parameters: Name Type Description Default start UTC start time None end UTC end time None Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_metrics ( self , start = None , end = None ): \"\"\"Iterate through metrics entries individually. Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" def filter_metrics ( s = None , e = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) prev_en = None for en_raw in self . entries : if '\"event_type\":\"stats\"' in en_raw : try : en = MetricsEntry . create_from_eve_raw_stats ( json . loads ( en_raw )) except ValueError : continue en_corrected = MetricsEntry . create_from_eve_raw_stats ( json . loads ( en_raw )) if s < en . time < e : if not prev_en : prev_en = en continue # A lot of Suricata stats are counters, meaning that they increment from zero starting at # process start. We address this by applying the difference between the current and previous # entries so that each entry represents an increment en_corrected . capture_kernel_packets = \\ max ( 0 , en . capture_kernel_packets - prev_en . capture_kernel_packets ) en_corrected . capture_errors = max ( 0 , en . capture_errors - prev_en . capture_errors ) en_corrected . capture_kernel_drops = \\ max ( 0 , en . capture_kernel_drops - prev_en . capture_kernel_drops ) en_corrected . http_events = max ( 0 , en . http_events - prev_en . http_events ) en_corrected . tls_events = max ( 0 , en . tls_events - prev_en . tls_events ) en_corrected . ssh_events = max ( 0 , en . ssh_events - prev_en . ssh_events ) en_corrected . imap_events = max ( 0 , en . imap_events - prev_en . imap_events ) en_corrected . msn_events = max ( 0 , en . msn_events - prev_en . msn_events ) en_corrected . smb_events = max ( 0 , en . smb_events - prev_en . smb_events ) en_corrected . dcerpc_tcp_events = max ( 0 , en . dcerpc_tcp_events - prev_en . dcerpc_tcp_events ) en_corrected . dns_tcp_events = max ( 0 , en . dns_tcp_events - prev_en . dns_tcp_events ) en_corrected . nfs_tcp_events = max ( 0 , en . nfs_tcp_events - prev_en . nfs_tcp_events ) en_corrected . ntp_events = max ( 0 , en . ntp_events - prev_en . ntp_events ) en_corrected . ftp_data_events = max ( 0 , en . ftp_data_events - prev_en . ftp_data_events ) en_corrected . tftp_events = max ( 0 , en . tftp_events - prev_en . tftp_events ) en_corrected . ikev2_data_events = max ( 0 , en . ikev2_data_events - prev_en . ikev2_data_events ) en_corrected . krb5_tcp_events = max ( 0 , en . krb5_tcp_events - prev_en . krb5_tcp_events ) en_corrected . dhcp_events = max ( 0 , en . dhcp_events - prev_en . dhcp_events ) en_corrected . failed_tcp_events = max ( 0 , en . failed_tcp_events - prev_en . failed_tcp_events ) en_corrected . dcerpc_udp_events = max ( 0 , en . dcerpc_udp_events - prev_en . dcerpc_udp_events ) en_corrected . dns_udp_events = max ( 0 , en . dns_udp_events - prev_en . dns_udp_events ) en_corrected . krb5_udp_events = max ( 0 , en . krb5_udp_events - prev_en . krb5_udp_events ) en_corrected . failed_udp_events = max ( 0 , en . failed_udp_events - prev_en . failed_udp_events ) prev_en = en yield en_corrected for log_entry in filter_metrics ( start , end ): yield log_entry parse_suricata_datetime ( t ) Parse a common suricata timestamp string Parameters: Name Type Description Default t str A '%Y-%m-%dT%H:%M:%S.%f' formatted string required Returns: A datetime object Source code in dynamite_nsm/services/suricata/logs.py def parse_suricata_datetime ( t : str ) -> datetime : \"\"\" Parse a common suricata timestamp string Args: t: A '%Y-%m-%dT%H:%M:%S.%f' formatted string Returns: A datetime object \"\"\" ret = datetime . strptime ( t [ 0 : 22 ], '%Y-%m- %d T%H:%M:%S. %f ' ) if t [ 26 ] == '+' : ret -= timedelta ( hours = int ( t [ 27 : 29 ]), minutes = int ( t [ 30 :])) elif t [ 26 ] == '-' : ret += timedelta ( hours = int ( t [ 27 : 29 ]), minutes = int ( t [ 30 :])) return ret","title":"logs"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.InvalidSuricataStatusLogEntry","text":"Thrown when a Suricata suricata.log entry is improperly formatted","title":"InvalidSuricataStatusLogEntry"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.InvalidSuricataStatusLogEntry.__init__","text":":param message: A more specific error message Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , message ): \"\"\" :param message: A more specific error message \"\"\" msg = \"Suricata log entry is invalid: {} \" . format ( message ) super ( InvalidSuricataStatusLogEntry , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MainEntry","text":"A single line item entry in suricata.log","title":"MainEntry"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MainEntry.__init__","text":"A single line item entry in the suricata.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the suricata.log required Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the suricata.log Args: entry_raw: A JSON serializable string representing a single line item entry in the suricata.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . log_level = None self . category = None self . error_code = None self . error = None self . message = None self . _parse_entry ()","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MainLog","text":"Provides an interface for working with suricata.log","title":"MainLog"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MainLog.__init__","text":"Work with Suricata's suricata.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries (or lines) to parse 10000 Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 10000 ): \"\"\"Work with Suricata's suricata.log Args: log_sample_size: The maximum number of entries (or lines) to parse Returns: None \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . suricata_logs = self . env_dict . get ( 'SURICATA_LOGS' ) self . log_path = os . path . join ( self . suricata_logs , 'suricata.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MainLog.iter_entries","text":"Iterate through MainEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime.datetime] UTC start time None end Optional[datetime.datetime] UTC end time None log_level Optional[str] DEBUG, INFO, WARN, ERROR, CRITICAL None category Optional[str] The log entry category None Returns: Type Description yields a MainEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , log_level : Optional [ str ] = None , category : Optional [ str ] = None ): \"\"\"Iterate through MainEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time log_level: DEBUG, INFO, WARN, ERROR, CRITICAL category: The log entry category Returns: yields a MainEntry for every iteration \"\"\" def filter_entries ( s = None , e = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) for en in self . entries : en = MainEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): if log_level : if log_entry . log_level . lower () != log_level . lower (): continue if category : if log_entry . category . lower () != category . lower (): continue yield log_entry","title":"iter_entries()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MainLog.tail","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Log Level' , 'Category' , 'Error' , 'Error Code' , 'Message' ], [ entry . time , entry . log_level , entry . category , entry . error , entry . error_code , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MetricsEntry","text":"A single Suricata stats.log (or eve.json \"stats\") metric entry","title":"MetricsEntry"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MetricsEntry.__init__","text":"A metrics entry Parameters: Name Type Description Default time str A string representing the time the metric was generated by Suricata required uptime str The amount of time suricata has been up required capture_kernel_packets int The number of packets the kernel has seen required capture_kernel_drops int The number of packets the kernel has dropped required capture_errors int Errors seen while acquiring packet required flow_memory int Memory (bytes) utilized for parsing flows required tcp_memory int Memory (bytes) utilized for TCP parsing required tcp_reassembly_memory int Memory (bytes) utilized for TCP stream reassembly required dns_memory int Memory (bytes) utilized for parsing DNS related traffic required http_memory int Memory (bytes) utilized for parsing HTTP related traffic required ftp_memory int Memory (bytes) utilized for parsing FTP related traffic required http_events int An internal Suricata event metric (useful for debugging) required tls_events int An internal Suricata event metric (useful for debugging) required ssh_events int An internal Suricata event metric (useful for debugging) required imap_events int An internal Suricata event metric (useful for debugging) required msn_events int An internal Suricata event metric (useful for debugging) required smb_events int An internal Suricata event metric (useful for debugging) required dcerpc_tcp_events int An internal Suricata event metric (useful for debugging) required dns_tcp_events int An internal Suricata event metric (useful for debugging) required nfs_tcp_events int An internal Suricata event metric (useful for debugging) required ntp_events int An internal Suricata event metric (useful for debugging) required ftp_data_events int An internal Suricata event metric (useful for debugging) required tftp_events int An internal Suricata event metric (useful for debugging) required ikev2_data_events int An internal Suricata event metric (useful for debugging) required krb5_tcp_events int An internal Suricata event metric (useful for debugging) required dhcp_events int An internal Suricata event metric (useful for debugging) required failed_tcp_events int An internal Suricata event metric (useful for debugging) required dcerpc_udp_events int An internal Suricata event metric (useful for debugging) required dns_udp_events int An internal Suricata event metric (useful for debugging) required krb5_udp_events int An internal Suricata event metric (useful for debugging) required failed_udp_events int An internal Suricata event metric (useful for debugging) required Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , time : str , uptime : str , capture_kernel_packets : int , capture_kernel_drops : int , capture_errors : int , flow_memory : int , tcp_memory : int , tcp_reassembly_memory : int , dns_memory : int , http_memory : int , ftp_memory : int , http_events : int , tls_events : int , ssh_events : int , imap_events : int , msn_events : int , smb_events : int , dcerpc_tcp_events : int , dns_tcp_events : int , nfs_tcp_events : int , ntp_events : int , ftp_data_events : int , tftp_events : int , ikev2_data_events : int , krb5_tcp_events : int , dhcp_events : int , failed_tcp_events : int , dcerpc_udp_events : int , dns_udp_events : int , krb5_udp_events : int , failed_udp_events : int ): \"\"\" A metrics entry Args: time: A string representing the time the metric was generated by Suricata uptime: The amount of time suricata has been up capture_kernel_packets: The number of packets the kernel has seen capture_kernel_drops: The number of packets the kernel has dropped capture_errors: Errors seen while acquiring packet flow_memory: Memory (bytes) utilized for parsing flows tcp_memory: Memory (bytes) utilized for TCP parsing tcp_reassembly_memory: Memory (bytes) utilized for TCP stream reassembly dns_memory: Memory (bytes) utilized for parsing DNS related traffic http_memory: Memory (bytes) utilized for parsing HTTP related traffic ftp_memory: Memory (bytes) utilized for parsing FTP related traffic http_events: An internal Suricata event metric (useful for debugging) tls_events: An internal Suricata event metric (useful for debugging) ssh_events: An internal Suricata event metric (useful for debugging) imap_events: An internal Suricata event metric (useful for debugging) msn_events: An internal Suricata event metric (useful for debugging) smb_events: An internal Suricata event metric (useful for debugging) dcerpc_tcp_events: An internal Suricata event metric (useful for debugging) dns_tcp_events: An internal Suricata event metric (useful for debugging) nfs_tcp_events: An internal Suricata event metric (useful for debugging) ntp_events: An internal Suricata event metric (useful for debugging) ftp_data_events: An internal Suricata event metric (useful for debugging) tftp_events: An internal Suricata event metric (useful for debugging) ikev2_data_events: An internal Suricata event metric (useful for debugging) krb5_tcp_events: An internal Suricata event metric (useful for debugging) dhcp_events: An internal Suricata event metric (useful for debugging) failed_tcp_events: An internal Suricata event metric (useful for debugging) dcerpc_udp_events: An internal Suricata event metric (useful for debugging) dns_udp_events: An internal Suricata event metric (useful for debugging) krb5_udp_events: An internal Suricata event metric (useful for debugging) failed_udp_events: An internal Suricata event metric (useful for debugging) \"\"\" self . timestamp = str ( time ) self . time = time self . uptime = uptime self . capture_kernel_packets = capture_kernel_packets self . capture_kernel_drops = capture_kernel_drops self . capture_errors = capture_errors self . flow_memory = flow_memory self . tcp_memory = tcp_memory self . tcp_reassembly_memory = tcp_reassembly_memory self . dns_memory = dns_memory self . http_memory = http_memory self . ftp_memory = ftp_memory self . http_events = http_events self . tls_events = tls_events self . ssh_events = ssh_events self . imap_events = imap_events self . msn_events = msn_events self . smb_events = smb_events self . dcerpc_tcp_events = dcerpc_tcp_events self . dns_tcp_events = dns_tcp_events self . nfs_tcp_events = nfs_tcp_events self . ntp_events = ntp_events self . ftp_data_events = ftp_data_events self . tftp_events = tftp_events self . ikev2_data_events = ikev2_data_events self . krb5_tcp_events = krb5_tcp_events self . dhcp_events = dhcp_events self . failed_tcp_events = failed_tcp_events self . dcerpc_udp_events = dcerpc_udp_events self . dns_udp_events = dns_udp_events self . krb5_udp_events = krb5_udp_events self . failed_udp_events = failed_udp_events self . capture_kernel_drops_percentage = 0 if self . capture_kernel_packets > 0 : self . capture_kernel_drops_percentage = round ( self . capture_kernel_drops / self . capture_kernel_packets , 2 )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MetricsEntry.create_from_eve_raw_stats","text":"Create a metrics entry from stats.log serialized entry. Parameters: Name Type Description Default entry_raw_eve A dictionary containing the various metrics fields found in eve.json stats entry required Returns: An instance of MetricsEntry class Source code in dynamite_nsm/services/suricata/logs.py @classmethod def create_from_eve_raw_stats ( cls , entry_raw_eve ): \"\"\" Create a metrics entry from stats.log serialized entry. Args: entry_raw_eve: A dictionary containing the various metrics fields found in eve.json `stats` entry Returns: An instance of MetricsEntry class \"\"\" entry_raw = entry_raw_eve stats = entry_raw [ 'stats' ] timestamp = entry_raw . get ( 'timestamp' ) time = parse_suricata_datetime ( timestamp ) uptime = stats . get ( 'uptime' ) capture_kernel_packets = stats . get ( 'capture' , {}) . get ( 'kernel_packets' , 0 ) capture_kernel_drops = stats . get ( 'capture' , {}) . get ( 'kernel_drops' , 0 ) capture_errors = stats . get ( 'capture' , {}) . get ( 'errors' , 0 ) flow_memory = stats . get ( 'flow' , {}) . get ( 'memuse' , 0 ) tcp_memory = stats . get ( 'tcp' , {}) . get ( 'memuse' , 0 ) tcp_reassembly_memory = stats . get ( 'tcp' , {}) . get ( 'reassembly_memuse' , 0 ) dns_memory = stats . get ( 'dns' , {}) . get ( 'memuse' , 0 ) http_memory = stats . get ( 'http' , {}) . get ( 'memuse' , 0 ) ftp_memory = stats . get ( 'ftp' , {}) . get ( 'memuse' , 0 ) http_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'http' , 0 ) tls_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'tls' , 0 ) ssh_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ssh' , 0 ) imap_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'imap' , 0 ) msn_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'msn' , 0 ) smb_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'smb' , 0 ) dcerpc_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dcerpc_tcp' , 0 ) dns_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dns_tcp' , 0 ) nfs_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'nfs_tcp' , 0 ) ntp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ntp' , 0 ) ftp_data_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ftp-data' , 0 ) tftp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'tftp' , 0 ) ikev2_data_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'ikev2' , 0 ) krb5_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'krb5_tcp' , 0 ) dhcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dhcp' , 0 ) failed_tcp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'failed_tcp' , 0 ) dcerpc_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dcerpc_udp' , 0 ) dns_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'dns_udp' , 0 ) krb5_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'krb5_udp' , 0 ) failed_udp_events = stats . get ( 'app_layer' , {}) . get ( 'flow' , {}) . get ( 'failed_udp' , 0 ) return cls ( time , uptime , capture_kernel_packets , capture_kernel_drops , capture_errors , flow_memory , tcp_memory , tcp_reassembly_memory , dns_memory , http_memory , ftp_memory , http_events , tls_events , ssh_events , imap_events , msn_events , smb_events , dcerpc_tcp_events , dns_tcp_events , nfs_tcp_events , ntp_events , ftp_data_events , tftp_events , ikev2_data_events , krb5_tcp_events , dhcp_events , failed_tcp_events , dcerpc_udp_events , dns_udp_events , krb5_udp_events , failed_udp_events )","title":"create_from_eve_raw_stats()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MetricsEntry.create_from_raw_stats_entry","text":"Create a metrics entry from stats.log serialized entry. Parameters: Name Type Description Default entry_raw_stats Dict A dictionary containing the various metrics fields found in stats.log. required Returns: An instance of MetricsEntry class Source code in dynamite_nsm/services/suricata/logs.py @classmethod def create_from_raw_stats_entry ( cls , entry_raw_stats : Dict ): \"\"\" Create a metrics entry from stats.log serialized entry. Args: entry_raw_stats: A dictionary containing the various metrics fields found in stats.log. Returns: An instance of MetricsEntry class \"\"\" entry_raw = entry_raw_stats time = entry_raw . get ( 'time' ) uptime = entry_raw . get ( 'uptime' ) capture_kernel_packets = entry_raw . get ( 'capture.kernel_packets' , 0 ) capture_kernel_drops = entry_raw . get ( 'capture.kernel_drops' , 0 ) capture_errors = entry_raw . get ( 'capture.errors' , 0 ) flow_memory = entry_raw . get ( 'flow.memuse' , 0 ) tcp_memory = entry_raw . get ( 'tcp.memuse' , 0 ) tcp_reassembly_memory = entry_raw . get ( 'tcp.reassembly_memuse' , 0 ) dns_memory = entry_raw . get ( 'dns.memuse' , 0 ) http_memory = entry_raw . get ( 'http.memuse' , 0 ) ftp_memory = entry_raw . get ( 'ftp.memuse' , 0 ) http_events = entry_raw . get ( 'app_layer.flow.http' , 0 ) tls_events = entry_raw . get ( 'app_layer.flow.tls' , 0 ) ssh_events = entry_raw . get ( 'app_layer.flow.ssh' , 0 ) imap_events = entry_raw . get ( 'app_layer.flow.imap' , 0 ) msn_events = entry_raw . get ( 'app_layer.flow.msn' , 0 ) smb_events = entry_raw . get ( 'app_layer.flow.smb' , 0 ) dcerpc_tcp_events = entry_raw . get ( 'app_layer.flow.dcerpc_tcp' , 0 ) dns_tcp_events = entry_raw . get ( 'app_layer.flow.dns_tcp' , 0 ) nfs_tcp_events = entry_raw . get ( 'app_layer.flow.nfs_tcp' , 0 ) ntp_events = entry_raw . get ( 'app_layer.flow.dcerpc_tcp' , 0 ) ftp_data_events = entry_raw . get ( 'app_layer.flow.ftp-data' , 0 ) tftp_events = entry_raw . get ( 'app_layer.flow.tftp' , 0 ) ikev2_data_events = entry_raw . get ( 'app_layer.flow.ikev2' , 0 ) krb5_tcp_events = entry_raw . get ( 'app_layer.flow.krb5_tcp' , 0 ) dhcp_events = entry_raw . get ( 'app_layer.flow.dhcp' , 0 ) failed_tcp_events = entry_raw . get ( 'app_layer.flow.failed_tcp' , 0 ) dcerpc_udp_events = entry_raw . get ( 'app_layer.flow.failed_udp' , 0 ) dns_udp_events = entry_raw . get ( 'app_layer.flow.dns_udp' , 0 ) krb5_udp_events = entry_raw . get ( 'app_layer.flow.krb5_udp' , 0 ) failed_udp_events = entry_raw . get ( 'app_layer.flow.failed_udp' , 0 ) return cls ( time , uptime , capture_kernel_packets , capture_kernel_drops , capture_errors , flow_memory , tcp_memory , tcp_reassembly_memory , dns_memory , http_memory , ftp_memory , http_events , tls_events , ssh_events , imap_events , msn_events , smb_events , dcerpc_tcp_events , dns_tcp_events , nfs_tcp_events , ntp_events , ftp_data_events , tftp_events , ikev2_data_events , krb5_tcp_events , dhcp_events , failed_tcp_events , dcerpc_udp_events , dns_udp_events , krb5_udp_events , failed_udp_events )","title":"create_from_raw_stats_entry()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MetricsEntry.get_total_memory","text":"Get the total amount of memory being used (in bytes) Warning Testing has proven this number to be unreliable. Returns: Type Description The total amount of memory being used by Suricata processes Source code in dynamite_nsm/services/suricata/logs.py def get_total_memory ( self ): \"\"\" Get the total amount of memory being used (in bytes) > **Warning** Testing has proven this number to be unreliable. Returns: The total amount of memory being used by Suricata processes \"\"\" return self . ftp_memory + self . http_memory + self . dns_memory + self . tcp_reassembly_memory + self . tcp_memory + \\ self . flow_memory","title":"get_total_memory()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.MetricsEntry.merge_metric_entry","text":"Merge another metrics entry into this one Parameters: Name Type Description Default metric_entry The MetricsEntry you wish to merge in required Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def merge_metric_entry ( self , metric_entry ): \"\"\"Merge another metrics entry into this one Args: metric_entry: The MetricsEntry you wish to merge in Returns: None \"\"\" if not isinstance ( metric_entry , MetricsEntry ): return self . capture_kernel_packets += metric_entry . capture_kernel_packets self . capture_kernel_drops += metric_entry . capture_kernel_drops self . capture_errors += metric_entry . capture_errors self . ftp_memory = math . ceil (( self . ftp_memory + metric_entry . ftp_memory ) / 2 ) self . flow_memory = math . ceil (( self . flow_memory + metric_entry . flow_memory ) / 2 ) self . http_memory = math . ceil (( self . http_memory + metric_entry . http_memory ) / 2 ) self . dns_memory = math . ceil (( self . dns_memory + metric_entry . dns_memory ) / 2 ) self . tcp_memory = math . ceil (( self . tcp_memory + metric_entry . tcp_memory ) / 2 ) self . tcp_reassembly_memory = \\ math . ceil (( self . tcp_reassembly_memory + metric_entry . tcp_reassembly_memory ) / 2 ) self . http_events += metric_entry . http_events self . tls_events += metric_entry . tls_events self . ssh_events += metric_entry . ssh_events self . imap_events += metric_entry . imap_events self . msn_events += metric_entry . msn_events self . smb_events += metric_entry . smb_events self . dcerpc_tcp_events += metric_entry . dcerpc_tcp_events self . dns_tcp_events += metric_entry . dns_tcp_events self . nfs_tcp_events += metric_entry . nfs_tcp_events self . ntp_events += metric_entry . ntp_events self . ftp_data_events += metric_entry . ftp_data_events self . tftp_events += metric_entry . tftp_events self . ikev2_data_events += metric_entry . ikev2_data_events self . krb5_tcp_events += metric_entry . krb5_tcp_events self . dhcp_events += metric_entry . dhcp_events self . failed_tcp_events += metric_entry . failed_tcp_events self . dcerpc_udp_events += metric_entry . dcerpc_udp_events self . dns_udp_events += metric_entry . dns_udp_events self . krb5_udp_events += metric_entry . krb5_udp_events self . failed_udp_events += metric_entry . failed_udp_events if self . capture_kernel_packets > 0 : self . capture_kernel_drops_percentage = round ( self . capture_kernel_drops / self . capture_kernel_packets , 6 )","title":"merge_metric_entry()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatsLog","text":"Provides an interface for working with Suricata's stats.log","title":"StatsLog"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatsLog.__init__","text":"Work with Suricata's stats.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries (or lines) to parse. 10000 The size of log_sample_size is set significantly higher as log entries can span multiple lines. Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 10000 ): \"\"\"Work with Suricata's stats.log Args: log_sample_size: The maximum number of entries (or lines) to parse. --- > The size of `log_sample_size` is set significantly higher as log entries can span multiple lines. \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . suricata_logs = self . env_dict . get ( 'SURICATA_LOGS' ) self . log_path = os . path . join ( self . suricata_logs , 'stats.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) self . log_path = os . path . join ( self . suricata_logs , 'stats.log' )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatsLog.iter_aggregated_metrics","text":"Aggregate events within tolerance_seconds into the same entry. Parameters: Name Type Description Default start Optional[datetime.datetime] UTC start time None end Optional[datetime.datetime] UTC end time None tolerance_seconds Optional[int] Specifies the maximum time distance between entries to combine them 60 Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_aggregated_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , tolerance_seconds : Optional [ int ] = 60 ): \"\"\"Aggregate events within tolerance_seconds into the same entry. Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum time distance between entries to combine them Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . time ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry","title":"iter_aggregated_metrics()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatsLog.iter_metrics","text":"Iterate through metrics entries individually. Parameters: Name Type Description Default start Optional[datetime.datetime] UTC start time None end Optional[datetime.datetime] UTC end time None Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ): \"\"\"Iterate through metrics entries individually. Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" self . _state_machine_parser () def filter_metrics ( s = None , e = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) prev_en = None for en_dict in self . entries : try : en = MetricsEntry . create_from_raw_stats_entry ( en_dict ) except ValueError : continue en_corrected = MetricsEntry . create_from_raw_stats_entry ( en_dict ) if s < en . time < e : if not prev_en : prev_en = en continue # A lot of Suricata stats are counters, meaning that they increment from zero starting at # process start. We address this by applying the difference between the current and previous # entries so that each entry represents an increment en_corrected . capture_kernel_packets = \\ max ( 0 , en . capture_kernel_packets - prev_en . capture_kernel_packets ) en_corrected . capture_errors = max ( 0 , en . capture_errors - prev_en . capture_errors ) en_corrected . capture_kernel_drops = \\ max ( 0 , en . capture_kernel_drops - prev_en . capture_kernel_drops ) en_corrected . http_events = max ( 0 , en . http_events - prev_en . http_events ) en_corrected . tls_events = max ( 0 , en . tls_events - prev_en . tls_events ) en_corrected . ssh_events = max ( 0 , en . ssh_events - prev_en . ssh_events ) en_corrected . imap_events = max ( 0 , en . imap_events - prev_en . imap_events ) en_corrected . msn_events = max ( 0 , en . msn_events - prev_en . msn_events ) en_corrected . smb_events = max ( 0 , en . smb_events - prev_en . smb_events ) en_corrected . dcerpc_tcp_events = max ( 0 , en . dcerpc_tcp_events - prev_en . dcerpc_tcp_events ) en_corrected . dns_tcp_events = max ( 0 , en . dns_tcp_events - prev_en . dns_tcp_events ) en_corrected . nfs_tcp_events = max ( 0 , en . nfs_tcp_events - prev_en . nfs_tcp_events ) en_corrected . ntp_events = max ( 0 , en . ntp_events - prev_en . ntp_events ) en_corrected . ftp_data_events = max ( 0 , en . ftp_data_events - prev_en . ftp_data_events ) en_corrected . tftp_events = max ( 0 , en . tftp_events - prev_en . tftp_events ) en_corrected . ikev2_data_events = max ( 0 , en . ikev2_data_events - prev_en . ikev2_data_events ) en_corrected . krb5_tcp_events = max ( 0 , en . krb5_tcp_events - prev_en . krb5_tcp_events ) en_corrected . dhcp_events = max ( 0 , en . dhcp_events - prev_en . dhcp_events ) en_corrected . failed_tcp_events = max ( 0 , en . failed_tcp_events - prev_en . failed_tcp_events ) en_corrected . dcerpc_udp_events = max ( 0 , en . dcerpc_udp_events - prev_en . dcerpc_udp_events ) en_corrected . dns_udp_events = max ( 0 , en . dns_udp_events - prev_en . dns_udp_events ) en_corrected . krb5_udp_events = max ( 0 , en . krb5_udp_events - prev_en . krb5_udp_events ) en_corrected . failed_udp_events = max ( 0 , en . failed_udp_events - prev_en . failed_udp_events ) prev_en = en yield en_corrected for log_entry in filter_metrics ( start , end ): yield log_entry","title":"iter_metrics()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatsLog.tail","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None Source code in dynamite_nsm/services/suricata/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ): \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for metric in self . iter_aggregated_metrics ( start = start , end = end ): if metric . timestamp not in visited : visited . append ( metric . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( metric )), indent = 1 )) else : status_table = [ [ 'Time' , 'Memory' , 'Packets Captured' , 'Packets Dropped' , 'Errors During Capture' ], [ metric . time , metric . get_total_memory (), metric . capture_kernel_packets , metric . capture_kernel_drops , metric . capture_errors ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatusLogEve","text":"A status entry from Suricata's eve.json","title":"StatusLogEve"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatusLogEve.__init__","text":"A status entry from eve.json log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of entries (or lines) to parse 10000 Source code in dynamite_nsm/services/suricata/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 10000 ): \"\"\"A status entry from eve.json log Args: log_sample_size: The maximum number of entries (or lines) to parse \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . suricata_logs = self . env_dict . get ( 'SURICATA_LOGS' ) self . log_path = os . path . join ( self . suricata_logs , 'eve.json' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatusLogEve.iter_aggregated_metrics","text":"Aggregate events within tolerance_seconds into the same entry. Parameters: Name Type Description Default start UTC start time None end UTC end time None tolerance_seconds Specifies the maximum time distance between entries to combine them 60 Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_aggregated_metrics ( self , start = None , end = None , tolerance_seconds = 60 ): \"\"\"Aggregate events within tolerance_seconds into the same entry. Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum time distance between entries to combine them Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . time ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry","title":"iter_aggregated_metrics()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.StatusLogEve.iter_metrics","text":"Iterate through metrics entries individually. Parameters: Name Type Description Default start UTC start time None end UTC end time None Returns: Type Description yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/suricata/logs.py def iter_metrics ( self , start = None , end = None ): \"\"\"Iterate through metrics entries individually. Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" def filter_metrics ( s = None , e = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) prev_en = None for en_raw in self . entries : if '\"event_type\":\"stats\"' in en_raw : try : en = MetricsEntry . create_from_eve_raw_stats ( json . loads ( en_raw )) except ValueError : continue en_corrected = MetricsEntry . create_from_eve_raw_stats ( json . loads ( en_raw )) if s < en . time < e : if not prev_en : prev_en = en continue # A lot of Suricata stats are counters, meaning that they increment from zero starting at # process start. We address this by applying the difference between the current and previous # entries so that each entry represents an increment en_corrected . capture_kernel_packets = \\ max ( 0 , en . capture_kernel_packets - prev_en . capture_kernel_packets ) en_corrected . capture_errors = max ( 0 , en . capture_errors - prev_en . capture_errors ) en_corrected . capture_kernel_drops = \\ max ( 0 , en . capture_kernel_drops - prev_en . capture_kernel_drops ) en_corrected . http_events = max ( 0 , en . http_events - prev_en . http_events ) en_corrected . tls_events = max ( 0 , en . tls_events - prev_en . tls_events ) en_corrected . ssh_events = max ( 0 , en . ssh_events - prev_en . ssh_events ) en_corrected . imap_events = max ( 0 , en . imap_events - prev_en . imap_events ) en_corrected . msn_events = max ( 0 , en . msn_events - prev_en . msn_events ) en_corrected . smb_events = max ( 0 , en . smb_events - prev_en . smb_events ) en_corrected . dcerpc_tcp_events = max ( 0 , en . dcerpc_tcp_events - prev_en . dcerpc_tcp_events ) en_corrected . dns_tcp_events = max ( 0 , en . dns_tcp_events - prev_en . dns_tcp_events ) en_corrected . nfs_tcp_events = max ( 0 , en . nfs_tcp_events - prev_en . nfs_tcp_events ) en_corrected . ntp_events = max ( 0 , en . ntp_events - prev_en . ntp_events ) en_corrected . ftp_data_events = max ( 0 , en . ftp_data_events - prev_en . ftp_data_events ) en_corrected . tftp_events = max ( 0 , en . tftp_events - prev_en . tftp_events ) en_corrected . ikev2_data_events = max ( 0 , en . ikev2_data_events - prev_en . ikev2_data_events ) en_corrected . krb5_tcp_events = max ( 0 , en . krb5_tcp_events - prev_en . krb5_tcp_events ) en_corrected . dhcp_events = max ( 0 , en . dhcp_events - prev_en . dhcp_events ) en_corrected . failed_tcp_events = max ( 0 , en . failed_tcp_events - prev_en . failed_tcp_events ) en_corrected . dcerpc_udp_events = max ( 0 , en . dcerpc_udp_events - prev_en . dcerpc_udp_events ) en_corrected . dns_udp_events = max ( 0 , en . dns_udp_events - prev_en . dns_udp_events ) en_corrected . krb5_udp_events = max ( 0 , en . krb5_udp_events - prev_en . krb5_udp_events ) en_corrected . failed_udp_events = max ( 0 , en . failed_udp_events - prev_en . failed_udp_events ) prev_en = en yield en_corrected for log_entry in filter_metrics ( start , end ): yield log_entry","title":"iter_metrics()"},{"location":"guides/developers/SDK/services/suricata/logs/#dynamite_nsm.services.suricata.logs.parse_suricata_datetime","text":"Parse a common suricata timestamp string Parameters: Name Type Description Default t str A '%Y-%m-%dT%H:%M:%S.%f' formatted string required Returns: A datetime object Source code in dynamite_nsm/services/suricata/logs.py def parse_suricata_datetime ( t : str ) -> datetime : \"\"\" Parse a common suricata timestamp string Args: t: A '%Y-%m-%dT%H:%M:%S.%f' formatted string Returns: A datetime object \"\"\" ret = datetime . strptime ( t [ 0 : 22 ], '%Y-%m- %d T%H:%M:%S. %f ' ) if t [ 26 ] == '+' : ret -= timedelta ( hours = int ( t [ 27 : 29 ]), minutes = int ( t [ 30 :])) elif t [ 26 ] == '-' : ret += timedelta ( hours = int ( t [ 27 : 29 ]), minutes = int ( t [ 30 :])) return ret","title":"parse_suricata_datetime()"},{"location":"guides/developers/SDK/services/suricata/process/","text":"Process Manager for Suricata processes and sub-processes. To import... from dynamite_nsm.services.suricata import process as suricata_process CallSuricataProcessError __init__ ( self , message ) special Thrown when suricata process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/suricata/process.py def __init__ ( self , message ): \"\"\"Thrown when suricata process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling suricata process: {} \" . format ( message ) super ( CallSuricataProcessError , self ) . __init__ ( msg ) ProcessManager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Suricata processes and sub-processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If true, status will be printed in a tabular form False Source code in dynamite_nsm/services/suricata/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Suricata processes and sub-processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If true, status will be printed in a tabular form \"\"\" process . BaseProcessManager . __init__ ( self , 'suricata.service' , 'suricata.process' , log_path = None , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not profile . ProcessProfiler () . is_installed (): raise exceptions . CallProcessError ( \"Suricata is not installed.\" )","title":"process"},{"location":"guides/developers/SDK/services/suricata/process/#dynamite_nsm.services.suricata.process.CallSuricataProcessError","text":"","title":"CallSuricataProcessError"},{"location":"guides/developers/SDK/services/suricata/process/#dynamite_nsm.services.suricata.process.CallSuricataProcessError.__init__","text":"Thrown when suricata process encounters an error state Parameters: Name Type Description Default message A more specific error message required Returns: Type Description None Source code in dynamite_nsm/services/suricata/process.py def __init__ ( self , message ): \"\"\"Thrown when suricata process encounters an error state Args: message: A more specific error message Returns: None \"\"\" msg = \"An error occurred while calling suricata process: {} \" . format ( message ) super ( CallSuricataProcessError , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/process/#dynamite_nsm.services.suricata.process.ProcessManager","text":"","title":"ProcessManager"},{"location":"guides/developers/SDK/services/suricata/process/#dynamite_nsm.services.suricata.process.ProcessManager.__init__","text":"Manage Suricata processes and sub-processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If true, status will be printed in a tabular form False Source code in dynamite_nsm/services/suricata/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Suricata processes and sub-processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If true, status will be printed in a tabular form \"\"\" process . BaseProcessManager . __init__ ( self , 'suricata.service' , 'suricata.process' , log_path = None , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not profile . ProcessProfiler () . is_installed (): raise exceptions . CallProcessError ( \"Suricata is not installed.\" )","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/profile/","text":"Profile Suricata processes to ensure they are running and installed properly. To import... from dynamite_nsm.services.suricata import profile as suricata_profile ProcessProfiler __init__ ( self ) special Get information about the Suricata service Source code in dynamite_nsm/services/suricata/profile.py def __init__ ( self ): \"\"\" Get information about the Suricata service \"\"\" self . env_dict = utilities . get_environment_file_dict () self . suricata_home = self . env_dict . get ( 'SURICATA_HOME' ) self . suricata_config = self . env_dict . get ( 'SURICATA_CONFIG' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . suricata_home , config_directory = self . suricata_config , required_install_files = [ 'bin' , 'include' , 'lib' ], required_config_files = [ 'rules' ]) is_attached_to_network ( self ) Determine if Suricata is bound to one or more network interfaces Returns: Type Description bool True, if attached to one or more network interfaces Source code in dynamite_nsm/services/suricata/profile.py def is_attached_to_network ( self ) -> bool : \"\"\"Determine if Suricata is bound to one or more network interfaces Returns: True, if attached to one or more network interfaces \"\"\" return any ( self . get_attached_interfaces ()) is_running ( self ) Determine of Suricata is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/suricata/profile.py def is_running ( self ) -> bool : \"\"\" Determine of Suricata is running Returns: True, if running \"\"\" if self . suricata_home : try : return suricata_process . ProcessManager () . status ()[ 'running' ] except KeyError : return suricata_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"profile"},{"location":"guides/developers/SDK/services/suricata/profile/#dynamite_nsm.services.suricata.profile.ProcessProfiler","text":"","title":"ProcessProfiler"},{"location":"guides/developers/SDK/services/suricata/profile/#dynamite_nsm.services.suricata.profile.ProcessProfiler.__init__","text":"Get information about the Suricata service Source code in dynamite_nsm/services/suricata/profile.py def __init__ ( self ): \"\"\" Get information about the Suricata service \"\"\" self . env_dict = utilities . get_environment_file_dict () self . suricata_home = self . env_dict . get ( 'SURICATA_HOME' ) self . suricata_config = self . env_dict . get ( 'SURICATA_CONFIG' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . suricata_home , config_directory = self . suricata_config , required_install_files = [ 'bin' , 'include' , 'lib' ], required_config_files = [ 'rules' ])","title":"__init__()"},{"location":"guides/developers/SDK/services/suricata/profile/#dynamite_nsm.services.suricata.profile.ProcessProfiler.is_attached_to_network","text":"Determine if Suricata is bound to one or more network interfaces Returns: Type Description bool True, if attached to one or more network interfaces Source code in dynamite_nsm/services/suricata/profile.py def is_attached_to_network ( self ) -> bool : \"\"\"Determine if Suricata is bound to one or more network interfaces Returns: True, if attached to one or more network interfaces \"\"\" return any ( self . get_attached_interfaces ())","title":"is_attached_to_network()"},{"location":"guides/developers/SDK/services/suricata/profile/#dynamite_nsm.services.suricata.profile.ProcessProfiler.is_running","text":"Determine of Suricata is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/suricata/profile.py def is_running ( self ) -> bool : \"\"\" Determine of Suricata is running Returns: True, if running \"\"\" if self . suricata_home : try : return suricata_process . ProcessManager () . status ()[ 'running' ] except KeyError : return suricata_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"is_running()"},{"location":"guides/developers/SDK/services/zeek/config/","text":"Configuration wrappers for a variety of Zeek related configuration files. To import... from dynamite_nsm.services.zeek import config as zeek_config BpfConfigManager Manage Berkley Packet Filters for Zeek __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Configure Berkley Packet Filters for Zeek monitored interfaces. Parameters: Name Type Description Default configuration_directory The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Instance Variables: bpf_filters - A bpf_filter.BpfFilters instance. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , configuration_directory , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configure Berkley Packet Filters for Zeek monitored interfaces. Args: configuration_directory: The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `bpf_filters` - A `bpf_filter.BpfFilters` instance. \"\"\" self . configuration_directory = configuration_directory self . bpf_filters = bpf_filter . BpfFilters () with open ( f ' { self . configuration_directory } /bpf_map_file.input' ) as config_f : config_data = dict ( data = config_f . readlines ()) super () . __init__ ( config_data , name = 'zeek.config.bpf' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = lambda data : bpf_filter . BpfFilters ( [ bpf_filter . BpfFilter ( interface_name = line . split ( ' \\t ' )[ 0 ] . strip (), pattern = line . split ( ' \\t ' )[ 1 ] . strip () ) for line in data [ 'data' ] if ' \\t ' in line . strip () . replace ( ' ' , '' )] ), attribute_name = 'bpf_filters' ) commit ( self , out_file_path = None , backup_directory = None ) Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /bpf_map_file.input' self . formatted_data = ' \\n ' . join ( self . bpf_filters . get_raw ()) super ( BpfConfigManager , self ) . commit ( out_file_path , backup_directory ) LocalNetworksConfigManager Manage the networks network.cfg for defining which networks Zeek will consider local __init__ ( self , install_directory , verbose = False , stdout = True ) special Configure the networks Zeek will consider local to the monitoring environment Parameters: Name Type Description Default install_directory str The path to the installation directory (E.G /opt/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Instance Variables: local_networks - A local_network.LocalNetworks instance representing a list of networks considered local by this cluster. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , install_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure the networks Zeek will consider local to the monitoring environment Args: install_directory: The path to the installation directory (E.G /opt/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `local_networks` - A `local_network.LocalNetworks` instance representing a list of networks considered local by this cluster. \"\"\" self . install_directory = install_directory self . local_networks = local_network . LocalNetworks () with open ( f ' { self . install_directory } /etc/networks.cfg' ) as config_f : config_data = dict ( data = config_f . readlines ()) super () . __init__ ( config_data , name = 'zeek.config.networks' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = self . _parse_local_networks , attribute_name = 'local_networks' ) commit ( self , out_file_path = None , backup_directory = None ) Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . install_directory } /etc/networks.cfg' self . formatted_data = ' \\n ' . join ( self . local_networks . get_raw ()) super ( LocalNetworksConfigManager , self ) . commit ( out_file_path , backup_directory ) from_raw_text ( raw_text , install_directory = None ) classmethod Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required install_directory Optional[str] The installation directory where the config file resides None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/zeek/config.py @classmethod def from_raw_text ( cls , raw_text : str , install_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file install_directory: The installation directory where the config file resides Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp/etc' tmp_config = f ' { tmp_dir } /networks.cfg' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( install_directory = f \" { tmp_dir } /../\" ) if install_directory : c . install_directory = install_directory return c NodeConfigManager Manage Zeek node.cfg used to determine which network interfaces to monitor __init__ ( self , install_directory , verbose = False , stdout = True ) special Configuration Manager for node.cfg file Parameters: Name Type Description Default install_directory str The path to the Zeek installation directory required Instance Variables: manager - A basic node.Manager instance representing a manager configuration (one per cluster) loggers - A node.Loggers instance representing one or more loggers. Loggers alleviate manager load proxies A node.Proxies instance representing one or more proxies. Offload workloads. workers A node.Workers instance representing one or more workers. The worker is the Zeek process that sniffs network traffic and does protocol analysis on the reassembled traffic streams. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , install_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configuration Manager for node.cfg file Args: install_directory: The path to the Zeek installation directory ___ # Instance Variables: - `manager` - A basic `node.Manager` instance representing a manager configuration (one per cluster) - `loggers` - A `node.Loggers` instance representing one or more loggers. Loggers alleviate manager load - `proxies` A `node.Proxies` instance representing one or more proxies. Offload workloads. - `workers` A `node.Workers` instance representing one or more workers. The worker is the Zeek process that sniffs network traffic and does protocol analysis on the reassembled traffic streams. \"\"\" self . install_directory = install_directory self . manager = None self . loggers = node . Loggers () self . proxies = node . Proxies () self . workers = node . Workers () config_parser = ConfigParser () with open ( f ' { self . install_directory } /etc/node.cfg' ) as config_f : config_parser . read_file ( config_f ) config_data = {} for section in config_parser . sections (): config_data [ section ] = {} for item in config_parser . items ( section ): key , value = item config_data [ section ][ key ] = value super () . __init__ ( config_data , name = 'zeek.config.node' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = lambda data : [ node . Manager ( manager_name = name , host = values . get ( 'host' ) ) for name , values in data . items () if values . get ( 'type' ) == 'manager' ][ 0 ], attribute_name = 'manager' ) self . add_parser ( parser = lambda data : node . Loggers ( [ node . Logger ( logger_name = name , host = values . get ( 'host' ) ) for name , values in data . items () if values . get ( 'type' ) == 'logger' ]), attribute_name = 'loggers' ) self . add_parser ( parser = lambda data : node . Proxies ( [ node . Proxy ( proxy_name = name , host = values . get ( 'host' ) ) for name , values in data . items () if values . get ( 'type' ) == 'proxy' ]), attribute_name = 'proxies' ) self . add_parser ( parser = lambda data : node . Workers ( [ node . Worker ( worker_name = name , interface_name = values . get ( 'interface' ), cluster_id = int ( values . get ( 'af_packet_fanout_id' )), cluster_type = values . get ( 'af_packet_fanout_mode' , 'FANOUT_HASH' ), load_balance_processes = int ( values . get ( 'lb_procs' )), pinned_cpus = tuple ( [ int ( cpu ) for cpu in values . get ( 'pin_cpus' , '' ) . split ( ',' )]), host = values . get ( 'host' )) for name , values in data . items () if values . get ( 'type' ) == 'worker' ]), attribute_name = 'workers' ) commit ( self , out_file_path = None , backup_directory = None ) Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" config_p = ConfigParser () def build_raw_component ( components : node . BaseComponents ): for component in components : config_p . add_section ( component . name ) for k , v in component . get_raw ()[ 1 ] . items (): config_p . set ( component . name , k , v ) if not out_file_path : out_file_path = f ' { self . install_directory } /etc/node.cfg' build_raw_component ( self . loggers ) build_raw_component ( self . proxies ) build_raw_component ( self . workers ) build_raw_component ( node . BaseComponents ([ self . manager ])) # A hack to maintain parody with the parent class self . formatted_data = StringIO () config_p . write ( self . formatted_data ) self . formatted_data . seek ( 0 ) self . formatted_data = self . formatted_data . read () super ( NodeConfigManager , self ) . commit ( out_file_path , backup_directory ) from_raw_text ( raw_text , install_directory = None ) classmethod Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required install_directory Optional[str] The install directory for Zeek None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/zeek/config.py @classmethod def from_raw_text ( cls , raw_text : str , install_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file install_directory: The install directory for Zeek Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp/etc' tmp_config = f ' { tmp_dir } /node.cfg' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( install_directory = f \" { tmp_dir } /../\" ) if install_directory : c . install_directory = install_directory return c get_optimal_zeek_worker_config ( interface_names , available_cpus = None ) staticmethod Algorithm for determining the assignment of CPUs for Zeek workers Parameters: Name Type Description Default interface_names List[str] A list of network interface names required available_cpus Optional[Tuple] If None, we'll derive this by looking at the cpu core count, otherwise a list of cpu cores None Returns: Type Description Workers A node.Workers object Source code in dynamite_nsm/services/zeek/config.py @staticmethod def get_optimal_zeek_worker_config ( interface_names : List [ str ], available_cpus : Optional [ Tuple ] = None ) -> node . Workers : \"\"\"Algorithm for determining the assignment of CPUs for Zeek workers Args: interface_names: A list of network interface names available_cpus: If None, we'll derive this by looking at the cpu core count, otherwise a list of cpu cores Returns: A node.Workers object \"\"\" zeek_worker_configs = node . Workers () if not available_cpus : # Reserve CPU 0 for KERNEL operations available_cpus = [ c for c in range ( 1 , utilities . get_cpu_core_count ())] for cpu_affinity_group in utilities . get_optimal_cpu_interface_config ( interface_names = interface_names , available_cpus = available_cpus ): net_interface = cpu_affinity_group [ 'interface_name' ] pinned_cpus = cpu_affinity_group [ 'pin_cpus' ] lb_processes = cpu_affinity_group [ 'thread_count' ] zeek_worker_configs . add_worker ( node . Worker ( worker_name = 'dynamite-worker-' + net_interface , host = 'localhost' , interface_name = net_interface , load_balance_processes = lb_processes , pinned_cpus = pinned_cpus , cluster_id = randint ( 1 , 32768 ), cluster_type = 'AF_Packet::FANOUT_QM' ) ) return zeek_worker_configs reset ( self , inspect_interfaces , out_file_path = None , default_config_path = None ) Reset a configuration file back to its default Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/zeek/config.py def reset ( self , inspect_interfaces : List [ str ], out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not install . BaseInstallManager . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) if not out_file_path : out_file_path = f ' { self . install_directory } /etc/node.cfg' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /zeek/broctl-nodes.cfg' super ( NodeConfigManager , self ) . reset ( out_file_path , default_config_path ) self . workers = node . Workers () for worker in self . get_optimal_zeek_worker_config ( inspect_interfaces ): self . workers . add_worker ( worker = worker ) self . commit ( out_file_path = out_file_path ) SiteLocalConfigManager Manage local/site.zeek file (contains scripts, definitions, and signatures to be loaded) __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Configure Zeek scripts, signatures, and definitions Parameters: Name Type Description Default configuration_directory str The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Instance Variables: scripts - A local_site.Scripts instance representing a set of enabled (and disabled) Zeek scripts. signatures A local_site.Signatures instance representing a set of signatures to load. definitions - A local_site.Definitions instance representing a set of script variables redefs . Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure Zeek scripts, signatures, and definitions Args: configuration_directory: The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `scripts` - A `local_site.Scripts` instance representing a set of enabled (and disabled) Zeek scripts. - `signatures` A `local_site.Signatures` instance representing a set of signatures to load. - `definitions` - A `local_site.Definitions` instance representing a set of script variables `redefs`. \"\"\" self . configuration_directory = configuration_directory self . scripts = local_site . Scripts () self . signatures = local_site . Signatures () self . definitions = local_site . Definitions () with open ( f ' { self . configuration_directory } /site/local.zeek' ) as config_f : config_data = dict ( data = config_f . readlines ()) super () . __init__ ( config_data , name = 'zeek.config.local' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = lambda data : local_site . Scripts ( [ local_site . Script ( name = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 5 :], enabled = line . replace ( ' ' , '' ) . strip ()[ 0 ] != '#' ) for line in data [ 'data' ] if self . _line_denotes_script ( line )] ), attribute_name = 'scripts' ) self . add_parser ( parser = lambda data : local_site . Signatures ( [ local_site . Signature ( name = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 10 :], enabled = line . replace ( ' ' , '' ) . strip ()[ 0 ] != '#' ) for line in data [ 'data' ] if self . _line_denotes_signature ( line )] ), attribute_name = 'signatures' ) self . add_parser ( parser = lambda data : local_site . Definitions ( [ local_site . Definition ( name = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 5 :] . split ( '=' )[ 0 ], value = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 5 :] . split ( '=' )[ 1 ], enabled = line . replace ( ' ' , '' ) . strip ()[ 0 ] != '#' ) for line in data [ 'data' ] if self . _line_denotes_definition ( line )] ), attribute_name = 'definitions' ) commit ( self , out_file_path = None , backup_directory = None ) Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /site/local.zeek' self . formatted_data = ' \\n ' . join ( self . signatures . get_raw () + self . scripts . get_raw () + self . definitions . get_raw () ) super ( SiteLocalConfigManager , self ) . commit ( out_file_path , backup_directory ) disable_all_definitions ( self ) Disable all definitions Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def disable_all_definitions ( self ) -> None : \"\"\"Disable all definitions Returns: None \"\"\" for definition in self . definitions : definition . enabled = False disable_all_scripts ( self ) Disable all scripts Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def disable_all_scripts ( self ) -> None : \"\"\"Disable all scripts Returns: None \"\"\" for script in self . scripts : script . enabled = False disable_all_signatures ( self ) Disable all scripts Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def disable_all_signatures ( self ) -> None : \"\"\"Disable all scripts Returns: None \"\"\" for sig in self . signatures : sig . enabled = False enable_all_definitions ( self ) Enable all definitions Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def enable_all_definitions ( self ) -> None : \"\"\"Enable all definitions Returns: None \"\"\" for definition in self . definitions : definition . enabled = False enable_all_scripts ( self ) Enable all scripts Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def enable_all_scripts ( self ) -> None : \"\"\"Enable all scripts Returns: None \"\"\" for script in self . scripts : script . enabled = True enable_all_signatures ( self ) Enable all signatures Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def enable_all_signatures ( self ) -> None : \"\"\"Enable all signatures Returns: None \"\"\" for sig in self . signatures : sig . enabled = True from_raw_text ( raw_text , configuration_directory = None ) classmethod Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required configuration_directory Optional[str] The configuration directory for Zeek None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/zeek/config.py @classmethod def from_raw_text ( cls , raw_text : str , configuration_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file configuration_directory: The configuration directory for Zeek Returns: An instance of ConfigManager \"\"\" tmp_root = f ' { const . CONFIG_PATH } /.tmp' tmp_dir = f ' { tmp_root } /site' tmp_config = f ' { tmp_dir } /local.zeek' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( configuration_directory = tmp_root ) if configuration_directory : c . configuration_directory = configuration_directory return c reset ( self , out_file_path = None , default_config_path = None ) Reset a configuration file back to its default Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/zeek/config.py def reset ( self , out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /site/local.zeek' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /zeek/local.zeek' super ( SiteLocalConfigManager , self ) . reset ( out_file_path , default_config_path ) self . commit ( out_file_path = out_file_path ) SiteLocalPackageManager __init__ ( self , configuration_directory , verbose = False , stdout = True ) special Configure Zeek packages installed through ZKG Parameters: Name Type Description Default configuration_directory str The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True Instance Variables: scripts - A local_site.Scripts instance representing a set of enabled (and disabled) Zeek scripts. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure Zeek packages installed through ZKG Args: configuration_directory: The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `scripts` - A `local_site.Scripts` instance representing a set of enabled (and disabled) Zeek scripts. \"\"\" self . configuration_directory = configuration_directory self . packages = [] self . _load_packages () lookup_script_definition ( script_id ) Return the definition, categories, and friendly_name of a given script Parameters: Name Type Description Default script_id str A unique identifier representing a Zeek script. required Returns: Type Description Dict A dictionary of the format {\"friendly_name\" , \"description\" , \"categories\" } Source code in dynamite_nsm/services/zeek/config.py def lookup_script_definition ( script_id : str ) -> Dict : \"\"\"Return the definition, categories, and friendly_name of a given script Args: script_id: A unique identifier representing a Zeek script. Returns: A dictionary of the format {\"friendly_name\" <str>, \"description\" <str>, \"categories\" <list>} \"\"\" try : zeek_script_defs = os . path . join ( const . DEFAULT_CONFIGS , 'zeek' , 'zeek_script_definitions.json' ) with open ( zeek_script_defs ) as f : zeek_defs = json . load ( f ) except FileNotFoundError : zeek_defs = {} definition = zeek_defs . get ( str ( script_id )) return definition","title":"config"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.BpfConfigManager","text":"Manage Berkley Packet Filters for Zeek","title":"BpfConfigManager"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.BpfConfigManager.__init__","text":"Configure Berkley Packet Filters for Zeek monitored interfaces. Parameters: Name Type Description Default configuration_directory The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.BpfConfigManager.__init__--instance-variables","text":"bpf_filters - A bpf_filter.BpfFilters instance. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , configuration_directory , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configure Berkley Packet Filters for Zeek monitored interfaces. Args: configuration_directory: The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `bpf_filters` - A `bpf_filter.BpfFilters` instance. \"\"\" self . configuration_directory = configuration_directory self . bpf_filters = bpf_filter . BpfFilters () with open ( f ' { self . configuration_directory } /bpf_map_file.input' ) as config_f : config_data = dict ( data = config_f . readlines ()) super () . __init__ ( config_data , name = 'zeek.config.bpf' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = lambda data : bpf_filter . BpfFilters ( [ bpf_filter . BpfFilter ( interface_name = line . split ( ' \\t ' )[ 0 ] . strip (), pattern = line . split ( ' \\t ' )[ 1 ] . strip () ) for line in data [ 'data' ] if ' \\t ' in line . strip () . replace ( ' ' , '' )] ), attribute_name = 'bpf_filters' )","title":"Instance Variables:"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.BpfConfigManager.commit","text":"Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /bpf_map_file.input' self . formatted_data = ' \\n ' . join ( self . bpf_filters . get_raw ()) super ( BpfConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.LocalNetworksConfigManager","text":"Manage the networks network.cfg for defining which networks Zeek will consider local","title":"LocalNetworksConfigManager"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.LocalNetworksConfigManager.__init__","text":"Configure the networks Zeek will consider local to the monitoring environment Parameters: Name Type Description Default install_directory str The path to the installation directory (E.G /opt/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.LocalNetworksConfigManager.__init__--instance-variables","text":"local_networks - A local_network.LocalNetworks instance representing a list of networks considered local by this cluster. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , install_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure the networks Zeek will consider local to the monitoring environment Args: install_directory: The path to the installation directory (E.G /opt/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `local_networks` - A `local_network.LocalNetworks` instance representing a list of networks considered local by this cluster. \"\"\" self . install_directory = install_directory self . local_networks = local_network . LocalNetworks () with open ( f ' { self . install_directory } /etc/networks.cfg' ) as config_f : config_data = dict ( data = config_f . readlines ()) super () . __init__ ( config_data , name = 'zeek.config.networks' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = self . _parse_local_networks , attribute_name = 'local_networks' )","title":"Instance Variables:"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.LocalNetworksConfigManager.commit","text":"Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . install_directory } /etc/networks.cfg' self . formatted_data = ' \\n ' . join ( self . local_networks . get_raw ()) super ( LocalNetworksConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.LocalNetworksConfigManager.from_raw_text","text":"Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required install_directory Optional[str] The installation directory where the config file resides None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/zeek/config.py @classmethod def from_raw_text ( cls , raw_text : str , install_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file install_directory: The installation directory where the config file resides Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp/etc' tmp_config = f ' { tmp_dir } /networks.cfg' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( install_directory = f \" { tmp_dir } /../\" ) if install_directory : c . install_directory = install_directory return c","title":"from_raw_text()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager","text":"Manage Zeek node.cfg used to determine which network interfaces to monitor","title":"NodeConfigManager"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager.__init__","text":"Configuration Manager for node.cfg file Parameters: Name Type Description Default install_directory str The path to the Zeek installation directory required","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager.__init__--instance-variables","text":"manager - A basic node.Manager instance representing a manager configuration (one per cluster) loggers - A node.Loggers instance representing one or more loggers. Loggers alleviate manager load proxies A node.Proxies instance representing one or more proxies. Offload workloads. workers A node.Workers instance representing one or more workers. The worker is the Zeek process that sniffs network traffic and does protocol analysis on the reassembled traffic streams. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , install_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\"Configuration Manager for node.cfg file Args: install_directory: The path to the Zeek installation directory ___ # Instance Variables: - `manager` - A basic `node.Manager` instance representing a manager configuration (one per cluster) - `loggers` - A `node.Loggers` instance representing one or more loggers. Loggers alleviate manager load - `proxies` A `node.Proxies` instance representing one or more proxies. Offload workloads. - `workers` A `node.Workers` instance representing one or more workers. The worker is the Zeek process that sniffs network traffic and does protocol analysis on the reassembled traffic streams. \"\"\" self . install_directory = install_directory self . manager = None self . loggers = node . Loggers () self . proxies = node . Proxies () self . workers = node . Workers () config_parser = ConfigParser () with open ( f ' { self . install_directory } /etc/node.cfg' ) as config_f : config_parser . read_file ( config_f ) config_data = {} for section in config_parser . sections (): config_data [ section ] = {} for item in config_parser . items ( section ): key , value = item config_data [ section ][ key ] = value super () . __init__ ( config_data , name = 'zeek.config.node' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = lambda data : [ node . Manager ( manager_name = name , host = values . get ( 'host' ) ) for name , values in data . items () if values . get ( 'type' ) == 'manager' ][ 0 ], attribute_name = 'manager' ) self . add_parser ( parser = lambda data : node . Loggers ( [ node . Logger ( logger_name = name , host = values . get ( 'host' ) ) for name , values in data . items () if values . get ( 'type' ) == 'logger' ]), attribute_name = 'loggers' ) self . add_parser ( parser = lambda data : node . Proxies ( [ node . Proxy ( proxy_name = name , host = values . get ( 'host' ) ) for name , values in data . items () if values . get ( 'type' ) == 'proxy' ]), attribute_name = 'proxies' ) self . add_parser ( parser = lambda data : node . Workers ( [ node . Worker ( worker_name = name , interface_name = values . get ( 'interface' ), cluster_id = int ( values . get ( 'af_packet_fanout_id' )), cluster_type = values . get ( 'af_packet_fanout_mode' , 'FANOUT_HASH' ), load_balance_processes = int ( values . get ( 'lb_procs' )), pinned_cpus = tuple ( [ int ( cpu ) for cpu in values . get ( 'pin_cpus' , '' ) . split ( ',' )]), host = values . get ( 'host' )) for name , values in data . items () if values . get ( 'type' ) == 'worker' ]), attribute_name = 'workers' )","title":"Instance Variables:"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager.commit","text":"Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" config_p = ConfigParser () def build_raw_component ( components : node . BaseComponents ): for component in components : config_p . add_section ( component . name ) for k , v in component . get_raw ()[ 1 ] . items (): config_p . set ( component . name , k , v ) if not out_file_path : out_file_path = f ' { self . install_directory } /etc/node.cfg' build_raw_component ( self . loggers ) build_raw_component ( self . proxies ) build_raw_component ( self . workers ) build_raw_component ( node . BaseComponents ([ self . manager ])) # A hack to maintain parody with the parent class self . formatted_data = StringIO () config_p . write ( self . formatted_data ) self . formatted_data . seek ( 0 ) self . formatted_data = self . formatted_data . read () super ( NodeConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager.from_raw_text","text":"Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required install_directory Optional[str] The install directory for Zeek None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/zeek/config.py @classmethod def from_raw_text ( cls , raw_text : str , install_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file install_directory: The install directory for Zeek Returns: An instance of ConfigManager \"\"\" tmp_dir = f ' { const . CONFIG_PATH } /.tmp/etc' tmp_config = f ' { tmp_dir } /node.cfg' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( install_directory = f \" { tmp_dir } /../\" ) if install_directory : c . install_directory = install_directory return c","title":"from_raw_text()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager.get_optimal_zeek_worker_config","text":"Algorithm for determining the assignment of CPUs for Zeek workers Parameters: Name Type Description Default interface_names List[str] A list of network interface names required available_cpus Optional[Tuple] If None, we'll derive this by looking at the cpu core count, otherwise a list of cpu cores None Returns: Type Description Workers A node.Workers object Source code in dynamite_nsm/services/zeek/config.py @staticmethod def get_optimal_zeek_worker_config ( interface_names : List [ str ], available_cpus : Optional [ Tuple ] = None ) -> node . Workers : \"\"\"Algorithm for determining the assignment of CPUs for Zeek workers Args: interface_names: A list of network interface names available_cpus: If None, we'll derive this by looking at the cpu core count, otherwise a list of cpu cores Returns: A node.Workers object \"\"\" zeek_worker_configs = node . Workers () if not available_cpus : # Reserve CPU 0 for KERNEL operations available_cpus = [ c for c in range ( 1 , utilities . get_cpu_core_count ())] for cpu_affinity_group in utilities . get_optimal_cpu_interface_config ( interface_names = interface_names , available_cpus = available_cpus ): net_interface = cpu_affinity_group [ 'interface_name' ] pinned_cpus = cpu_affinity_group [ 'pin_cpus' ] lb_processes = cpu_affinity_group [ 'thread_count' ] zeek_worker_configs . add_worker ( node . Worker ( worker_name = 'dynamite-worker-' + net_interface , host = 'localhost' , interface_name = net_interface , load_balance_processes = lb_processes , pinned_cpus = pinned_cpus , cluster_id = randint ( 1 , 32768 ), cluster_type = 'AF_Packet::FANOUT_QM' ) ) return zeek_worker_configs","title":"get_optimal_zeek_worker_config()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.NodeConfigManager.reset","text":"Reset a configuration file back to its default Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/zeek/config.py def reset ( self , inspect_interfaces : List [ str ], out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not install . BaseInstallManager . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) if not out_file_path : out_file_path = f ' { self . install_directory } /etc/node.cfg' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /zeek/broctl-nodes.cfg' super ( NodeConfigManager , self ) . reset ( out_file_path , default_config_path ) self . workers = node . Workers () for worker in self . get_optimal_zeek_worker_config ( inspect_interfaces ): self . workers . add_worker ( worker = worker ) self . commit ( out_file_path = out_file_path )","title":"reset()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager","text":"Manage local/site.zeek file (contains scripts, definitions, and signatures to be loaded)","title":"SiteLocalConfigManager"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.__init__","text":"Configure Zeek scripts, signatures, and definitions Parameters: Name Type Description Default configuration_directory str The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.__init__--instance-variables","text":"scripts - A local_site.Scripts instance representing a set of enabled (and disabled) Zeek scripts. signatures A local_site.Signatures instance representing a set of signatures to load. definitions - A local_site.Definitions instance representing a set of script variables redefs . Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure Zeek scripts, signatures, and definitions Args: configuration_directory: The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `scripts` - A `local_site.Scripts` instance representing a set of enabled (and disabled) Zeek scripts. - `signatures` A `local_site.Signatures` instance representing a set of signatures to load. - `definitions` - A `local_site.Definitions` instance representing a set of script variables `redefs`. \"\"\" self . configuration_directory = configuration_directory self . scripts = local_site . Scripts () self . signatures = local_site . Signatures () self . definitions = local_site . Definitions () with open ( f ' { self . configuration_directory } /site/local.zeek' ) as config_f : config_data = dict ( data = config_f . readlines ()) super () . __init__ ( config_data , name = 'zeek.config.local' , verbose = verbose , stdout = stdout ) self . add_parser ( parser = lambda data : local_site . Scripts ( [ local_site . Script ( name = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 5 :], enabled = line . replace ( ' ' , '' ) . strip ()[ 0 ] != '#' ) for line in data [ 'data' ] if self . _line_denotes_script ( line )] ), attribute_name = 'scripts' ) self . add_parser ( parser = lambda data : local_site . Signatures ( [ local_site . Signature ( name = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 10 :], enabled = line . replace ( ' ' , '' ) . strip ()[ 0 ] != '#' ) for line in data [ 'data' ] if self . _line_denotes_signature ( line )] ), attribute_name = 'signatures' ) self . add_parser ( parser = lambda data : local_site . Definitions ( [ local_site . Definition ( name = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 5 :] . split ( '=' )[ 0 ], value = line . replace ( ' ' , '' ) . replace ( '#' , '' ) . strip ()[ 5 :] . split ( '=' )[ 1 ], enabled = line . replace ( ' ' , '' ) . strip ()[ 0 ] != '#' ) for line in data [ 'data' ] if self . _line_denotes_definition ( line )] ), attribute_name = 'definitions' )","title":"Instance Variables:"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.commit","text":"Write the changes out to configuration file Parameters: Name Type Description Default out_file_path Optional[str] The path to the configuration file to write (or overwrite) None backup_directory Optional[str] The path to the backup directory None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def commit ( self , out_file_path : Optional [ str ] = None , backup_directory : Optional [ str ] = None ) -> None : \"\"\"Write the changes out to configuration file Args: out_file_path: The path to the configuration file to write (or overwrite) backup_directory: The path to the backup directory Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /site/local.zeek' self . formatted_data = ' \\n ' . join ( self . signatures . get_raw () + self . scripts . get_raw () + self . definitions . get_raw () ) super ( SiteLocalConfigManager , self ) . commit ( out_file_path , backup_directory )","title":"commit()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.disable_all_definitions","text":"Disable all definitions Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def disable_all_definitions ( self ) -> None : \"\"\"Disable all definitions Returns: None \"\"\" for definition in self . definitions : definition . enabled = False","title":"disable_all_definitions()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.disable_all_scripts","text":"Disable all scripts Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def disable_all_scripts ( self ) -> None : \"\"\"Disable all scripts Returns: None \"\"\" for script in self . scripts : script . enabled = False","title":"disable_all_scripts()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.disable_all_signatures","text":"Disable all scripts Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def disable_all_signatures ( self ) -> None : \"\"\"Disable all scripts Returns: None \"\"\" for sig in self . signatures : sig . enabled = False","title":"disable_all_signatures()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.enable_all_definitions","text":"Enable all definitions Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def enable_all_definitions ( self ) -> None : \"\"\"Enable all definitions Returns: None \"\"\" for definition in self . definitions : definition . enabled = False","title":"enable_all_definitions()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.enable_all_scripts","text":"Enable all scripts Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def enable_all_scripts ( self ) -> None : \"\"\"Enable all scripts Returns: None \"\"\" for script in self . scripts : script . enabled = True","title":"enable_all_scripts()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.enable_all_signatures","text":"Enable all signatures Returns: Type Description None None Source code in dynamite_nsm/services/zeek/config.py def enable_all_signatures ( self ) -> None : \"\"\"Enable all signatures Returns: None \"\"\" for sig in self . signatures : sig . enabled = True","title":"enable_all_signatures()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.from_raw_text","text":"Alternative method for creating configuration file from raw text Parameters: Name Type Description Default raw_text str The string representing the configuration file required configuration_directory Optional[str] The configuration directory for Zeek None Returns: Type Description An instance of ConfigManager Source code in dynamite_nsm/services/zeek/config.py @classmethod def from_raw_text ( cls , raw_text : str , configuration_directory : Optional [ str ] = None ): \"\"\"Alternative method for creating configuration file from raw text Args: raw_text: The string representing the configuration file configuration_directory: The configuration directory for Zeek Returns: An instance of ConfigManager \"\"\" tmp_root = f ' { const . CONFIG_PATH } /.tmp' tmp_dir = f ' { tmp_root } /site' tmp_config = f ' { tmp_dir } /local.zeek' utilities . makedirs ( tmp_dir ) with open ( tmp_config , 'w' ) as out_f : out_f . write ( raw_text ) c = cls ( configuration_directory = tmp_root ) if configuration_directory : c . configuration_directory = configuration_directory return c","title":"from_raw_text()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalConfigManager.reset","text":"Reset a configuration file back to its default Parameters: Name Type Description Default out_file_path Optional[str] The path to the output file None default_config_path Optional[str] The path to the default configuration None Returns: Type Description None Source code in dynamite_nsm/services/zeek/config.py def reset ( self , out_file_path : Optional [ str ] = None , default_config_path : Optional [ str ] = None ): \"\"\"Reset a configuration file back to its default Args: out_file_path: The path to the output file default_config_path: The path to the default configuration Returns: None \"\"\" if not out_file_path : out_file_path = f ' { self . configuration_directory } /site/local.zeek' if not default_config_path : default_config_path = f ' { const . DEFAULT_CONFIGS } /zeek/local.zeek' super ( SiteLocalConfigManager , self ) . reset ( out_file_path , default_config_path ) self . commit ( out_file_path = out_file_path )","title":"reset()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalPackageManager","text":"","title":"SiteLocalPackageManager"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalPackageManager.__init__","text":"Configure Zeek packages installed through ZKG Parameters: Name Type Description Default configuration_directory str The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) required verbose Optional[bool] Include detailed debug messages False stdout Optional[bool] Print output to console True","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.SiteLocalPackageManager.__init__--instance-variables","text":"scripts - A local_site.Scripts instance representing a set of enabled (and disabled) Zeek scripts. Source code in dynamite_nsm/services/zeek/config.py def __init__ ( self , configuration_directory : str , verbose : Optional [ bool ] = False , stdout : Optional [ bool ] = True ): \"\"\" Configure Zeek packages installed through ZKG Args: configuration_directory: The path to the Zeek configuration directory (E.G /etc/dynamite/zeek) verbose: Include detailed debug messages stdout: Print output to console ___ # Instance Variables: - `scripts` - A `local_site.Scripts` instance representing a set of enabled (and disabled) Zeek scripts. \"\"\" self . configuration_directory = configuration_directory self . packages = [] self . _load_packages ()","title":"Instance Variables:"},{"location":"guides/developers/SDK/services/zeek/config/#dynamite_nsm.services.zeek.config.lookup_script_definition","text":"Return the definition, categories, and friendly_name of a given script Parameters: Name Type Description Default script_id str A unique identifier representing a Zeek script. required Returns: Type Description Dict A dictionary of the format {\"friendly_name\" , \"description\" , \"categories\" } Source code in dynamite_nsm/services/zeek/config.py def lookup_script_definition ( script_id : str ) -> Dict : \"\"\"Return the definition, categories, and friendly_name of a given script Args: script_id: A unique identifier representing a Zeek script. Returns: A dictionary of the format {\"friendly_name\" <str>, \"description\" <str>, \"categories\" <list>} \"\"\" try : zeek_script_defs = os . path . join ( const . DEFAULT_CONFIGS , 'zeek' , 'zeek_script_definitions.json' ) with open ( zeek_script_defs ) as f : zeek_defs = json . load ( f ) except FileNotFoundError : zeek_defs = {} definition = zeek_defs . get ( str ( script_id )) return definition","title":"lookup_script_definition()"},{"location":"guides/developers/SDK/services/zeek/install/","text":"Installation Manager for Zeek and its dependencies. To import... from dynamite_nsm.services.zeek import install as zeek_install InstallManager Manage Zeek installation process __init__ ( self , configuration_directory , install_directory , download_zeek_archive = True , skip_interface_validation = False , stdout = False , verbose = False ) special Install Zeek Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/zeek/) required install_directory str Path to the install directory (E.G /opt/dynamite/zeek/) required download_zeek_archive Optional[bool] If True, download the Zeek archive from a mirror True skip_interface_validation Optional[bool] If included we don't check if the interface is available on the system False stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/zeek/install.py def __init__ ( self , configuration_directory : str , install_directory : str , download_zeek_archive : Optional [ bool ] = True , skip_interface_validation : Optional [ bool ] = False , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Zeek Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/zeek/) install_directory: Path to the install directory (E.G /opt/dynamite/zeek/) download_zeek_archive: If True, download the Zeek archive from a mirror skip_interface_validation: If included we don't check if the interface is available on the system stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . skip_interface_validation = skip_interface_validation self . stdout = stdout self . verbose = verbose super ( InstallManager , self ) . __init__ ( name = 'zeek.install' , verbose = verbose , stdout = stdout ) if not shutil . which ( 'python3-config' ): raise InstallError ( 'Python3 development bindings must be installed for Zeek installation to fully succeed. ' 'Common Packages: \"python3-dev\" (Debian based) \"python3-devel\" (RHEL based)' ) if download_zeek_archive : self . logger . info ( \"Attempting to download Zeek archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . ZEEK_MIRRORS ) self . logger . info ( f 'Attempting to extract Zeek archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . ZEEK_MIRRORS ) configure_compile_zeek ( self , parallel_threads = None ) Configure and build Zeek from source Parameters: Name Type Description Default parallel_threads Optional[int] Number of parallel threads to use during the compiling process None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/install.py def configure_compile_zeek ( self , parallel_threads : Optional [ int ] = None ) -> None : \"\"\"Configure and build Zeek from source Args: parallel_threads: Number of parallel threads to use during the compiling process Returns: None \"\"\" zeek_source_install_cache = os . path . join ( const . INSTALL_CACHE , self . local_mirror_root ) configure_args = [ f '--prefix= { self . install_directory } ' , f '--scriptdir= { self . configuration_directory } ' , '--enable-jemalloc' , '--with-python=/usr/bin/python3' ] self . configure_source_package ( zeek_source_install_cache , configure_args = configure_args ) time . sleep ( 1 ) self . compile_source_package ( zeek_source_install_cache , parallel_threads = parallel_threads , expected_lines_printed = COMPILE_PROCESS_EXPECTED_LINE_COUNT ) create_update_zeek_environment_variables ( self ) Creates all the required Zeek environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/zeek/install.py def create_update_zeek_environment_variables ( self ) -> None : \"\"\"Creates all the required Zeek environmental variables Args: Returns: None \"\"\" self . create_update_env_variable ( 'ZEEK_HOME' , self . install_directory ) self . create_update_env_variable ( 'ZEEK_SCRIPTS' , self . configuration_directory ) install_zeek_dependencies ( self ) Install Zeek dependencies (And PowerTools repo if on redhat based distro) Returns: Type Description None None Source code in dynamite_nsm/services/zeek/install.py def install_zeek_dependencies ( self ) -> None : \"\"\"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Args: Returns: None \"\"\" def install_powertools_rhel ( pacman_type ): \"\"\"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Args: Returns: None \"\"\" if pacman_type != 'yum' : self . logger . info ( 'Skipping RHEL PowerTools install, as it is not needed on this distribution.' ) return self . install_dependencies ( yum_packages = [ 'dnf-plugins-core' , 'epel-release' ]) enable_powertools_p = subprocess . Popen ([ 'yum' , 'config-manager' , '--set-enabled' , 'powertools' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) enable_powertools_p . communicate () if enable_powertools_p . returncode == 0 : self . logger . info ( \"Installed PowerTools.\" ) apt_get_packages = \\ [ 'bison' , 'cmake' , 'cmake3' , 'flex' , 'g++' , 'gcc' , 'libjemalloc-dev' , 'libpcap-dev' , 'libssl-dev' , 'linux-headers-$(uname -r)' , 'linux-headers-generic' , 'make' , 'swig' , 'tar' , 'sqlite3' , 'zlib1g-dev' ] yum_packages = \\ [ 'bison' , 'cmake' , 'cmake3' , 'flex' , 'gcc' , 'gcc-c++' , 'jemalloc-devel' , 'kernel-devel' , 'libpcap-devel' , 'make' , 'openssl-devel' , 'swig' , 'tar' , 'sqlite-devel' , 'zlib-devel' ] self . install_dependencies ( apt_get_packages = apt_get_packages , yum_packages = yum_packages , pre_install_function = install_powertools_rhel ) setup ( self , inspect_interfaces ) Setup Zeek Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required Returns: Type Description None Source code in dynamite_nsm/services/zeek/install.py def setup ( self , inspect_interfaces : List [ str ]): \"\"\"Setup Zeek Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) Returns: None \"\"\" if not self . skip_interface_validation : if not self . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) sysctl = systemctl . SystemCtl () self . install_zeek_dependencies () self . create_update_zeek_environment_variables () self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . info ( 'Setting up Zeek from source. This can take up to 15 minutes.' ) if self . stdout : utilities . print_coffee_art () self . configure_compile_zeek () self . logger . info ( 'Setting up Zeek package manager.' ) zkg_installer = zkg_install . InstallManager () zkg_installer . setup () try : package . InstallPackageManager ( const . ZEEK_PACKAGES , stdout = self . stdout , verbose = self . verbose ) . setup () except zkg . InstallZeekPackageError as e : self . logger . error ( f 'An error occurred while installing one or more Zeek packages: { e } ' ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /zeek/broctl-nodes.cfg' , f ' { self . install_directory } /etc/node.cfg' ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /zeek/local.zeek' , f ' { self . configuration_directory } /site/local.zeek' ) # Optimize Configurations node_config = config . NodeConfigManager ( self . install_directory , stdout = self . stdout , verbose = self . verbose ) node_config . workers = node . Workers () for worker in node_config . get_optimal_zeek_worker_config ( inspect_interfaces ): node_config . workers . add_worker ( worker = worker ) self . logger . info ( 'Applying node configuration.' ) node_config . commit () self . logger . info ( 'Setting up BPF input configuration' ) with open ( f ' { self . configuration_directory } /bpf_map_file.input' , 'w' ) as bpf_config_f : bpf_config_f . write ( '' ) # Fix Permissions self . logger . info ( 'Setting up file permissions.' ) utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . configuration_directory } /site/local.zeek' , 660 ) utilities . set_permissions_of_file ( f ' { self . configuration_directory } /site/bpf_map_file.input' , 660 ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . install_directory } /etc/node.cfg' , 660 ) utilities . set_permissions_of_file ( f ' { self . install_directory } /etc/networks.cfg' , 660 ) utilities . set_permissions_of_file ( f ' { self . install_directory } /etc/networks.cfg' , 660 ) self . logger . info ( 'Setting up Zeek capture rules for dynamite user.' ) set_caps . SetCapturePermissions ( self . install_directory ) . invoke ( shell = True ) self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/zeek.service' ) sysctl . install_and_enable ( os . path . join ( const . DEFAULT_CONFIGS , 'systemd' , 'zeek.service' )) UninstallManager Manage Zeek uninstallation process __init__ ( self , purge_config = True , stdout = False , verbose = False ) special Uninstall Zeek Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/zeek/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Zeek Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages \"\"\" from dynamite_nsm.services.zeek.process import ProcessManager env_vars = utilities . get_environment_file_dict () zeek_directories = [ env_vars . get ( 'ZEEK_HOME' )] if purge_config : zeek_directories . append ( env_vars . get ( 'ZEEK_SCRIPTS' )) super () . __init__ ( 'zeek.uninstall' , directories = zeek_directories , sysctl_service_name = 'zeek.service' , environ_vars = [ 'ZEEK_HOME' , 'ZEEK_SCRIPTS' ], process = ProcessManager ( stdout = stdout , verbose = verbose ), stdout = stdout , verbose = verbose )","title":"install"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.InstallManager","text":"Manage Zeek installation process","title":"InstallManager"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.InstallManager.__init__","text":"Install Zeek Parameters: Name Type Description Default configuration_directory str Path to the configuration directory (E.G /etc/dynamite/zeek/) required install_directory str Path to the install directory (E.G /opt/dynamite/zeek/) required download_zeek_archive Optional[bool] If True, download the Zeek archive from a mirror True skip_interface_validation Optional[bool] If included we don't check if the interface is available on the system False stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/zeek/install.py def __init__ ( self , configuration_directory : str , install_directory : str , download_zeek_archive : Optional [ bool ] = True , skip_interface_validation : Optional [ bool ] = False , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Install Zeek Args: configuration_directory: Path to the configuration directory (E.G /etc/dynamite/zeek/) install_directory: Path to the install directory (E.G /opt/dynamite/zeek/) download_zeek_archive: If True, download the Zeek archive from a mirror skip_interface_validation: If included we don't check if the interface is available on the system stdout: Print output to console verbose: Include detailed debug messages \"\"\" self . configuration_directory = configuration_directory self . install_directory = install_directory self . skip_interface_validation = skip_interface_validation self . stdout = stdout self . verbose = verbose super ( InstallManager , self ) . __init__ ( name = 'zeek.install' , verbose = verbose , stdout = stdout ) if not shutil . which ( 'python3-config' ): raise InstallError ( 'Python3 development bindings must be installed for Zeek installation to fully succeed. ' 'Common Packages: \"python3-dev\" (Debian based) \"python3-devel\" (RHEL based)' ) if download_zeek_archive : self . logger . info ( \"Attempting to download Zeek archive.\" ) _ , archive_name , self . local_mirror_root = self . download_from_mirror ( const . ZEEK_MIRRORS ) self . logger . info ( f 'Attempting to extract Zeek archive ( { archive_name } ).' ) self . extract_archive ( os . path . join ( const . INSTALL_CACHE , archive_name )) self . logger . info ( \"Extraction completed.\" ) else : _ , _ , self . local_mirror_root = self . get_mirror_info ( const . ZEEK_MIRRORS )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.InstallManager.configure_compile_zeek","text":"Configure and build Zeek from source Parameters: Name Type Description Default parallel_threads Optional[int] Number of parallel threads to use during the compiling process None Returns: Type Description None None Source code in dynamite_nsm/services/zeek/install.py def configure_compile_zeek ( self , parallel_threads : Optional [ int ] = None ) -> None : \"\"\"Configure and build Zeek from source Args: parallel_threads: Number of parallel threads to use during the compiling process Returns: None \"\"\" zeek_source_install_cache = os . path . join ( const . INSTALL_CACHE , self . local_mirror_root ) configure_args = [ f '--prefix= { self . install_directory } ' , f '--scriptdir= { self . configuration_directory } ' , '--enable-jemalloc' , '--with-python=/usr/bin/python3' ] self . configure_source_package ( zeek_source_install_cache , configure_args = configure_args ) time . sleep ( 1 ) self . compile_source_package ( zeek_source_install_cache , parallel_threads = parallel_threads , expected_lines_printed = COMPILE_PROCESS_EXPECTED_LINE_COUNT )","title":"configure_compile_zeek()"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.InstallManager.create_update_zeek_environment_variables","text":"Creates all the required Zeek environmental variables Returns: Type Description None None Source code in dynamite_nsm/services/zeek/install.py def create_update_zeek_environment_variables ( self ) -> None : \"\"\"Creates all the required Zeek environmental variables Args: Returns: None \"\"\" self . create_update_env_variable ( 'ZEEK_HOME' , self . install_directory ) self . create_update_env_variable ( 'ZEEK_SCRIPTS' , self . configuration_directory )","title":"create_update_zeek_environment_variables()"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.InstallManager.install_zeek_dependencies","text":"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Returns: Type Description None None Source code in dynamite_nsm/services/zeek/install.py def install_zeek_dependencies ( self ) -> None : \"\"\"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Args: Returns: None \"\"\" def install_powertools_rhel ( pacman_type ): \"\"\"Install Zeek dependencies (And PowerTools repo if on redhat based distro) Args: Returns: None \"\"\" if pacman_type != 'yum' : self . logger . info ( 'Skipping RHEL PowerTools install, as it is not needed on this distribution.' ) return self . install_dependencies ( yum_packages = [ 'dnf-plugins-core' , 'epel-release' ]) enable_powertools_p = subprocess . Popen ([ 'yum' , 'config-manager' , '--set-enabled' , 'powertools' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) enable_powertools_p . communicate () if enable_powertools_p . returncode == 0 : self . logger . info ( \"Installed PowerTools.\" ) apt_get_packages = \\ [ 'bison' , 'cmake' , 'cmake3' , 'flex' , 'g++' , 'gcc' , 'libjemalloc-dev' , 'libpcap-dev' , 'libssl-dev' , 'linux-headers-$(uname -r)' , 'linux-headers-generic' , 'make' , 'swig' , 'tar' , 'sqlite3' , 'zlib1g-dev' ] yum_packages = \\ [ 'bison' , 'cmake' , 'cmake3' , 'flex' , 'gcc' , 'gcc-c++' , 'jemalloc-devel' , 'kernel-devel' , 'libpcap-devel' , 'make' , 'openssl-devel' , 'swig' , 'tar' , 'sqlite-devel' , 'zlib-devel' ] self . install_dependencies ( apt_get_packages = apt_get_packages , yum_packages = yum_packages , pre_install_function = install_powertools_rhel )","title":"install_zeek_dependencies()"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.InstallManager.setup","text":"Setup Zeek Parameters: Name Type Description Default inspect_interfaces List[str] A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) required Returns: Type Description None Source code in dynamite_nsm/services/zeek/install.py def setup ( self , inspect_interfaces : List [ str ]): \"\"\"Setup Zeek Args: inspect_interfaces: A list of network interfaces to capture on (E.G [\"mon0\", \"mon1\"]) Returns: None \"\"\" if not self . skip_interface_validation : if not self . validate_inspect_interfaces ( inspect_interfaces ): raise install . NetworkInterfaceNotFound ( inspect_interfaces ) sysctl = systemctl . SystemCtl () self . install_zeek_dependencies () self . create_update_zeek_environment_variables () self . logger . debug ( f 'Creating directory: { self . configuration_directory } ' ) utilities . makedirs ( self . configuration_directory ) self . logger . debug ( f 'Creating directory: { self . install_directory } ' ) utilities . makedirs ( self . install_directory ) self . logger . info ( 'Setting up Zeek from source. This can take up to 15 minutes.' ) if self . stdout : utilities . print_coffee_art () self . configure_compile_zeek () self . logger . info ( 'Setting up Zeek package manager.' ) zkg_installer = zkg_install . InstallManager () zkg_installer . setup () try : package . InstallPackageManager ( const . ZEEK_PACKAGES , stdout = self . stdout , verbose = self . verbose ) . setup () except zkg . InstallZeekPackageError as e : self . logger . error ( f 'An error occurred while installing one or more Zeek packages: { e } ' ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /zeek/broctl-nodes.cfg' , f ' { self . install_directory } /etc/node.cfg' ) self . copy_file_or_directory_to_destination ( f ' { const . DEFAULT_CONFIGS } /zeek/local.zeek' , f ' { self . configuration_directory } /site/local.zeek' ) # Optimize Configurations node_config = config . NodeConfigManager ( self . install_directory , stdout = self . stdout , verbose = self . verbose ) node_config . workers = node . Workers () for worker in node_config . get_optimal_zeek_worker_config ( inspect_interfaces ): node_config . workers . add_worker ( worker = worker ) self . logger . info ( 'Applying node configuration.' ) node_config . commit () self . logger . info ( 'Setting up BPF input configuration' ) with open ( f ' { self . configuration_directory } /bpf_map_file.input' , 'w' ) as bpf_config_f : bpf_config_f . write ( '' ) # Fix Permissions self . logger . info ( 'Setting up file permissions.' ) utilities . set_ownership_of_file ( self . configuration_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . configuration_directory } /site/local.zeek' , 660 ) utilities . set_permissions_of_file ( f ' { self . configuration_directory } /site/bpf_map_file.input' , 660 ) utilities . set_ownership_of_file ( self . install_directory , user = 'dynamite' , group = 'dynamite' ) utilities . set_permissions_of_file ( f ' { self . install_directory } /etc/node.cfg' , 660 ) utilities . set_permissions_of_file ( f ' { self . install_directory } /etc/networks.cfg' , 660 ) utilities . set_permissions_of_file ( f ' { self . install_directory } /etc/networks.cfg' , 660 ) self . logger . info ( 'Setting up Zeek capture rules for dynamite user.' ) set_caps . SetCapturePermissions ( self . install_directory ) . invoke ( shell = True ) self . logger . info ( f 'Installing service -> { const . DEFAULT_CONFIGS } /systemd/zeek.service' ) sysctl . install_and_enable ( os . path . join ( const . DEFAULT_CONFIGS , 'systemd' , 'zeek.service' ))","title":"setup()"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.UninstallManager","text":"Manage Zeek uninstallation process","title":"UninstallManager"},{"location":"guides/developers/SDK/services/zeek/install/#dynamite_nsm.services.zeek.install.UninstallManager.__init__","text":"Uninstall Zeek Parameters: Name Type Description Default purge_config Optional[bool] If enabled, remove all the configuration files associated with this installation True stdout Optional[bool] Print output to console False verbose Optional[bool] Include detailed debug messages False Source code in dynamite_nsm/services/zeek/install.py def __init__ ( self , purge_config : Optional [ bool ] = True , stdout : Optional [ bool ] = False , verbose : Optional [ bool ] = False ): \"\"\"Uninstall Zeek Args: purge_config: If enabled, remove all the configuration files associated with this installation stdout: Print output to console verbose: Include detailed debug messages \"\"\" from dynamite_nsm.services.zeek.process import ProcessManager env_vars = utilities . get_environment_file_dict () zeek_directories = [ env_vars . get ( 'ZEEK_HOME' )] if purge_config : zeek_directories . append ( env_vars . get ( 'ZEEK_SCRIPTS' )) super () . __init__ ( 'zeek.uninstall' , directories = zeek_directories , sysctl_service_name = 'zeek.service' , environ_vars = [ 'ZEEK_HOME' , 'ZEEK_SCRIPTS' ], process = ProcessManager ( stdout = stdout , verbose = verbose ), stdout = stdout , verbose = verbose )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/","text":"Work with a variety of Zeek logs useful for troubleshooting and performance analysis. Currently, supports: broker.log cluster.log reporter.log stats.log To import... from dynamite_nsm.services.zeek import logs as zeek_logs BrokerEntry A single line item entry for Zeek's broker.log __init__ ( self , entry_raw ) special A single line item entry in the broker.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the broker.log required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the broker.log Args: entry_raw: A JSON serializable string representing a single line item entry in the broker.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . category = None self . event = None self . peer_address = None self . peer_port = None self . message = None self . _parse_entry () BrokerLog Provides an interface for working with Zeek's broker.log __init__ ( self , log_sample_size = 500 , include_archived_logs = False ) special Work with Zeek's broker.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of log entries to load into memory 500 include_archived_logs Optional[bool] If True, include gzipped archive logs False Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\" Work with Zeek's broker.log Args: log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'broker.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'broker.log' , log_sample_size = log_sample_size ) . entries iter_entries ( self , start = None , end = None ) Iterate through BrokerEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[BrokerEntry] yields a BrokerEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ BrokerEntry ]: \"\"\"Iterate through BrokerEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time Returns: yields a BrokerEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( days = 365 ) for en in self . entries : en = BrokerEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): yield log_entry tail ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Category' , 'Peer' , 'Message' ], [ entry . time , entry . category , f ' { entry . peer_address } : { entry . peer_port } ' , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) ClusterEntry A single line item entry for Zeek's cluster.log __init__ ( self , entry_raw ) special A single line item entry in the cluster.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the cluster.log required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the cluster.log Args: entry_raw: A JSON serializable string representing a single line item entry in the cluster.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . message = None self . _parse_entry () ClusterLog Provides an interface for working with Zeek's cluster.log __init__ ( self , log_sample_size = 500 , include_archived_logs = False ) special Work with Zeek's cluster.log log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\"Work with Zeek's cluster.log log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'cluster.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'cluster.log' , log_sample_size = log_sample_size ) . entries iter_entries ( self , start = None , end = None ) Iterate through ClusterEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[ClusterEntry] yields a ClusterEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ ClusterEntry ]: \"\"\"Iterate through ClusterEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time Returns: yields a ClusterEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( days = 365 ) for en in self . entries : en = ClusterEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): yield log_entry tail ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Node' , 'Message' ], [ entry . time , entry . node , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) InvalidZeekBrokerLogEntry Thrown when a Zeek broker.log entry is improperly formatted __init__ ( self , message ) special Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek broker log entry is invalid: { message } ' super ( InvalidZeekBrokerLogEntry , self ) . __init__ ( msg ) InvalidZeekClusterLogEntry Thrown when a Zeek cluster.log entry is improperly formatted __init__ ( self , message ) special Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek cluster log entry is invalid: { message } ' super ( InvalidZeekClusterLogEntry , self ) . __init__ ( msg ) InvalidZeekReporterLogEntry Thrown when a Zeek reporter.log entry is improperly formatted __init__ ( self , message ) special Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek reporter log entry is invalid: { message } ' super ( InvalidZeekReporterLogEntry , self ) . __init__ ( msg ) InvalidZeekStatusLogEntry Thrown when a Zeek stats.log entry is improperly formatted __init__ ( self , message ) special Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek status log entry is invalid: { message } ' super ( InvalidZeekStatusLogEntry , self ) . __init__ ( msg ) MetricsEntry A single Filebeat metrics entry for a specific time-interval __init__ ( self , entry ) special A metrics entry derived from the stats.log Parameters: Name Type Description Default entry Dict A dictionary containing a variety of analyzable fields within a line item metrics entry required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry : Dict ): \"\"\" A metrics entry derived from the stats.log Args: entry: A dictionary containing a variety of analyzable fields within a line item metrics entry \"\"\" self . entry_raw = entry self . timestamp = entry . get ( 'ts' ) self . time = parse_zeek_datetime ( self . timestamp ) self . peer = entry . get ( 'peer' ) self . peers = [ self . peer ] self . memory = entry . get ( 'mem' , 0 ) self . packets_processed = entry . get ( 'pkts_proc' , 0 ) self . bytes_received = entry . get ( 'bytes_recv' , 0 ) self . packets_dropped = entry . get ( 'pkts_dropped' , 0 ) self . packets_link = entry . get ( 'pkts_link' , 0 ) self . packet_lag = entry . get ( 'pkt_lag' , 0 ) self . events_processed = entry . get ( 'events_proc' , 0 ) self . events_queued = entry . get ( 'events_queued' , 0 ) self . active_tcp_connections = entry . get ( 'active_tcp_conns' , 0 ) self . active_udp_connections = entry . get ( 'active_udp_conns' , 0 ) self . active_icmp_connections = entry . get ( 'active_icmp_conns' , 0 ) self . tcp_connections = entry . get ( 'tcp_conns' , 0 ) self . udp_connections = entry . get ( 'udp_conns' , 0 ) self . icmp_connections = entry . get ( 'icmp_conns' , 0 ) self . timers = entry . get ( 'timers' , 0 ) self . files = entry . get ( 'files' , 0 ) self . active_files = entry . get ( 'active_files' , 0 ) self . dns_requests = entry . get ( 'dns_requests' , 0 ) self . active_dns_requests = entry . get ( 'active_dns_requests' , 0 ) self . reassembly_tcp_size = entry . get ( 'reassem_tcp_size' , 0 ) self . reassembly_file_size = entry . get ( 'reassem_file_size' , 0 ) self . reassembly_fragment_size = entry . get ( 'reassem_frag_size' , 0 ) self . reassembly_unknown_size = entry . get ( 'reassem_unknown_size' , 0 ) self . packets_dropped_percentage = 0 if self . packets_link > 0 : self . packets_dropped_percentage = round ( self . packets_dropped / self . packets_link , 2 ) merge_metric_entry ( self , metric_entry ) Merge another metrics entry into this one Parameters: Name Type Description Default metric_entry MetricsEntry The MetricsEntry you wish to merge in required Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def merge_metric_entry ( self , metric_entry : MetricsEntry ) -> None : \"\"\"Merge another metrics entry into this one Args: metric_entry: The MetricsEntry you wish to merge in Returns: None \"\"\" self . peer = None self . peers . append ( metric_entry . peer ) self . memory = self . memory + metric_entry . memory self . packets_processed = self . packets_processed + metric_entry . packets_processed self . bytes_received = self . bytes_received + metric_entry . bytes_received self . packets_dropped = self . packets_dropped + metric_entry . packets_dropped self . packets_link = self . packets_link + metric_entry . packets_link self . packet_lag = self . packet_lag + metric_entry . packet_lag self . events_processed = self . events_processed + metric_entry . events_processed self . events_queued = self . events_queued + metric_entry . events_queued self . active_tcp_connections = self . active_tcp_connections + metric_entry . active_tcp_connections self . active_udp_connections = self . active_udp_connections + metric_entry . active_udp_connections self . active_icmp_connections = self . active_icmp_connections + metric_entry . active_icmp_connections self . tcp_connections = self . tcp_connections + metric_entry . tcp_connections self . udp_connections = self . udp_connections + metric_entry . udp_connections self . icmp_connections = self . icmp_connections + metric_entry . icmp_connections self . timers = self . timers + metric_entry . timers self . files = self . files + metric_entry . files self . active_files = self . active_files + metric_entry . active_files self . dns_requests = self . dns_requests + metric_entry . dns_requests self . active_dns_requests = self . active_dns_requests + metric_entry . active_dns_requests self . reassembly_tcp_size = self . reassembly_tcp_size + metric_entry . reassembly_tcp_size self . reassembly_file_size = self . reassembly_file_size + metric_entry . reassembly_file_size self . reassembly_fragment_size = self . reassembly_fragment_size + metric_entry . reassembly_fragment_size self . reassembly_unknown_size = self . reassembly_unknown_size + metric_entry . reassembly_unknown_size if self . packets_processed > 0 : self . packets_dropped_percentage = round ( self . packets_dropped / self . packets_link , 6 ) ReporterEntry A single line item entry for Zeek's reporter.log __init__ ( self , entry_raw ) special A single line item entry in the reporter.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the reporter.log required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the reporter.log Args: entry_raw: A JSON serializable string representing a single line item entry in the reporter.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . log_level = None self . message = None self . location = None self . _parse_entry () ReporterLog Provides an interface for working with Zeek's reporter.log __init__ ( self , log_sample_size = 500 , include_archived_logs = False ) special Work with Zeek's reporter.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of log entries to load into memory 500 include_archived_logs Optional[bool] If True, include gzipped archive logs content False Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\"Work with Zeek's reporter.log Args: log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs content \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'reporter.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'reporter.log' , log_sample_size = log_sample_size ) . entries iter_entries ( self , start = None , end = None ) Iterate through ReporterEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[ReporterEntry] yields a ReporterEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ ReporterEntry ]: \"\"\"Iterate through ReporterEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time Returns: yields a ReporterEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( days = 365 ) for en in self . entries : en = ReporterEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): yield log_entry tail ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Log Level' , 'Location' , 'Message' ], [ entry . time , entry . log_level , entry . location , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) StatusLog Provides an interface for working with Zeek's stats.log __init__ ( self , log_sample_size = 500 , include_archived_logs = False ) special Work with Zeek's stats.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of log entries to load into memory 500 include_archived_logs Optional[bool] If True, include gzipped archive logs content False Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\"Work with Zeek's stats.log Args: log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs content \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'stats.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'stats.log' , log_sample_size = log_sample_size ) . entries iter_aggregated_metrics ( self , start = None , end = None , tolerance_seconds = 60 ) Aggregate peers and combine events within tolerance_seconds into the same entry. Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None tolerance_seconds Optional[int] Specifies the maximum time distance between entries to combine them 60 Returns: Type Description Generator[MetricsEntry] yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_aggregated_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , tolerance_seconds : Optional [ int ] = 60 ) -> Generator [ MetricsEntry ]: \"\"\"Aggregate peers and combine events within tolerance_seconds into the same entry. Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum time distance between entries to combine them Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . timestamp ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry iter_metrics ( self , start = None , end = None ) Iterate through metrics entries individually. Metrics are given for each individual Zeek peer. Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[MetricsEntry] yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ MetricsEntry ]: \"\"\"Iterate through metrics entries individually. Metrics are given for each individual Zeek peer. Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" def filter_metrics ( s : Optional [ datetime ] = None , e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) for en in self . entries : en = MetricsEntry ( json . loads ( en )) if s < en . time < e : yield en for log_entry in filter_metrics ( start , end ): yield log_entry tail ( self , pretty_print = True ) Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for metric in self . iter_aggregated_metrics ( start = start , end = end ): if metric . timestamp not in visited : visited . append ( metric . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( metric )), indent = 1 )) else : status_table = [ [ 'Time' , 'Memory' , 'Timers' , 'Packets on Link' , 'Packets Processed' , 'Packets Dropped' , 'Peers' ], [ metric . time , metric . memory , metric . timers , metric . packets_link , metric . packets_processed , metric . packets_dropped , ' \\n ' . join ( metric . peers )] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' )) ZeekLogsProxy A convenience class providing accessibility to Zeek's archived logs; supports gzipped content __init__ ( self , log_name , log_sample_size = 1000 ) special Access a log and all of its corresponding archived logs until log_sample_size is reached. Parameters: Name Type Description Default log_name str The name of the Zeek log to retrieve required log_sample_size Optional[int] The max number of log entries to retrieve 1000 Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_name : str , log_sample_size : Optional [ int ] = 1000 ): \"\"\"Access a log and all of its corresponding archived logs until log_sample_size is reached. Args: log_name: The name of the Zeek log to retrieve log_sample_size: The max number of log entries to retrieve \"\"\" self . entries = [] self . log_name = log_name self . log_sample_size = log_sample_size self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . current_log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , log_name ) self . log_archive_directory = os . path . join ( self . zeek_home , 'logs' ) self . load_all_logs () load_all_logs ( self ) Load all logs into memory up to self.log_sample_size Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def load_all_logs ( self ) -> None : \"\"\" Load all logs into memory up to self.log_sample_size Returns: None \"\"\" archive_directories = [] sorted_log_paths = [] for log_archive_directory in os . listdir ( self . log_archive_directory ): try : archive_directories . append ( ( log_archive_directory , datetime . strptime ( log_archive_directory , '%Y-%m- %d ' ))) except ValueError : pass sorted_archive_directories = sorted ( archive_directories , key = lambda x : x [ 1 ]) for archive_dir_name , _ in sorted_archive_directories : relevant_log_names = [ fname for fname in os . listdir ( os . path . join ( self . log_archive_directory , archive_dir_name )) if fname . startswith ( self . log_name . replace ( '.log' , '' )) and fname . endswith ( '.gz' ) ] for log_archive_file_name in relevant_log_names : log_rotate_time = log_archive_file_name . split ( '.' )[ 1 ] . split ( '-' )[ 0 ] sorted_log_paths . append ( ( os . path . join ( self . log_archive_directory , archive_dir_name , log_archive_file_name ), datetime . strptime ( archive_dir_name + ' ' + log_rotate_time , '%Y-%m- %d %H:%M:%S' )) ) sorted_log_paths = sorted ( sorted_log_paths , key = lambda x : x [ 1 ], reverse = True ) current_log_file = logs . LogFile ( log_path = self . current_log_path , log_sample_size = self . log_sample_size , gzip_decode = False ) self . entries . extend ( current_log_file . entries ) for log_path , log_rotate_date in sorted_log_paths : archived_log_file = logs . LogFile ( log_path , log_sample_size = self . log_sample_size , gzip_decode = True ) remaining_entries_available = self . log_sample_size - len ( self . entries ) if remaining_entries_available > 0 : self . entries . extend ( archived_log_file . entries [ 0 : remaining_entries_available ]) else : break","title":"logs"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.BrokerEntry","text":"A single line item entry for Zeek's broker.log","title":"BrokerEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.BrokerEntry.__init__","text":"A single line item entry in the broker.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the broker.log required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the broker.log Args: entry_raw: A JSON serializable string representing a single line item entry in the broker.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . category = None self . event = None self . peer_address = None self . peer_port = None self . message = None self . _parse_entry ()","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.BrokerLog","text":"Provides an interface for working with Zeek's broker.log","title":"BrokerLog"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.BrokerLog.__init__","text":"Work with Zeek's broker.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of log entries to load into memory 500 include_archived_logs Optional[bool] If True, include gzipped archive logs False Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\" Work with Zeek's broker.log Args: log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'broker.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'broker.log' , log_sample_size = log_sample_size ) . entries","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.BrokerLog.iter_entries","text":"Iterate through BrokerEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[BrokerEntry] yields a BrokerEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ BrokerEntry ]: \"\"\"Iterate through BrokerEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time Returns: yields a BrokerEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( days = 365 ) for en in self . entries : en = BrokerEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): yield log_entry","title":"iter_entries()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.BrokerLog.tail","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Category' , 'Peer' , 'Message' ], [ entry . time , entry . category , f ' { entry . peer_address } : { entry . peer_port } ' , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ClusterEntry","text":"A single line item entry for Zeek's cluster.log","title":"ClusterEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ClusterEntry.__init__","text":"A single line item entry in the cluster.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the cluster.log required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the cluster.log Args: entry_raw: A JSON serializable string representing a single line item entry in the cluster.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . message = None self . _parse_entry ()","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ClusterLog","text":"Provides an interface for working with Zeek's cluster.log","title":"ClusterLog"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ClusterLog.__init__","text":"Work with Zeek's cluster.log log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\"Work with Zeek's cluster.log log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'cluster.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'cluster.log' , log_sample_size = log_sample_size ) . entries","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ClusterLog.iter_entries","text":"Iterate through ClusterEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[ClusterEntry] yields a ClusterEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ ClusterEntry ]: \"\"\"Iterate through ClusterEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time Returns: yields a ClusterEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( days = 365 ) for en in self . entries : en = ClusterEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): yield log_entry","title":"iter_entries()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ClusterLog.tail","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Node' , 'Message' ], [ entry . time , entry . node , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekBrokerLogEntry","text":"Thrown when a Zeek broker.log entry is improperly formatted","title":"InvalidZeekBrokerLogEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekBrokerLogEntry.__init__","text":"Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek broker log entry is invalid: { message } ' super ( InvalidZeekBrokerLogEntry , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekClusterLogEntry","text":"Thrown when a Zeek cluster.log entry is improperly formatted","title":"InvalidZeekClusterLogEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekClusterLogEntry.__init__","text":"Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek cluster log entry is invalid: { message } ' super ( InvalidZeekClusterLogEntry , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekReporterLogEntry","text":"Thrown when a Zeek reporter.log entry is improperly formatted","title":"InvalidZeekReporterLogEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekReporterLogEntry.__init__","text":"Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek reporter log entry is invalid: { message } ' super ( InvalidZeekReporterLogEntry , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekStatusLogEntry","text":"Thrown when a Zeek stats.log entry is improperly formatted","title":"InvalidZeekStatusLogEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.InvalidZeekStatusLogEntry.__init__","text":"Parameters: Name Type Description Default message A more specific error message required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , message ): \"\"\" Args: message: A more specific error message \"\"\" msg = f 'Zeek status log entry is invalid: { message } ' super ( InvalidZeekStatusLogEntry , self ) . __init__ ( msg )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.MetricsEntry","text":"A single Filebeat metrics entry for a specific time-interval","title":"MetricsEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.MetricsEntry.__init__","text":"A metrics entry derived from the stats.log Parameters: Name Type Description Default entry Dict A dictionary containing a variety of analyzable fields within a line item metrics entry required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry : Dict ): \"\"\" A metrics entry derived from the stats.log Args: entry: A dictionary containing a variety of analyzable fields within a line item metrics entry \"\"\" self . entry_raw = entry self . timestamp = entry . get ( 'ts' ) self . time = parse_zeek_datetime ( self . timestamp ) self . peer = entry . get ( 'peer' ) self . peers = [ self . peer ] self . memory = entry . get ( 'mem' , 0 ) self . packets_processed = entry . get ( 'pkts_proc' , 0 ) self . bytes_received = entry . get ( 'bytes_recv' , 0 ) self . packets_dropped = entry . get ( 'pkts_dropped' , 0 ) self . packets_link = entry . get ( 'pkts_link' , 0 ) self . packet_lag = entry . get ( 'pkt_lag' , 0 ) self . events_processed = entry . get ( 'events_proc' , 0 ) self . events_queued = entry . get ( 'events_queued' , 0 ) self . active_tcp_connections = entry . get ( 'active_tcp_conns' , 0 ) self . active_udp_connections = entry . get ( 'active_udp_conns' , 0 ) self . active_icmp_connections = entry . get ( 'active_icmp_conns' , 0 ) self . tcp_connections = entry . get ( 'tcp_conns' , 0 ) self . udp_connections = entry . get ( 'udp_conns' , 0 ) self . icmp_connections = entry . get ( 'icmp_conns' , 0 ) self . timers = entry . get ( 'timers' , 0 ) self . files = entry . get ( 'files' , 0 ) self . active_files = entry . get ( 'active_files' , 0 ) self . dns_requests = entry . get ( 'dns_requests' , 0 ) self . active_dns_requests = entry . get ( 'active_dns_requests' , 0 ) self . reassembly_tcp_size = entry . get ( 'reassem_tcp_size' , 0 ) self . reassembly_file_size = entry . get ( 'reassem_file_size' , 0 ) self . reassembly_fragment_size = entry . get ( 'reassem_frag_size' , 0 ) self . reassembly_unknown_size = entry . get ( 'reassem_unknown_size' , 0 ) self . packets_dropped_percentage = 0 if self . packets_link > 0 : self . packets_dropped_percentage = round ( self . packets_dropped / self . packets_link , 2 )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.MetricsEntry.merge_metric_entry","text":"Merge another metrics entry into this one Parameters: Name Type Description Default metric_entry MetricsEntry The MetricsEntry you wish to merge in required Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def merge_metric_entry ( self , metric_entry : MetricsEntry ) -> None : \"\"\"Merge another metrics entry into this one Args: metric_entry: The MetricsEntry you wish to merge in Returns: None \"\"\" self . peer = None self . peers . append ( metric_entry . peer ) self . memory = self . memory + metric_entry . memory self . packets_processed = self . packets_processed + metric_entry . packets_processed self . bytes_received = self . bytes_received + metric_entry . bytes_received self . packets_dropped = self . packets_dropped + metric_entry . packets_dropped self . packets_link = self . packets_link + metric_entry . packets_link self . packet_lag = self . packet_lag + metric_entry . packet_lag self . events_processed = self . events_processed + metric_entry . events_processed self . events_queued = self . events_queued + metric_entry . events_queued self . active_tcp_connections = self . active_tcp_connections + metric_entry . active_tcp_connections self . active_udp_connections = self . active_udp_connections + metric_entry . active_udp_connections self . active_icmp_connections = self . active_icmp_connections + metric_entry . active_icmp_connections self . tcp_connections = self . tcp_connections + metric_entry . tcp_connections self . udp_connections = self . udp_connections + metric_entry . udp_connections self . icmp_connections = self . icmp_connections + metric_entry . icmp_connections self . timers = self . timers + metric_entry . timers self . files = self . files + metric_entry . files self . active_files = self . active_files + metric_entry . active_files self . dns_requests = self . dns_requests + metric_entry . dns_requests self . active_dns_requests = self . active_dns_requests + metric_entry . active_dns_requests self . reassembly_tcp_size = self . reassembly_tcp_size + metric_entry . reassembly_tcp_size self . reassembly_file_size = self . reassembly_file_size + metric_entry . reassembly_file_size self . reassembly_fragment_size = self . reassembly_fragment_size + metric_entry . reassembly_fragment_size self . reassembly_unknown_size = self . reassembly_unknown_size + metric_entry . reassembly_unknown_size if self . packets_processed > 0 : self . packets_dropped_percentage = round ( self . packets_dropped / self . packets_link , 6 )","title":"merge_metric_entry()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ReporterEntry","text":"A single line item entry for Zeek's reporter.log","title":"ReporterEntry"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ReporterEntry.__init__","text":"A single line item entry in the reporter.log Parameters: Name Type Description Default entry_raw str A JSON serializable string representing a single line item entry in the reporter.log required Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , entry_raw : str ): \"\"\" A single line item entry in the reporter.log Args: entry_raw: A JSON serializable string representing a single line item entry in the reporter.log \"\"\" self . entry_raw = entry_raw self . time = None self . timestamp = None self . log_level = None self . message = None self . location = None self . _parse_entry ()","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ReporterLog","text":"Provides an interface for working with Zeek's reporter.log","title":"ReporterLog"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ReporterLog.__init__","text":"Work with Zeek's reporter.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of log entries to load into memory 500 include_archived_logs Optional[bool] If True, include gzipped archive logs content False Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\"Work with Zeek's reporter.log Args: log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs content \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'reporter.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'reporter.log' , log_sample_size = log_sample_size ) . entries","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ReporterLog.iter_entries","text":"Iterate through ReporterEntries while providing some basic filtering options Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[ReporterEntry] yields a ReporterEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_entries ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ ReporterEntry ]: \"\"\"Iterate through ReporterEntries while providing some basic filtering options Args: start: UTC start time end: UTC end time Returns: yields a ReporterEntry for every iteration \"\"\" def filter_entries ( s : Optional [ datetime ], e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( days = 365 ) for en in self . entries : en = ReporterEntry ( en ) if s < en . time < e : yield en for log_entry in filter_entries ( start , end ): yield log_entry","title":"iter_entries()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ReporterLog.tail","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for entry in self . iter_entries ( start = start , end = end ): if entry . timestamp not in visited : visited . append ( entry . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( entry )), indent = 1 )) else : status_table = [ [ 'Time' , 'Log Level' , 'Location' , 'Message' ], [ entry . time , entry . log_level , entry . location , entry . message ] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.StatusLog","text":"Provides an interface for working with Zeek's stats.log","title":"StatusLog"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.StatusLog.__init__","text":"Work with Zeek's stats.log Parameters: Name Type Description Default log_sample_size Optional[int] The maximum number of log entries to load into memory 500 include_archived_logs Optional[bool] If True, include gzipped archive logs content False Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_sample_size : Optional [ int ] = 500 , include_archived_logs : Optional [ bool ] = False ): \"\"\"Work with Zeek's stats.log Args: log_sample_size: The maximum number of log entries to load into memory include_archived_logs: If True, include gzipped archive logs content \"\"\" self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , 'stats.log' ) logs . LogFile . __init__ ( self , log_path = self . log_path , log_sample_size = log_sample_size ) if include_archived_logs : self . entries = ZeekLogsProxy ( 'stats.log' , log_sample_size = log_sample_size ) . entries","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.StatusLog.iter_aggregated_metrics","text":"Aggregate peers and combine events within tolerance_seconds into the same entry. Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None tolerance_seconds Optional[int] Specifies the maximum time distance between entries to combine them 60 Returns: Type Description Generator[MetricsEntry] yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_aggregated_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , tolerance_seconds : Optional [ int ] = 60 ) -> Generator [ MetricsEntry ]: \"\"\"Aggregate peers and combine events within tolerance_seconds into the same entry. Args: start: UTC start time end: UTC end time tolerance_seconds: Specifies the maximum time distance between entries to combine them Returns: yields a MetricsEntry for every iteration \"\"\" sorted_by_time = [ metric for metric in self . iter_metrics ( start , end )] if not sorted_by_time : return sorted_by_time = sorted ( sorted_by_time , key = lambda x : x . timestamp ) start = sorted_by_time [ 0 ] . time for name , group in itertools . groupby ( sorted_by_time , lambda x : int (( x . time - start ) . total_seconds () // tolerance_seconds + 1 )): aggregated_entry = None for entry in group : if not aggregated_entry : aggregated_entry = entry else : aggregated_entry . merge_metric_entry ( entry ) yield aggregated_entry","title":"iter_aggregated_metrics()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.StatusLog.iter_metrics","text":"Iterate through metrics entries individually. Metrics are given for each individual Zeek peer. Parameters: Name Type Description Default start Optional[datetime] UTC start time None end Optional[datetime] UTC end time None Returns: Type Description Generator[MetricsEntry] yields a MetricsEntry for every iteration Source code in dynamite_nsm/services/zeek/logs.py def iter_metrics ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None ) -> Generator [ MetricsEntry ]: \"\"\"Iterate through metrics entries individually. Metrics are given for each individual Zeek peer. Args: start: UTC start time end: UTC end time Returns: yields a MetricsEntry for every iteration \"\"\" def filter_metrics ( s : Optional [ datetime ] = None , e : Optional [ datetime ] = None ): if not e : e = datetime . utcnow () if not s : s = datetime . utcnow () - timedelta ( minutes = 60 ) for en in self . entries : en = MetricsEntry ( json . loads ( en )) if s < en . time < e : yield en for log_entry in filter_metrics ( start , end ): yield log_entry","title":"iter_metrics()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.StatusLog.tail","text":"Tail and follow a log to console Parameters: Name Type Description Default pretty_print Optional[bool] Print the log entry in a nice tabular view True Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def tail ( self , pretty_print : Optional [ bool ] = True ) -> None : \"\"\"Tail and follow a log to console Args: pretty_print: Print the log entry in a nice tabular view Returns: None \"\"\" visited = [] start = datetime . utcnow () - timedelta ( days = 365 ) try : while True : end = datetime . utcnow () self . refresh () for metric in self . iter_aggregated_metrics ( start = start , end = end ): if metric . timestamp not in visited : visited . append ( metric . timestamp ) if not pretty_print : print ( json . dumps ( json . loads ( str ( metric )), indent = 1 )) else : status_table = [ [ 'Time' , 'Memory' , 'Timers' , 'Packets on Link' , 'Packets Processed' , 'Packets Dropped' , 'Peers' ], [ metric . time , metric . memory , metric . timers , metric . packets_link , metric . packets_processed , metric . packets_dropped , ' \\n ' . join ( metric . peers )] ] print ( tabulate . tabulate ( status_table , tablefmt = 'fancy_grid' )) if len ( visited ) > 100 : visited = [] start = datetime . utcnow () - timedelta ( seconds = 60 ) time . sleep ( 5 ) except KeyboardInterrupt : print ( utilities . PrintDecorations . colorize ( 'OK' , 'green' ))","title":"tail()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ZeekLogsProxy","text":"A convenience class providing accessibility to Zeek's archived logs; supports gzipped content","title":"ZeekLogsProxy"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ZeekLogsProxy.__init__","text":"Access a log and all of its corresponding archived logs until log_sample_size is reached. Parameters: Name Type Description Default log_name str The name of the Zeek log to retrieve required log_sample_size Optional[int] The max number of log entries to retrieve 1000 Source code in dynamite_nsm/services/zeek/logs.py def __init__ ( self , log_name : str , log_sample_size : Optional [ int ] = 1000 ): \"\"\"Access a log and all of its corresponding archived logs until log_sample_size is reached. Args: log_name: The name of the Zeek log to retrieve log_sample_size: The max number of log entries to retrieve \"\"\" self . entries = [] self . log_name = log_name self . log_sample_size = log_sample_size self . env_file = os . path . join ( const . CONFIG_PATH , 'environment' ) self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . current_log_path = os . path . join ( self . zeek_home , 'logs' , 'current' , log_name ) self . log_archive_directory = os . path . join ( self . zeek_home , 'logs' ) self . load_all_logs ()","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/logs/#dynamite_nsm.services.zeek.logs.ZeekLogsProxy.load_all_logs","text":"Load all logs into memory up to self.log_sample_size Returns: Type Description None None Source code in dynamite_nsm/services/zeek/logs.py def load_all_logs ( self ) -> None : \"\"\" Load all logs into memory up to self.log_sample_size Returns: None \"\"\" archive_directories = [] sorted_log_paths = [] for log_archive_directory in os . listdir ( self . log_archive_directory ): try : archive_directories . append ( ( log_archive_directory , datetime . strptime ( log_archive_directory , '%Y-%m- %d ' ))) except ValueError : pass sorted_archive_directories = sorted ( archive_directories , key = lambda x : x [ 1 ]) for archive_dir_name , _ in sorted_archive_directories : relevant_log_names = [ fname for fname in os . listdir ( os . path . join ( self . log_archive_directory , archive_dir_name )) if fname . startswith ( self . log_name . replace ( '.log' , '' )) and fname . endswith ( '.gz' ) ] for log_archive_file_name in relevant_log_names : log_rotate_time = log_archive_file_name . split ( '.' )[ 1 ] . split ( '-' )[ 0 ] sorted_log_paths . append ( ( os . path . join ( self . log_archive_directory , archive_dir_name , log_archive_file_name ), datetime . strptime ( archive_dir_name + ' ' + log_rotate_time , '%Y-%m- %d %H:%M:%S' )) ) sorted_log_paths = sorted ( sorted_log_paths , key = lambda x : x [ 1 ], reverse = True ) current_log_file = logs . LogFile ( log_path = self . current_log_path , log_sample_size = self . log_sample_size , gzip_decode = False ) self . entries . extend ( current_log_file . entries ) for log_path , log_rotate_date in sorted_log_paths : archived_log_file = logs . LogFile ( log_path , log_sample_size = self . log_sample_size , gzip_decode = True ) remaining_entries_available = self . log_sample_size - len ( self . entries ) if remaining_entries_available > 0 : self . entries . extend ( archived_log_file . entries [ 0 : remaining_entries_available ]) else : break","title":"load_all_logs()"},{"location":"guides/developers/SDK/services/zeek/process/","text":"Process Manager for Zeek processes and sub-processes. To import... from dynamite_nsm.services.zeek import process as zeek_process ProcessManager __init__ ( self , stdout = True , verbose = False , pretty_print_status = False ) special Manage Zeek processes and sub-processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If true, status will be printed in a tabular form False Source code in dynamite_nsm/services/zeek/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Zeek processes and sub-processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If true, status will be printed in a tabular form \"\"\" self . environment_variables = utilities . get_environment_file_dict () self . install_directory = self . environment_variables . get ( 'ZEEK_HOME' ) process . BaseProcessManager . __init__ ( self , 'zeek.service' , 'zeek.process' , log_path = None , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not zeek_profile . ProcessProfiler () . is_installed (): raise general_exceptions . CallProcessError ( \"Zeek is not installed.\" ) status ( self ) Get the status of Zeek processes Returns: Type Description Union[Dict, str] A dictionary or string depending on the value of self.pretty_print_status Source code in dynamite_nsm/services/zeek/process.py def status ( self ) -> Union [ Dict , str ]: \"\"\" Get the status of Zeek processes Returns: A dictionary or string depending on the value of self.pretty_print_status \"\"\" p = subprocess . Popen ( ' {} status' . format ( os . path . join ( self . install_directory , 'bin' , 'zeekctl' )), shell = True , stdout = subprocess . PIPE ) out , err = p . communicate () raw_output = out . decode ( 'utf-8' ) systemd_info = self . sysctl . status ( 'zeek.service' ) systemd_info_dict = { 'command' : systemd_info . cmd , 'exit_code' : systemd_info . exit , } zeek_status = { 'running' : systemd_info . exit == 0 , 'enabled_on_startup' : self . sysctl . is_enabled ( self . systemd_service ) } zeek_subprocesses = [] for line in raw_output . split ( ' \\n ' )[ 1 :]: tokenized_line = re . findall ( r '\\S+' , line ) if len ( tokenized_line ) == 8 : name , _type , host , status , pid , _ , _ , _ = tokenized_line elif len ( tokenized_line ) == 4 : name , _type , host , status = tokenized_line pid = None else : continue zeek_subprocesses . append ( { 'process_name' : name , 'process_type' : _type , 'host' : host , 'status' : status , 'pid' : pid } ) if self . verbose : zeek_status [ 'subprocesses' ] = zeek_subprocesses systemd_info_dict . update ({ 'stdout' : utilities . wrap_text ( systemd_info . out ), 'stderr' : utilities . wrap_text ( systemd_info . err ) }) else : zeek_status [ 'subprocess_count' ] = len ( zeek_subprocesses ) if self . log_path : zeek_status . update ({ 'logs' : self . log_path }) zeek_status [ 'info' ] = systemd_info_dict if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize status_tbl = [[ 'Service' , self . name , ], [ 'Running' , colorize ( 'yes' , 'green' ) if zeek_status [ 'running' ] else colorize ( 'no' , 'red' )], [ 'Enabled on Startup' , colorize ( 'yes' , 'green' ) if zeek_status [ 'enabled_on_startup' ] else colorize ( 'no' , 'red' )]] if self . verbose : for sp in zeek_subprocesses : status_tbl . append ( [ sp [ 'process_name' ], ' {} ' . format ( sp [ 'pid' ]) ] ) else : status_tbl . append ([ 'Subprocesses' , len ( zeek_subprocesses )]) if zeek_status [ 'info' ] . get ( 'command' ): status_tbl . append ([ 'Command' , zeek_status [ 'info' ] . get ( 'command' ) ]) if zeek_status [ 'info' ] . get ( 'exit_code' ): status_tbl . append ([ 'Exit Code' , zeek_status [ 'info' ] . get ( 'exit_code' ) ]) if zeek_status [ 'info' ] . get ( 'stdout' ): status_tbl . append ([ 'STDOUT' , zeek_status [ 'info' ] . get ( 'stdout' ) ]) if zeek_status [ 'info' ] . get ( 'stderr' ): status_tbl . append ([ 'STDERR' , zeek_status [ 'info' ] . get ( 'stderr' ) ]) return tabulate . tabulate ( status_tbl , tablefmt = 'fancy_grid' ) return zeek_status","title":"process"},{"location":"guides/developers/SDK/services/zeek/process/#dynamite_nsm.services.zeek.process.ProcessManager","text":"","title":"ProcessManager"},{"location":"guides/developers/SDK/services/zeek/process/#dynamite_nsm.services.zeek.process.ProcessManager.__init__","text":"Manage Zeek processes and sub-processes Parameters: Name Type Description Default stdout Optional[bool] Print output to console True verbose Optional[bool] Include detailed debug messages False pretty_print_status Optional[bool] If true, status will be printed in a tabular form False Source code in dynamite_nsm/services/zeek/process.py def __init__ ( self , stdout : Optional [ bool ] = True , verbose : Optional [ bool ] = False , pretty_print_status : Optional [ bool ] = False ): \"\"\"Manage Zeek processes and sub-processes Args: stdout: Print output to console verbose: Include detailed debug messages pretty_print_status: If true, status will be printed in a tabular form \"\"\" self . environment_variables = utilities . get_environment_file_dict () self . install_directory = self . environment_variables . get ( 'ZEEK_HOME' ) process . BaseProcessManager . __init__ ( self , 'zeek.service' , 'zeek.process' , log_path = None , stdout = stdout , verbose = verbose , pretty_print_status = pretty_print_status ) if not zeek_profile . ProcessProfiler () . is_installed (): raise general_exceptions . CallProcessError ( \"Zeek is not installed.\" )","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/process/#dynamite_nsm.services.zeek.process.ProcessManager.status","text":"Get the status of Zeek processes Returns: Type Description Union[Dict, str] A dictionary or string depending on the value of self.pretty_print_status Source code in dynamite_nsm/services/zeek/process.py def status ( self ) -> Union [ Dict , str ]: \"\"\" Get the status of Zeek processes Returns: A dictionary or string depending on the value of self.pretty_print_status \"\"\" p = subprocess . Popen ( ' {} status' . format ( os . path . join ( self . install_directory , 'bin' , 'zeekctl' )), shell = True , stdout = subprocess . PIPE ) out , err = p . communicate () raw_output = out . decode ( 'utf-8' ) systemd_info = self . sysctl . status ( 'zeek.service' ) systemd_info_dict = { 'command' : systemd_info . cmd , 'exit_code' : systemd_info . exit , } zeek_status = { 'running' : systemd_info . exit == 0 , 'enabled_on_startup' : self . sysctl . is_enabled ( self . systemd_service ) } zeek_subprocesses = [] for line in raw_output . split ( ' \\n ' )[ 1 :]: tokenized_line = re . findall ( r '\\S+' , line ) if len ( tokenized_line ) == 8 : name , _type , host , status , pid , _ , _ , _ = tokenized_line elif len ( tokenized_line ) == 4 : name , _type , host , status = tokenized_line pid = None else : continue zeek_subprocesses . append ( { 'process_name' : name , 'process_type' : _type , 'host' : host , 'status' : status , 'pid' : pid } ) if self . verbose : zeek_status [ 'subprocesses' ] = zeek_subprocesses systemd_info_dict . update ({ 'stdout' : utilities . wrap_text ( systemd_info . out ), 'stderr' : utilities . wrap_text ( systemd_info . err ) }) else : zeek_status [ 'subprocess_count' ] = len ( zeek_subprocesses ) if self . log_path : zeek_status . update ({ 'logs' : self . log_path }) zeek_status [ 'info' ] = systemd_info_dict if self . pretty_print_status : colorize = utilities . PrintDecorations . colorize status_tbl = [[ 'Service' , self . name , ], [ 'Running' , colorize ( 'yes' , 'green' ) if zeek_status [ 'running' ] else colorize ( 'no' , 'red' )], [ 'Enabled on Startup' , colorize ( 'yes' , 'green' ) if zeek_status [ 'enabled_on_startup' ] else colorize ( 'no' , 'red' )]] if self . verbose : for sp in zeek_subprocesses : status_tbl . append ( [ sp [ 'process_name' ], ' {} ' . format ( sp [ 'pid' ]) ] ) else : status_tbl . append ([ 'Subprocesses' , len ( zeek_subprocesses )]) if zeek_status [ 'info' ] . get ( 'command' ): status_tbl . append ([ 'Command' , zeek_status [ 'info' ] . get ( 'command' ) ]) if zeek_status [ 'info' ] . get ( 'exit_code' ): status_tbl . append ([ 'Exit Code' , zeek_status [ 'info' ] . get ( 'exit_code' ) ]) if zeek_status [ 'info' ] . get ( 'stdout' ): status_tbl . append ([ 'STDOUT' , zeek_status [ 'info' ] . get ( 'stdout' ) ]) if zeek_status [ 'info' ] . get ( 'stderr' ): status_tbl . append ([ 'STDERR' , zeek_status [ 'info' ] . get ( 'stderr' ) ]) return tabulate . tabulate ( status_tbl , tablefmt = 'fancy_grid' ) return zeek_status","title":"status()"},{"location":"guides/developers/SDK/services/zeek/profile/","text":"Profile Zeek processes to ensure they are running and installed properly. To import... from dynamite_nsm.services.zeek import profile as zeek_profile ProcessProfiler __init__ ( self ) special Get information about the Zeek service Source code in dynamite_nsm/services/zeek/profile.py def __init__ ( self ): \"\"\" Get information about the Zeek service \"\"\" self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . zeek_scripts = self . env_dict . get ( 'ZEEK_SCRIPTS' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . zeek_home , config_directory = self . zeek_scripts , required_install_files = [ 'bin' , 'etc' ], required_config_files = [ 'site' ]) is_attached_to_network ( self ) Determine if Zeek is bound to one or more network interfaces Returns: Type Description bool True, if attached to one or more network interfaces Source code in dynamite_nsm/services/zeek/profile.py def is_attached_to_network ( self ) -> bool : \"\"\"Determine if Zeek is bound to one or more network interfaces Returns: True, if attached to one or more network interfaces \"\"\" return any ( self . get_attached_interfaces ()) is_running ( self ) Determine of Zeek is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/zeek/profile.py def is_running ( self ) -> bool : \"\"\" Determine of Zeek is running Returns: True, if running \"\"\" if self . zeek_home : try : return zeek_process . ProcessManager () . status ()[ 'running' ] except KeyError : return zeek_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"profile"},{"location":"guides/developers/SDK/services/zeek/profile/#dynamite_nsm.services.zeek.profile.ProcessProfiler","text":"","title":"ProcessProfiler"},{"location":"guides/developers/SDK/services/zeek/profile/#dynamite_nsm.services.zeek.profile.ProcessProfiler.__init__","text":"Get information about the Zeek service Source code in dynamite_nsm/services/zeek/profile.py def __init__ ( self ): \"\"\" Get information about the Zeek service \"\"\" self . env_dict = utilities . get_environment_file_dict () self . zeek_home = self . env_dict . get ( 'ZEEK_HOME' ) self . zeek_scripts = self . env_dict . get ( 'ZEEK_SCRIPTS' ) profile . BaseProcessProfiler . __init__ ( self , install_directory = self . zeek_home , config_directory = self . zeek_scripts , required_install_files = [ 'bin' , 'etc' ], required_config_files = [ 'site' ])","title":"__init__()"},{"location":"guides/developers/SDK/services/zeek/profile/#dynamite_nsm.services.zeek.profile.ProcessProfiler.is_attached_to_network","text":"Determine if Zeek is bound to one or more network interfaces Returns: Type Description bool True, if attached to one or more network interfaces Source code in dynamite_nsm/services/zeek/profile.py def is_attached_to_network ( self ) -> bool : \"\"\"Determine if Zeek is bound to one or more network interfaces Returns: True, if attached to one or more network interfaces \"\"\" return any ( self . get_attached_interfaces ())","title":"is_attached_to_network()"},{"location":"guides/developers/SDK/services/zeek/profile/#dynamite_nsm.services.zeek.profile.ProcessProfiler.is_running","text":"Determine of Zeek is running Returns: Type Description bool True, if running Source code in dynamite_nsm/services/zeek/profile.py def is_running ( self ) -> bool : \"\"\" Determine of Zeek is running Returns: True, if running \"\"\" if self . zeek_home : try : return zeek_process . ProcessManager () . status ()[ 'running' ] except KeyError : return zeek_process . ProcessManager () . status ()[ 'RUNNING' ] return False","title":"is_running()"},{"location":"guides/for_security_analysts/kibana/","text":"Description Kibana is a powerful tool for visualizing data stored within Elasticsearch. DynamiteNSM has been redesigned to work on top of the OpenSearch stack , a fully open-source derivative of the ElasticStack. Packages DynamiteNSM 1.0 introduces the concept of Kibana packages. Packages contain things like dashboards, visualizations, saved searches and index patterns and can easily be installed uninstalled through the dynamite commandline utility.","title":"Index"},{"location":"guides/for_security_analysts/kibana/#description","text":"Kibana is a powerful tool for visualizing data stored within Elasticsearch. DynamiteNSM has been redesigned to work on top of the OpenSearch stack , a fully open-source derivative of the ElasticStack.","title":"Description"},{"location":"guides/for_security_analysts/kibana/#packages","text":"DynamiteNSM 1.0 introduces the concept of Kibana packages. Packages contain things like dashboards, visualizations, saved searches and index patterns and can easily be installed uninstalled through the dynamite commandline utility.","title":"Packages"},{"location":"guides/for_security_analysts/kibana/packages/dynamite_investigator/","text":"Description Installed by default this Kibana package provides several dashboards for discovering suspicious/malicious behaviors on your network. The dashboards are organized around three primary data-models: alerts, events, and hosts. This package takes advantage of the shared identifiers embedded in generated models to facilitate deep-dives and pivots. Hosts Information from the perspective of hosts on the network. Events and alerts are aggregated over a 24 hour time period to record statistical information for hosts. Host definitions are refreshed every 10 minutes . This dashboard provides the ability to drilldown into alerts and connections associated with a host. Hosts within this view can easily be filtered by whether they are internal or external to the monitored network. Alerts Traditional IDS (EmergingThreats) alerts show up here. This dashboard provides the ability to pivot to corresponding connections and underlying protocols. Connections Conversations between hosts show up here. This view is primarily concerned with transport layer details of a connection. Connections can be associated with one or more protocol logs or alerts. This dashboard provides the ability to pivot from a connection into its underlying protocols (if one was detected and parsed). Protocols Application protocol specifics for conversations between hosts. This view is primarily concerned with providing additional details to connections where a protocol was detected.","title":"Dynamite investigator"},{"location":"guides/for_security_analysts/kibana/packages/dynamite_investigator/#description","text":"Installed by default this Kibana package provides several dashboards for discovering suspicious/malicious behaviors on your network. The dashboards are organized around three primary data-models: alerts, events, and hosts. This package takes advantage of the shared identifiers embedded in generated models to facilitate deep-dives and pivots.","title":"Description"},{"location":"guides/for_security_analysts/kibana/packages/dynamite_investigator/#hosts","text":"Information from the perspective of hosts on the network. Events and alerts are aggregated over a 24 hour time period to record statistical information for hosts. Host definitions are refreshed every 10 minutes . This dashboard provides the ability to drilldown into alerts and connections associated with a host. Hosts within this view can easily be filtered by whether they are internal or external to the monitored network.","title":"Hosts"},{"location":"guides/for_security_analysts/kibana/packages/dynamite_investigator/#alerts","text":"Traditional IDS (EmergingThreats) alerts show up here. This dashboard provides the ability to pivot to corresponding connections and underlying protocols.","title":"Alerts"},{"location":"guides/for_security_analysts/kibana/packages/dynamite_investigator/#connections","text":"Conversations between hosts show up here. This view is primarily concerned with transport layer details of a connection. Connections can be associated with one or more protocol logs or alerts. This dashboard provides the ability to pivot from a connection into its underlying protocols (if one was detected and parsed).","title":"Connections"},{"location":"guides/for_security_analysts/kibana/packages/dynamite_investigator/#protocols","text":"Application protocol specifics for conversations between hosts. This view is primarily concerned with providing additional details to connections where a protocol was detected.","title":"Protocols"},{"location":"installation/01_overview/","text":"Installation Overview \u26a0\ufe0f We will need to make changes to your system in order to function properly. We highly recommend installing the dynamite-nsm package on a fresh installation of Linux . \u26a0\ufe0f root access is needed as dynamite will need to be able to install and uninstall services. dynamite-nsm is a Python3.7+ compatible package that can be installed directly through pip . sudo pip install dynamite-nsm Alternatively, developers may also wish to build from source. git clone https://github.com/DynamiteAI/dynamite-nsm.git && sudo pip install dynamite-nsm/ \u24d8 If you opt, for this method, and are curious about contributing to this project be sure to check out our developer guides ! Once installed, you should be able to call dynamite directly from the commandline. We will use the dynamite commandline utility for setting up and managing all the services we install. Keep in mind that the dynamite utility requires root access in order to run install and uninstall commands. Before you can begin installing services you must initialize the system with the setup bootstrapper as root. sudo dynamite setup install This command will download configuration updates, setup directory structure, and add a sudoers.d/ policy allowing users in the dynamite group to run some elevated commands. Troubleshooting Command not found Symptoms : You run any dynamite command, but the shell reports that the dynamite Could not be found or something similar. Problem Description Solution dynamite not in $PATH Occurs when /usr/local/bin is not in the $PATH. Simply create a symlink ln -s /usr/local/bin/dynamite /usr/bin/dynamite and then re-run the command. pip install dynamite failed pip install can partially succeed then fail on a single dependency preventing the installation to continue. Re-run the pip install command again, note any errors. Often these can be corrected by installing pre-requisite OS libraries. A few common libraries that when missing can cause issues: ( gcc , g++ , make , python3-pip )","title":"Installation Overview"},{"location":"installation/01_overview/#installation-overview","text":"\u26a0\ufe0f We will need to make changes to your system in order to function properly. We highly recommend installing the dynamite-nsm package on a fresh installation of Linux . \u26a0\ufe0f root access is needed as dynamite will need to be able to install and uninstall services. dynamite-nsm is a Python3.7+ compatible package that can be installed directly through pip . sudo pip install dynamite-nsm Alternatively, developers may also wish to build from source. git clone https://github.com/DynamiteAI/dynamite-nsm.git && sudo pip install dynamite-nsm/ \u24d8 If you opt, for this method, and are curious about contributing to this project be sure to check out our developer guides ! Once installed, you should be able to call dynamite directly from the commandline. We will use the dynamite commandline utility for setting up and managing all the services we install. Keep in mind that the dynamite utility requires root access in order to run install and uninstall commands. Before you can begin installing services you must initialize the system with the setup bootstrapper as root. sudo dynamite setup install This command will download configuration updates, setup directory structure, and add a sudoers.d/ policy allowing users in the dynamite group to run some elevated commands.","title":"Installation Overview"},{"location":"installation/01_overview/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/01_overview/#command-not-found","text":"Symptoms : You run any dynamite command, but the shell reports that the dynamite Could not be found or something similar. Problem Description Solution dynamite not in $PATH Occurs when /usr/local/bin is not in the $PATH. Simply create a symlink ln -s /usr/local/bin/dynamite /usr/bin/dynamite and then re-run the command. pip install dynamite failed pip install can partially succeed then fail on a single dependency preventing the installation to continue. Re-run the pip install command again, note any errors. Often these can be corrected by installing pre-requisite OS libraries. A few common libraries that when missing can cause issues: ( gcc , g++ , make , python3-pip )","title":"Command not found"},{"location":"installation/agent/01_overview/","text":"Overview There are many variables that influence how you set up an agent, such as the hardware you have, the amount of traffic you expect, and the type of traffic you want to monitor. There are two paths for installing the agent. Install all the agent services on the same physical instance. This is the easiest option but requires more serious hardware . Install services on separate instances . Useful when either Zeek or Suricata is not needed, or when the available hardware is below the above hardware specifications for a shared instance.","title":"Overview"},{"location":"installation/agent/01_overview/#overview","text":"There are many variables that influence how you set up an agent, such as the hardware you have, the amount of traffic you expect, and the type of traffic you want to monitor. There are two paths for installing the agent. Install all the agent services on the same physical instance. This is the easiest option but requires more serious hardware . Install services on separate instances . Useful when either Zeek or Suricata is not needed, or when the available hardware is below the above hardware specifications for a shared instance.","title":"Overview"},{"location":"installation/agent/02_configure_inspection_interface/","text":"Inspection Interface(s) Inspection interfaces receive traffic from a SPAN port or TAP device . Typically, they do not need IP addresses. \u24d8 A notable exception is on AWS's port-mirroring implementation for VPCs which relies on VXLAN encapsulation requiring the monitored interface to have a routable ip address. Disabling any NIC offloading functions such as tso , gso , and gro can also improve performance. /etc/network/interfaces auto mon0 iface mon0 inet manual up ifconfig $IFACE -arp up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down post-up for i in rx tx sg tso ufo gso gro lro; do ethtool -K $IFACE $i off; done post-up echo 1 > /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6","title":"Preparing Inspection Interfaces"},{"location":"installation/agent/02_configure_inspection_interface/#inspection-interfaces","text":"Inspection interfaces receive traffic from a SPAN port or TAP device . Typically, they do not need IP addresses. \u24d8 A notable exception is on AWS's port-mirroring implementation for VPCs which relies on VXLAN encapsulation requiring the monitored interface to have a routable ip address. Disabling any NIC offloading functions such as tso , gso , and gro can also improve performance.","title":"Inspection Interface(s)"},{"location":"installation/agent/02_configure_inspection_interface/#etcnetworkinterfaces","text":"auto mon0 iface mon0 inet manual up ifconfig $IFACE -arp up up ip link set $IFACE promisc on down ip link set $IFACE promisc off down ifconfig $IFACE down post-up for i in rx tx sg tso ufo gso gro lro; do ethtool -K $IFACE $i off; done post-up echo 1 > /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6","title":"/etc/network/interfaces"},{"location":"installation/agent/03_install_same_instance/","text":"Install on Same Instance \u26a0\ufe0f If you ever change the number of CPU cores or inspection interfaces on an agent instance simply run: dynamite agent optimize --inspect-interfaces <int1f> <intf2>... to automatically adjust CPU-affinity and threading families. Update Default Configs and Mirrors Make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install Install Using the Agent Service DynamiteNSM provides a convenience service called agent which bundles zeek , suricata , and filebeat services into a single deployment. In the below example traffic on the interfaces eth0 eth1 and eth3 will be monitored; results will be sent to an elasticsearch instance to the url: https://dynamite-monitor:9200 . sudo dynamite agent install --inspect-interfaces eth0 eth1 eth2 --targets https://dynamite-monitor:9200 Start the Processes Once installed, you can start the process and check its status using the below commands. sudo dynamite agent process start sudo dynamite agent process status Check the Logs Once Filebeat has successfully started, you can see how many events were sent to Elasticsearch. sudo dynamite filebeat logs metrics If Filebeat is not started or events are not populating downstream check the Filebeat log file or tail the summary with the below command. sudo dynamite filebeat logs main","title":"Install on the Same Instance"},{"location":"installation/agent/03_install_same_instance/#install-on-same-instance","text":"\u26a0\ufe0f If you ever change the number of CPU cores or inspection interfaces on an agent instance simply run: dynamite agent optimize --inspect-interfaces <int1f> <intf2>... to automatically adjust CPU-affinity and threading families.","title":"Install on Same Instance"},{"location":"installation/agent/03_install_same_instance/#update-default-configs-and-mirrors","text":"Make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install","title":"Update Default Configs and Mirrors"},{"location":"installation/agent/03_install_same_instance/#install-using-the-agent-service","text":"DynamiteNSM provides a convenience service called agent which bundles zeek , suricata , and filebeat services into a single deployment. In the below example traffic on the interfaces eth0 eth1 and eth3 will be monitored; results will be sent to an elasticsearch instance to the url: https://dynamite-monitor:9200 . sudo dynamite agent install --inspect-interfaces eth0 eth1 eth2 --targets https://dynamite-monitor:9200","title":"Install Using the Agent Service"},{"location":"installation/agent/03_install_same_instance/#start-the-processes","text":"Once installed, you can start the process and check its status using the below commands. sudo dynamite agent process start sudo dynamite agent process status","title":"Start the Processes"},{"location":"installation/agent/03_install_same_instance/#check-the-logs","text":"Once Filebeat has successfully started, you can see how many events were sent to Elasticsearch. sudo dynamite filebeat logs metrics If Filebeat is not started or events are not populating downstream check the Filebeat log file or tail the summary with the below command. sudo dynamite filebeat logs main","title":"Check the Logs"},{"location":"installation/agent/04_install_separate_instances/","text":"Install across Separate Instances \u26a0\ufe0f If you ever change the number of CPU cores or inspection interfaces on an agent instance simply run: dynamite agent optimize --inspect-interfaces <int1f> <intf2>... to automatically adjust CPU-affinity and threading families. Zeek and Suricata can also be installed independently on their own dedicated instances. However, Filebeat must be installed alongside for logs to be forwarded downstream. Update Default Configs and Mirrors On each instance make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install Dedicated Zeek Instance Install Zeek Service Install Zeek configured to monitor several inspection interfaces. In the below example traffic on the interfaces eth0 eth1 and eth3 will be monitored. sudo dynamite zeek install --inspect-interfaces eth0 eth1 eth2 Install Filebeat Service Next, install Filebeat which in our below example will forward the logs on to a Kafka instance at the address dynamite-broker.local:9092 sudo dynamite filebeat install --target-type=kafka --target-strings dynamite-broker.local:9092 Configure Filebeat Service Let's say our Kafka instance requires authentication and of course we need to specify a topic. We don't handle this at initial install time, instead, once installation is complete you can set a password via dynamite filebeat config command. sudo dynamite filebeat config main kafka_targets --username admin --password admin --topic events \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 topic \u2502 events \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 username \u2502 admin \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 password \u2502 admin \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Start the Processes Once installed and configured you can start each process separately or use the dynamite agent convenience service. sudo dynamite filebeat process start sudo dynamite zeek process start Dedicated Suricata Instance Install Suricata to install a single inspection interface in the below example traffic on mon0 will be monitored. Install Suricata Service sudo dynamite suricata install --inspect-interfaces mon0 Install Filebeat Service Now, install Filebeat which in this example will forward the logs directly to Elasticsearch instance at https://dynamite-monitor:9200 . sudo dynamite filebeat install --target-type=elasticsearch --target-strings https://dynamite-monitor:9200 Configure Filebeat Service Configure the credentials separately via dynamite filebeat config . sudo dynamite filebeat config main elasticsearch_targets --username admin --password admin Start the Processes Once installed and configured you can start each process separately or use the dynamite agent convenience service. sudo dynamite filebeat process start sudo dynamite suricata process start","title":"Install across Separate Instances"},{"location":"installation/agent/04_install_separate_instances/#install-across-separate-instances","text":"\u26a0\ufe0f If you ever change the number of CPU cores or inspection interfaces on an agent instance simply run: dynamite agent optimize --inspect-interfaces <int1f> <intf2>... to automatically adjust CPU-affinity and threading families. Zeek and Suricata can also be installed independently on their own dedicated instances. However, Filebeat must be installed alongside for logs to be forwarded downstream.","title":"Install across Separate Instances"},{"location":"installation/agent/04_install_separate_instances/#update-default-configs-and-mirrors","text":"On each instance make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install","title":"Update Default Configs and Mirrors"},{"location":"installation/agent/04_install_separate_instances/#dedicated-zeek-instance","text":"","title":"Dedicated Zeek Instance"},{"location":"installation/agent/04_install_separate_instances/#install-zeek-service","text":"Install Zeek configured to monitor several inspection interfaces. In the below example traffic on the interfaces eth0 eth1 and eth3 will be monitored. sudo dynamite zeek install --inspect-interfaces eth0 eth1 eth2","title":"Install Zeek Service"},{"location":"installation/agent/04_install_separate_instances/#install-filebeat-service","text":"Next, install Filebeat which in our below example will forward the logs on to a Kafka instance at the address dynamite-broker.local:9092 sudo dynamite filebeat install --target-type=kafka --target-strings dynamite-broker.local:9092","title":"Install Filebeat Service"},{"location":"installation/agent/04_install_separate_instances/#configure-filebeat-service","text":"Let's say our Kafka instance requires authentication and of course we need to specify a topic. We don't handle this at initial install time, instead, once installation is complete you can set a password via dynamite filebeat config command. sudo dynamite filebeat config main kafka_targets --username admin --password admin --topic events \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Config Option \u2502 Value \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 topic \u2502 events \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 username \u2502 admin \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 password \u2502 admin \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b","title":"Configure Filebeat Service"},{"location":"installation/agent/04_install_separate_instances/#start-the-processes","text":"Once installed and configured you can start each process separately or use the dynamite agent convenience service. sudo dynamite filebeat process start sudo dynamite zeek process start","title":"Start the Processes"},{"location":"installation/agent/04_install_separate_instances/#dedicated-suricata-instance","text":"Install Suricata to install a single inspection interface in the below example traffic on mon0 will be monitored.","title":"Dedicated Suricata Instance"},{"location":"installation/agent/04_install_separate_instances/#install-suricata-service","text":"sudo dynamite suricata install --inspect-interfaces mon0","title":"Install Suricata Service"},{"location":"installation/agent/04_install_separate_instances/#install-filebeat-service_1","text":"Now, install Filebeat which in this example will forward the logs directly to Elasticsearch instance at https://dynamite-monitor:9200 . sudo dynamite filebeat install --target-type=elasticsearch --target-strings https://dynamite-monitor:9200","title":"Install Filebeat Service"},{"location":"installation/agent/04_install_separate_instances/#configure-filebeat-service_1","text":"Configure the credentials separately via dynamite filebeat config . sudo dynamite filebeat config main elasticsearch_targets --username admin --password admin","title":"Configure Filebeat Service"},{"location":"installation/agent/04_install_separate_instances/#start-the-processes_1","text":"Once installed and configured you can start each process separately or use the dynamite agent convenience service. sudo dynamite filebeat process start sudo dynamite suricata process start","title":"Start the Processes"},{"location":"installation/monitor/01_overview/","text":"Overview The monitor refers to the services required for retention, querying, and presentation. The monitor will install Elasticsearch and Kibana. \u26a0\ufe0f Logstash, is an optional component that can be installed separately, however we currently don't provide an automated integration strategy with Logstash. Meaning, if you wish to have agents forward to Logstash, you will need to create your own input and output pipelines. Install Elasticsearch and Kibana on the same physical instance. This is the simplest option, and in most situations the best one. Use these hardware guidelines as a starting place. Install Elasticsearch and Kibana on separate instances. Setup Elasticsearch for authentication to Kibana. This might be a good option if you want Elasticsearch running in a different network segment.","title":"Overview"},{"location":"installation/monitor/01_overview/#overview","text":"The monitor refers to the services required for retention, querying, and presentation. The monitor will install Elasticsearch and Kibana. \u26a0\ufe0f Logstash, is an optional component that can be installed separately, however we currently don't provide an automated integration strategy with Logstash. Meaning, if you wish to have agents forward to Logstash, you will need to create your own input and output pipelines. Install Elasticsearch and Kibana on the same physical instance. This is the simplest option, and in most situations the best one. Use these hardware guidelines as a starting place. Install Elasticsearch and Kibana on separate instances. Setup Elasticsearch for authentication to Kibana. This might be a good option if you want Elasticsearch running in a different network segment.","title":"Overview"},{"location":"installation/monitor/02_network_interface_configuration/","text":"Configure Management Interface The management interface is used for connecting to a variety of services available for viewing and working with data produced by the agent. You can use DHCP for the management interface, however within production environments static network interfaces are encouraged. /etc/network/interfaces DHCP auto eth0 iface eth0 inet dhcp Static auto eth0 iface eth0 inet static address 192.168.1.14 gateway 192.168.1.1 netmask 255.255.255.0 network 192.168.1.0 broadcast 192.168.1.255 dns-nameservers 192.168.1.1 192.168.1.2 Post Configuration Once your interfaces are setup. Reboot to apply the changes. Once the reboot is complete, confirm that your interface is up with a tool such as ifconfig .","title":"Setup Management Interface"},{"location":"installation/monitor/02_network_interface_configuration/#configure-management-interface","text":"The management interface is used for connecting to a variety of services available for viewing and working with data produced by the agent. You can use DHCP for the management interface, however within production environments static network interfaces are encouraged.","title":"Configure Management Interface"},{"location":"installation/monitor/02_network_interface_configuration/#etcnetworkinterfaces","text":"","title":"/etc/network/interfaces"},{"location":"installation/monitor/02_network_interface_configuration/#dhcp","text":"auto eth0 iface eth0 inet dhcp","title":"DHCP"},{"location":"installation/monitor/02_network_interface_configuration/#static","text":"auto eth0 iface eth0 inet static address 192.168.1.14 gateway 192.168.1.1 netmask 255.255.255.0 network 192.168.1.0 broadcast 192.168.1.255 dns-nameservers 192.168.1.1 192.168.1.2","title":"Static"},{"location":"installation/monitor/02_network_interface_configuration/#post-configuration","text":"Once your interfaces are setup. Reboot to apply the changes. Once the reboot is complete, confirm that your interface is up with a tool such as ifconfig .","title":"Post Configuration"},{"location":"installation/monitor/03_install_same_instance/","text":"Install on Same Instance Update Default Configs and Mirrors Make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install Install Using the Monitor Service Install Elasticsearch and Kibana with several security features enabled and Dynamite's default Kibana packages installed . Both services will listen on the primary interface (default route). sudo dynamite monitor install Start the Processes Once installed, you can check the process using the below command. sudo dynamite monitor process status","title":"Install on the Same Instance"},{"location":"installation/monitor/03_install_same_instance/#install-on-same-instance","text":"","title":"Install on Same Instance"},{"location":"installation/monitor/03_install_same_instance/#update-default-configs-and-mirrors","text":"Make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install","title":"Update Default Configs and Mirrors"},{"location":"installation/monitor/03_install_same_instance/#install-using-the-monitor-service","text":"Install Elasticsearch and Kibana with several security features enabled and Dynamite's default Kibana packages installed . Both services will listen on the primary interface (default route). sudo dynamite monitor install","title":"Install Using the Monitor Service"},{"location":"installation/monitor/03_install_same_instance/#start-the-processes","text":"Once installed, you can check the process using the below command. sudo dynamite monitor process status","title":"Start the Processes"},{"location":"installation/monitor/04_install_separate_instances/","text":"Install across Separate Instances In some situations, such as a multi-node Elasticsearch deployment, you may not always wish to install Elasticsearch or Kibana separately. Update Default Configs and Mirrors On each instance make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install Dedicated Elasticsearch Instance By default, Elasticsearch will bind itself to the primary IP address, this can be overridden with the --network-host flag. Install Elasticsearch Service sudo dynamite elasticsearch install Start the Process Once installed, you can manage Elasticsearch with the process command. sudo dynamite elasticsearch process start sudo dynamite elasticsearch process status --verbose Dedicated Kibana Instance Test the Connection to Elasticsearch Before you begin the Kibana installation ensure that Elasticsearch API is up and reachable from whatever host you want to install Kibana on. curl https://192.168.194.143:9200 -u admin:admin --insecure You should receive a message similar to the one below: { \"name\" : \"ip1723021417_es_node\", \"cluster_name\" : \"dynamite-nsm-cluster\", \"cluster_uuid\" : \"GIUvsLdlRb-WyQD5j3kd6Q\", \"version\" : { \"number\" : \"7.10.2\", \"build_flavor\" : \"oss\", \"build_type\" : \"tar\", \"build_hash\" : \"747e1cc71def077253878a59143c1f785afa92b9\", \"build_date\" : \"2021-01-13T00:42:12.435326Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.7.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Now simply run the dynamite kibana install command specifying any --elasticsearch-targets . As with elasticsearch you can override the bound IP address with the --host flag. Install Kibana Service sudo dynamite kibana install --elasticsearch-targets https://192.168.194.143:9200 Start the Process Once installed, you can manage Kibana with the process command. sudo dynamite kibana process start sudo dynamite kibana process status --verbose","title":"Install across Separate Instances"},{"location":"installation/monitor/04_install_separate_instances/#install-across-separate-instances","text":"In some situations, such as a multi-node Elasticsearch deployment, you may not always wish to install Elasticsearch or Kibana separately.","title":"Install across Separate Instances"},{"location":"installation/monitor/04_install_separate_instances/#update-default-configs-and-mirrors","text":"On each instance make sure you have the latest default configurations and mirrors for the version of DynamiteNSM you have installed. sudo dynamite updates install","title":"Update Default Configs and Mirrors"},{"location":"installation/monitor/04_install_separate_instances/#dedicated-elasticsearch-instance","text":"By default, Elasticsearch will bind itself to the primary IP address, this can be overridden with the --network-host flag.","title":"Dedicated Elasticsearch Instance"},{"location":"installation/monitor/04_install_separate_instances/#install-elasticsearch-service","text":"sudo dynamite elasticsearch install","title":"Install Elasticsearch Service"},{"location":"installation/monitor/04_install_separate_instances/#start-the-process","text":"Once installed, you can manage Elasticsearch with the process command. sudo dynamite elasticsearch process start sudo dynamite elasticsearch process status --verbose","title":"Start the Process"},{"location":"installation/monitor/04_install_separate_instances/#dedicated-kibana-instance","text":"","title":"Dedicated Kibana Instance"},{"location":"installation/monitor/04_install_separate_instances/#test-the-connection-to-elasticsearch","text":"Before you begin the Kibana installation ensure that Elasticsearch API is up and reachable from whatever host you want to install Kibana on. curl https://192.168.194.143:9200 -u admin:admin --insecure You should receive a message similar to the one below: { \"name\" : \"ip1723021417_es_node\", \"cluster_name\" : \"dynamite-nsm-cluster\", \"cluster_uuid\" : \"GIUvsLdlRb-WyQD5j3kd6Q\", \"version\" : { \"number\" : \"7.10.2\", \"build_flavor\" : \"oss\", \"build_type\" : \"tar\", \"build_hash\" : \"747e1cc71def077253878a59143c1f785afa92b9\", \"build_date\" : \"2021-01-13T00:42:12.435326Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.7.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Now simply run the dynamite kibana install command specifying any --elasticsearch-targets . As with elasticsearch you can override the bound IP address with the --host flag.","title":"Test the Connection to Elasticsearch"},{"location":"installation/monitor/04_install_separate_instances/#install-kibana-service","text":"sudo dynamite kibana install --elasticsearch-targets https://192.168.194.143:9200","title":"Install Kibana Service"},{"location":"installation/monitor/04_install_separate_instances/#start-the-process_1","text":"Once installed, you can manage Kibana with the process command. sudo dynamite kibana process start sudo dynamite kibana process status --verbose","title":"Start the Process"},{"location":"requirements/01_supported_operating_systems/","text":"Supported Operating Systems DynamiteNSM has been tested on all the below Linux Distributions. Included below are preparation guides for installing common dependencies. For some of these operating systems we have compiled some basic preparation guides. Linux Distro Agent Services Monitor Services Preparation Guide AlmaLinux 8 \u2705 \u2705 View CentOs 7 \u2705 \u2705 View CentOs 8 \u2705 \u2705 Debian 9 \u2705 \u2705 Debian 10 \u2705 \u2705 View Ubuntu 18.04 \u2705 \u2705 View Ubuntu 18.10 \u2705 \u2705 Ubuntu 19.04 \u2705 \u2705 Ubuntu 19.10 \u2705 \u2705 Ubuntu 20.04 \u2705 \u2705 View","title":"Supported Operating Systems"},{"location":"requirements/01_supported_operating_systems/#supported-operating-systems","text":"DynamiteNSM has been tested on all the below Linux Distributions. Included below are preparation guides for installing common dependencies. For some of these operating systems we have compiled some basic preparation guides. Linux Distro Agent Services Monitor Services Preparation Guide AlmaLinux 8 \u2705 \u2705 View CentOs 7 \u2705 \u2705 View CentOs 8 \u2705 \u2705 Debian 9 \u2705 \u2705 Debian 10 \u2705 \u2705 View Ubuntu 18.04 \u2705 \u2705 View Ubuntu 18.10 \u2705 \u2705 Ubuntu 19.04 \u2705 \u2705 Ubuntu 19.10 \u2705 \u2705 Ubuntu 20.04 \u2705 \u2705 View","title":"Supported Operating Systems"},{"location":"requirements/02_agent_specifications/","text":"Agent Specifications CPUs Choosing the right CPUs can have a major impact on overall system performance. \u24d8 CPU considerations Minimum 2.1GHz Hyper Threading Enabled At least 10MB L3 cache Sandy Bridge Microarchitecture or newer Agent Size Max Sustained Throughput CPU/Cores Threads Mini 500 Mbps 8 16 Small 1 Gbps 16 32 Medium 6 Gbps 36 72 Large 12 Gbps 72 144 Memory As with CPU, the type and amount of RAM heavily influence system performance. The following best-practices should be considered when selecting RAM for use in Dynamite Agents or Monitors. \u24d8 Memory considerations Use fast memory Use minimal DIMMs, e.g. 8x 16GB vs 16x 8GB Evenly distribute DIMMs per socket Agent Size Max Sustained Throughput RAM (GB) Mini 300 Mbps 32 Small 1 Gbps 64 Medium 6 Gbps 320 Large 12 Gbps 512 Network Inspection Interfaces The type of network interface card (NIC) heavily influences the agent\u2019s ability to efficiently capture and process packets. Dynamite recommends the following NIC models for use as traffic inspection interfaces as they have well-maintained drivers with the feature sets needed for tuning: Intel X550 Intel X710 Mellanox Data Drives A separate data storage volume should be created to maximize write performance and alleviate I/O contention with the OS. Creating a separate data volume also helps ensure storage consumption will not adversely affect OS operation. The data storage volume should be configured with: \u24d8 HDD considerations - 10K RPM HDD or SSD (higher I/O but fewer possible read/write operations over lifespan) - All drives the same make/model - RAID 0 for fast I/O Agent Size Max Sustained Throughput Data Storage (TB) Mini 500 Mbps 1 Small 1 Gbps 2 Medium 6 Gbps 8 Large 12 Gbps 16","title":"Agent Specifications"},{"location":"requirements/02_agent_specifications/#agent-specifications","text":"","title":"Agent Specifications"},{"location":"requirements/02_agent_specifications/#cpus","text":"Choosing the right CPUs can have a major impact on overall system performance. \u24d8 CPU considerations Minimum 2.1GHz Hyper Threading Enabled At least 10MB L3 cache Sandy Bridge Microarchitecture or newer Agent Size Max Sustained Throughput CPU/Cores Threads Mini 500 Mbps 8 16 Small 1 Gbps 16 32 Medium 6 Gbps 36 72 Large 12 Gbps 72 144","title":"CPUs"},{"location":"requirements/02_agent_specifications/#memory","text":"As with CPU, the type and amount of RAM heavily influence system performance. The following best-practices should be considered when selecting RAM for use in Dynamite Agents or Monitors. \u24d8 Memory considerations Use fast memory Use minimal DIMMs, e.g. 8x 16GB vs 16x 8GB Evenly distribute DIMMs per socket Agent Size Max Sustained Throughput RAM (GB) Mini 300 Mbps 32 Small 1 Gbps 64 Medium 6 Gbps 320 Large 12 Gbps 512","title":"Memory"},{"location":"requirements/02_agent_specifications/#network-inspection-interfaces","text":"The type of network interface card (NIC) heavily influences the agent\u2019s ability to efficiently capture and process packets. Dynamite recommends the following NIC models for use as traffic inspection interfaces as they have well-maintained drivers with the feature sets needed for tuning: Intel X550 Intel X710 Mellanox","title":"Network Inspection Interfaces"},{"location":"requirements/02_agent_specifications/#data-drives","text":"A separate data storage volume should be created to maximize write performance and alleviate I/O contention with the OS. Creating a separate data volume also helps ensure storage consumption will not adversely affect OS operation. The data storage volume should be configured with: \u24d8 HDD considerations - 10K RPM HDD or SSD (higher I/O but fewer possible read/write operations over lifespan) - All drives the same make/model - RAID 0 for fast I/O Agent Size Max Sustained Throughput Data Storage (TB) Mini 500 Mbps 1 Small 1 Gbps 2 Medium 6 Gbps 8 Large 12 Gbps 16","title":"Data Drives"},{"location":"requirements/03_monitor_specifications/","text":"Monitor Deployment Considerations Monitors combine Elasticsearch, and Kibana into a single instance, and receive traffic from the agents. \u24d8 Logstash is not installed by default, however it is available for installation as a separate component via: dynamite logstash install -h . Manual configuration is required to further integrate Logstash with a Dynamite monitor instance. Data Drives A separate data storage volume should be created to maximize write performance and alleviate I/O contention with the OS. Creating a separate data volume also helps ensure storage consumption will not adversely affect OS operation. The data storage volume should be configured with: - 10K RPM HDD or SSD (higher I/O but fewer possible read/write operations over lifespan) - All drives the same make/model - RAID 0 for fast I/O CPUs Most operations within Elasticsearch are CPU bound, and there are many variables beyond events-per-second that contribute to load. The following options are a good starting place when benchmarking your monitor. Events per Second CPUs/Cores 250 4 1000 8 2500 12 Memory & Disk Elasticsearch is built upon Lucene data-structures which require large HashMaps remain in memory at all time. Depending on the size of your indices, query operations can become very expensive. The following are a good place to start. \u24d8 To avoid I/O contention and ensure data storage consumption does not affect the OS\u2019s ability to function, Dynamite recommends creating a separate storage volume exclusively for use by the OS. The OS storage volume should be configured with: SSD drives for fast I/O, the same make/model RAID 1 for full data redundancy Events per Second RAM (GB) Disk (30-days) Elasticsearch JVM Heap 250 24 305 GB 8 GB 1000 32 1.22 TB 12 GB 2500 64 3.05 TB 24 GB","title":"Monitor Specifications"},{"location":"requirements/03_monitor_specifications/#monitor-deployment-considerations","text":"Monitors combine Elasticsearch, and Kibana into a single instance, and receive traffic from the agents. \u24d8 Logstash is not installed by default, however it is available for installation as a separate component via: dynamite logstash install -h . Manual configuration is required to further integrate Logstash with a Dynamite monitor instance.","title":"Monitor Deployment Considerations"},{"location":"requirements/03_monitor_specifications/#data-drives","text":"A separate data storage volume should be created to maximize write performance and alleviate I/O contention with the OS. Creating a separate data volume also helps ensure storage consumption will not adversely affect OS operation. The data storage volume should be configured with: - 10K RPM HDD or SSD (higher I/O but fewer possible read/write operations over lifespan) - All drives the same make/model - RAID 0 for fast I/O","title":"Data Drives"},{"location":"requirements/03_monitor_specifications/#cpus","text":"Most operations within Elasticsearch are CPU bound, and there are many variables beyond events-per-second that contribute to load. The following options are a good starting place when benchmarking your monitor. Events per Second CPUs/Cores 250 4 1000 8 2500 12","title":"CPUs"},{"location":"requirements/03_monitor_specifications/#memory-disk","text":"Elasticsearch is built upon Lucene data-structures which require large HashMaps remain in memory at all time. Depending on the size of your indices, query operations can become very expensive. The following are a good place to start. \u24d8 To avoid I/O contention and ensure data storage consumption does not affect the OS\u2019s ability to function, Dynamite recommends creating a separate storage volume exclusively for use by the OS. The OS storage volume should be configured with: SSD drives for fast I/O, the same make/model RAID 1 for full data redundancy Events per Second RAM (GB) Disk (30-days) Elasticsearch JVM Heap 250 24 305 GB 8 GB 1000 32 1.22 TB 12 GB 2500 64 3.05 TB 24 GB","title":"Memory &amp; Disk"},{"location":"requirements/04_span_vs_tap/","text":"Introduction To be able to start forwarding events an agent must be deployed on a SPAN port or a network TAP. Both have their advantages and disadvantages. A SPAN port (Switch Port Analyzer), is a feature provided by most managed switches allowing a device to be plugged into a special port where traffic is mirrored from the other switch ports. A network TAP (Test Access Point) a dedicated device that transmit both the send and receive data streams simultaneously on separate channels. They are deployed in-line and are a single point of failure. Be careful when choosing a network tap! Pros and Cons Span Ports Available on almost all managed switches Does not sit inline, if the span port fails, it will not disrupt network connectivity. Remotely configurable Network Taps A high quality tap typically handles much better under high traffic load (will not drop packets.) Court admissible and provides forensically sound data/evidence. Have no IP address and no MAC address and are not vulnerable to conventional network attacks. Conclusion At the end of the day, network TAPs usually emerge as the best option, but SPAN ports are a very reasonable alternative if you expect low-medium levels of traffic or do not care especially about dropped packets.","title":"SPAN vs TAP"},{"location":"requirements/04_span_vs_tap/#introduction","text":"To be able to start forwarding events an agent must be deployed on a SPAN port or a network TAP. Both have their advantages and disadvantages. A SPAN port (Switch Port Analyzer), is a feature provided by most managed switches allowing a device to be plugged into a special port where traffic is mirrored from the other switch ports. A network TAP (Test Access Point) a dedicated device that transmit both the send and receive data streams simultaneously on separate channels. They are deployed in-line and are a single point of failure. Be careful when choosing a network tap!","title":"Introduction"},{"location":"requirements/04_span_vs_tap/#pros-and-cons","text":"","title":"Pros and Cons"},{"location":"requirements/04_span_vs_tap/#span-ports","text":"Available on almost all managed switches Does not sit inline, if the span port fails, it will not disrupt network connectivity. Remotely configurable","title":"Span Ports"},{"location":"requirements/04_span_vs_tap/#network-taps","text":"A high quality tap typically handles much better under high traffic load (will not drop packets.) Court admissible and provides forensically sound data/evidence. Have no IP address and no MAC address and are not vulnerable to conventional network attacks.","title":"Network Taps"},{"location":"requirements/04_span_vs_tap/#conclusion","text":"At the end of the day, network TAPs usually emerge as the best option, but SPAN ports are a very reasonable alternative if you expect low-medium levels of traffic or do not care especially about dropped packets.","title":"Conclusion"},{"location":"requirements/os_setup_guides/alma8/","text":"AlmaLinux 8 Preparation Setup Tasks Install Python3.8 For AlmaLinux 8, we suggest users use Python3.8 which has been tested extensively by our team. Install Required Dependencies dnf remove python3 python3-pip dnf module install python3.8 dnf install python38-devel Download Python 3.8.3 wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz Compile from Source tar xvf Python-3.8.3.tgz cd Python-3.8*/ ./configure --enable-optimizations --enable-loadable-sqlite-extensions sudo make altinstall Install DynamiteNSM Package Locate Python 3.8 whereis python3.8 Install DynamiteNSM via PIP sudo python3.8 -m pip install dynamite-nsm","title":"AlmaLinux 8 Preparation"},{"location":"requirements/os_setup_guides/alma8/#almalinux-8-preparation","text":"","title":"AlmaLinux 8 Preparation"},{"location":"requirements/os_setup_guides/alma8/#setup-tasks","text":"","title":"Setup Tasks"},{"location":"requirements/os_setup_guides/alma8/#install-python38","text":"For AlmaLinux 8, we suggest users use Python3.8 which has been tested extensively by our team.","title":"Install Python3.8"},{"location":"requirements/os_setup_guides/alma8/#install-required-dependencies","text":"dnf remove python3 python3-pip dnf module install python3.8 dnf install python38-devel","title":"Install Required Dependencies"},{"location":"requirements/os_setup_guides/alma8/#download-python-383","text":"wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz","title":"Download Python 3.8.3"},{"location":"requirements/os_setup_guides/alma8/#compile-from-source","text":"tar xvf Python-3.8.3.tgz cd Python-3.8*/ ./configure --enable-optimizations --enable-loadable-sqlite-extensions sudo make altinstall","title":"Compile from Source"},{"location":"requirements/os_setup_guides/alma8/#install-dynamitensm-package","text":"","title":"Install DynamiteNSM Package"},{"location":"requirements/os_setup_guides/alma8/#locate-python-38","text":"whereis python3.8","title":"Locate Python 3.8"},{"location":"requirements/os_setup_guides/alma8/#install-dynamitensm-via-pip","text":"sudo python3.8 -m pip install dynamite-nsm","title":"Install DynamiteNSM via PIP"},{"location":"requirements/os_setup_guides/centos7/","text":"Centos 7 Preparation Setup Tasks Install Python3.8 For CentOS 7, we suggest users use Python3.8 which has been tested extensively by our team. Install Required Dependencies sudo yum -y update sudo yum -y groupinstall \"Development Tools\" sudo yum -y install gcc gcc-c++ openssl-devel bzip2-devel libffi-devel wget Download Python 3.8.3 wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz Compile from Source tar xvf Python-3.8.3.tgz cd Python-3.8*/ ./configure --enable-optimizations --enable-loadable-sqlite-extensions sudo make altinstall Install DynamiteNSM Package Locate Python 3.8 whereis python3.8 Install DynamiteNSM via PIP sudo python3.8 -m pip install dynamite-nsm","title":"Centos 7 Preparation"},{"location":"requirements/os_setup_guides/centos7/#centos-7-preparation","text":"","title":"Centos 7 Preparation"},{"location":"requirements/os_setup_guides/centos7/#setup-tasks","text":"","title":"Setup Tasks"},{"location":"requirements/os_setup_guides/centos7/#install-python38","text":"For CentOS 7, we suggest users use Python3.8 which has been tested extensively by our team.","title":"Install Python3.8"},{"location":"requirements/os_setup_guides/centos7/#install-required-dependencies","text":"sudo yum -y update sudo yum -y groupinstall \"Development Tools\" sudo yum -y install gcc gcc-c++ openssl-devel bzip2-devel libffi-devel wget","title":"Install Required Dependencies"},{"location":"requirements/os_setup_guides/centos7/#download-python-383","text":"wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz","title":"Download Python 3.8.3"},{"location":"requirements/os_setup_guides/centos7/#compile-from-source","text":"tar xvf Python-3.8.3.tgz cd Python-3.8*/ ./configure --enable-optimizations --enable-loadable-sqlite-extensions sudo make altinstall","title":"Compile from Source"},{"location":"requirements/os_setup_guides/centos7/#install-dynamitensm-package","text":"","title":"Install DynamiteNSM Package"},{"location":"requirements/os_setup_guides/centos7/#locate-python-38","text":"whereis python3.8","title":"Locate Python 3.8"},{"location":"requirements/os_setup_guides/centos7/#install-dynamitensm-via-pip","text":"sudo python3.8 -m pip install dynamite-nsm","title":"Install DynamiteNSM via PIP"},{"location":"requirements/os_setup_guides/debian10/","text":"Debian 10 Preparation Setup Tasks Install Python Development Packages Debian 10 ships with Python 3.7.3 which has been tested extensively by our team. Install Python Development Packages sudo apt-get install python3-dev python3-pip Make sure /usr/sbin/ is in your $PATH Debian 10 doesn't include /usr/sbin/ in the path, causing some of our basic commandline wrappers to fail. sudo su export PATH=\"$PATH:/usr/sbin/\" source ~/.bashrc Install DynamiteNSM Package Install DynamiteNSM via PIP sudo python3.8 -m pip install dynamite-nsm","title":"Debian 10 Preparation"},{"location":"requirements/os_setup_guides/debian10/#debian-10-preparation","text":"","title":"Debian 10 Preparation"},{"location":"requirements/os_setup_guides/debian10/#setup-tasks","text":"","title":"Setup Tasks"},{"location":"requirements/os_setup_guides/debian10/#install-python-development-packages","text":"Debian 10 ships with Python 3.7.3 which has been tested extensively by our team.","title":"Install Python Development Packages"},{"location":"requirements/os_setup_guides/debian10/#install-python-development-packages_1","text":"sudo apt-get install python3-dev python3-pip","title":"Install Python Development Packages"},{"location":"requirements/os_setup_guides/debian10/#make-sure-usrsbin-is-in-your-path","text":"Debian 10 doesn't include /usr/sbin/ in the path, causing some of our basic commandline wrappers to fail. sudo su export PATH=\"$PATH:/usr/sbin/\" source ~/.bashrc","title":"Make sure /usr/sbin/ is in your $PATH"},{"location":"requirements/os_setup_guides/debian10/#install-dynamitensm-package","text":"","title":"Install DynamiteNSM Package"},{"location":"requirements/os_setup_guides/debian10/#install-dynamitensm-via-pip","text":"sudo python3.8 -m pip install dynamite-nsm","title":"Install DynamiteNSM via PIP"},{"location":"requirements/os_setup_guides/ubuntu1804/","text":"Ubuntu 18.04 Preparation Setup Tasks Install Python3.8 For Ubuntu 18.04, we suggest users use Python3.8 which has been tested extensively by our team. Install the Deadsnakes Repository sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa Install Python and Development Tools sudo apt-get install python3.8-dev python3.8-pip Install DynamiteNSM Package Locate Python 3.8 whereis python3.8 Install DynamiteNSM via PIP sudo python3.8 -m pip install dynamite-nsm","title":"Ubuntu 18.04 Preparation"},{"location":"requirements/os_setup_guides/ubuntu1804/#ubuntu-1804-preparation","text":"","title":"Ubuntu 18.04 Preparation"},{"location":"requirements/os_setup_guides/ubuntu1804/#setup-tasks","text":"","title":"Setup Tasks"},{"location":"requirements/os_setup_guides/ubuntu1804/#install-python38","text":"For Ubuntu 18.04, we suggest users use Python3.8 which has been tested extensively by our team.","title":"Install Python3.8"},{"location":"requirements/os_setup_guides/ubuntu1804/#install-the-deadsnakes-repository","text":"sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa","title":"Install the Deadsnakes Repository"},{"location":"requirements/os_setup_guides/ubuntu1804/#install-python-and-development-tools","text":"sudo apt-get install python3.8-dev python3.8-pip","title":"Install Python and Development Tools"},{"location":"requirements/os_setup_guides/ubuntu1804/#install-dynamitensm-package","text":"","title":"Install DynamiteNSM Package"},{"location":"requirements/os_setup_guides/ubuntu1804/#locate-python-38","text":"whereis python3.8","title":"Locate Python 3.8"},{"location":"requirements/os_setup_guides/ubuntu1804/#install-dynamitensm-via-pip","text":"sudo python3.8 -m pip install dynamite-nsm","title":"Install DynamiteNSM via PIP"},{"location":"requirements/os_setup_guides/ubuntu2004/","text":"Ubuntu 20.04 Preparation Setup Tasks Install Python Development Tools Python3 should already be installed by default on this version of Ubuntu, however you will need to install a few tools to install dynamite-nsm package. sudo apt-get install python3-dev python3-pip Install DynamiteNSM Package Locate Python 3.8 whereis python3.8 Install DynamiteNSM via PIP sudo python3 -m pip install dynamite-nsm","title":"Ubuntu 20.04 Preparation"},{"location":"requirements/os_setup_guides/ubuntu2004/#ubuntu-2004-preparation","text":"","title":"Ubuntu 20.04 Preparation"},{"location":"requirements/os_setup_guides/ubuntu2004/#setup-tasks","text":"","title":"Setup Tasks"},{"location":"requirements/os_setup_guides/ubuntu2004/#install-python-development-tools","text":"Python3 should already be installed by default on this version of Ubuntu, however you will need to install a few tools to install dynamite-nsm package. sudo apt-get install python3-dev python3-pip","title":"Install Python Development Tools"},{"location":"requirements/os_setup_guides/ubuntu2004/#install-dynamitensm-package","text":"","title":"Install DynamiteNSM Package"},{"location":"requirements/os_setup_guides/ubuntu2004/#locate-python-38","text":"whereis python3.8","title":"Locate Python 3.8"},{"location":"requirements/os_setup_guides/ubuntu2004/#install-dynamitensm-via-pip","text":"sudo python3 -m pip install dynamite-nsm","title":"Install DynamiteNSM via PIP"},{"location":"resources/coding_guidelines/","text":"Coding Guidelines Language Internally, we use a Python3.7 development environment on Ubuntu 20.04 . Styles Overriding Principle Names that are visible to the user as public parts of the API should follow conventions that reflect usage rather than implementation. Style Language Internals UPPERCASE constants lower_case_with_underscores methods, functions, and modules CapitalizedWords classes Tabs or Spaces Always use spaces instead of tabs. Indention Each indention should be 4 spaces . Maximum Line Length The maximum 120 characters. Blank lines Number of Blank Lines After Language Internals 2 function 1 method 2 import statement Other Styling Stuff In general, follow PEP-8 guidelines outlined here . Use a tool like flake8 to improve readability. Doc Strings All methods and functions should document their parameters and return types. Google Style Type-hints should be added to all method and function parameters. Third-Party Dependencies We love open-source software, but a poorly maintained project merged into the main branch can create technical debt down the line. If you have a third party dependency make sure it is well maintained.","title":"Coding Guidelines"},{"location":"resources/coding_guidelines/#coding-guidelines","text":"","title":"Coding Guidelines"},{"location":"resources/coding_guidelines/#language","text":"Internally, we use a Python3.7 development environment on Ubuntu 20.04 .","title":"Language"},{"location":"resources/coding_guidelines/#styles","text":"","title":"Styles"},{"location":"resources/coding_guidelines/#overriding-principle","text":"Names that are visible to the user as public parts of the API should follow conventions that reflect usage rather than implementation. Style Language Internals UPPERCASE constants lower_case_with_underscores methods, functions, and modules CapitalizedWords classes","title":"Overriding Principle"},{"location":"resources/coding_guidelines/#tabs-or-spaces","text":"Always use spaces instead of tabs.","title":"Tabs or Spaces"},{"location":"resources/coding_guidelines/#indention","text":"Each indention should be 4 spaces .","title":"Indention"},{"location":"resources/coding_guidelines/#maximum-line-length","text":"The maximum 120 characters.","title":"Maximum Line Length"},{"location":"resources/coding_guidelines/#blank-lines","text":"Number of Blank Lines After Language Internals 2 function 1 method 2 import statement","title":"Blank lines"},{"location":"resources/coding_guidelines/#other-styling-stuff","text":"In general, follow PEP-8 guidelines outlined here . Use a tool like flake8 to improve readability.","title":"Other Styling Stuff"},{"location":"resources/coding_guidelines/#doc-strings","text":"All methods and functions should document their parameters and return types. Google Style Type-hints should be added to all method and function parameters.","title":"Doc Strings"},{"location":"resources/coding_guidelines/#third-party-dependencies","text":"We love open-source software, but a poorly maintained project merged into the main branch can create technical debt down the line. If you have a third party dependency make sure it is well maintained.","title":"Third-Party Dependencies"},{"location":"resources/contributing_guide/","text":"Contribute to a DynamiteNSM Getting Started You do not need to be a developer to contribute to this project. DynamiteNSM is relatively modular in design and offers quite a few places to plug in. Dynamite Projects DynamiteNSM consists of several projects that are independent of one another. Project Description Contributing Guide dynamite-nsm Contains core libraries for setting up the Dynamite stack. View configurations Default configuration sets that can be used to deploy DynamiteNSM services in pre-defined states N/A kibana_packages Collections of Kibana saved_objects that can be managed via kibana package utility. View Types of Contribution Discussions are where we have conversations. If you'd like help troubleshooting a docs PR you're working on, have a great new idea, or want to share something amazing you've learned in our docs, join us in discussions . Issues are used to track tasks that contributors can help with. We'll use the issue to have a conversation about the problem you want to fix. A pull request is a way to suggest changes in our repository. When one is made, a member of our team will accept , reject or most often request changes before it is scheduled for a future release. Before going any further please check out our developer guide to better understand the areas we are receiving pull requests in.","title":"Contributing Guide"},{"location":"resources/contributing_guide/#contribute-to-a-dynamitensm","text":"","title":"Contribute to a DynamiteNSM"},{"location":"resources/contributing_guide/#getting-started","text":"You do not need to be a developer to contribute to this project. DynamiteNSM is relatively modular in design and offers quite a few places to plug in.","title":"Getting Started"},{"location":"resources/contributing_guide/#dynamite-projects","text":"DynamiteNSM consists of several projects that are independent of one another. Project Description Contributing Guide dynamite-nsm Contains core libraries for setting up the Dynamite stack. View configurations Default configuration sets that can be used to deploy DynamiteNSM services in pre-defined states N/A kibana_packages Collections of Kibana saved_objects that can be managed via kibana package utility. View","title":"Dynamite Projects"},{"location":"resources/contributing_guide/#types-of-contribution","text":"Discussions are where we have conversations. If you'd like help troubleshooting a docs PR you're working on, have a great new idea, or want to share something amazing you've learned in our docs, join us in discussions . Issues are used to track tasks that contributors can help with. We'll use the issue to have a conversation about the problem you want to fix. A pull request is a way to suggest changes in our repository. When one is made, a member of our team will accept , reject or most often request changes before it is scheduled for a future release. Before going any further please check out our developer guide to better understand the areas we are receiving pull requests in.","title":"Types of Contribution"},{"location":"services/00_setup/","text":"Setup Before any Dynamite services can be installed and managed the setup bootstrapper must be called. Setup provides two modes install and uninstall . The install command retrieves the latest default configurations, sets up the directory structure, and creates a dynamite policy in the /etc/sudoers.d/ directory. $ sudo dynamite setup -h usage: dynamite [-h] {install,uninstall} ... Setup DynamiteNSM 1.1.2 positional arguments: {install,uninstall} install Setup required files and directories. uninstall Uninstall DynamiteNSM on this machine. optional arguments: -h, --help show this help message and exit","title":"Setup"},{"location":"services/00_setup/#setup","text":"Before any Dynamite services can be installed and managed the setup bootstrapper must be called. Setup provides two modes install and uninstall . The install command retrieves the latest default configurations, sets up the directory structure, and creates a dynamite policy in the /etc/sudoers.d/ directory. $ sudo dynamite setup -h usage: dynamite [-h] {install,uninstall} ... Setup DynamiteNSM 1.1.2 positional arguments: {install,uninstall} install Setup required files and directories. uninstall Uninstall DynamiteNSM on this machine. optional arguments: -h, --help show this help message and exit","title":"Setup"},{"location":"services/01_updates/","text":"Updates The Dynamite team publishes a set of default configurations and mirrors for each minor release of dynamite-nsm. To take advantage of any new improvements to the default configuration sets simply run: sudo dynamite updates install . Now the next time you install a new service the new default configurations will be applied. $ dynamite updates install -h usage: dynamite [-h] {install} ... Update default configurations and mirrors. positional arguments: {install} install Update mirrors and default configurations optional arguments: -h, --help show this help message and exit Defaults Directories Default Configurations: /etc/dynamite/default_configs/ Mirrors: /etc/dynamite/mirrors/","title":"Updates"},{"location":"services/01_updates/#updates","text":"The Dynamite team publishes a set of default configurations and mirrors for each minor release of dynamite-nsm. To take advantage of any new improvements to the default configuration sets simply run: sudo dynamite updates install . Now the next time you install a new service the new default configurations will be applied. $ dynamite updates install -h usage: dynamite [-h] {install} ... Update default configurations and mirrors. positional arguments: {install} install Update mirrors and default configurations optional arguments: -h, --help show this help message and exit","title":"Updates"},{"location":"services/01_updates/#defaults","text":"","title":"Defaults"},{"location":"services/01_updates/#directories","text":"Default Configurations: /etc/dynamite/default_configs/ Mirrors: /etc/dynamite/mirrors/","title":"Directories"},{"location":"services/02_monitor/","text":"Monitor The monitor is a convenience service that provides a single interface around some of Elasticsearch and Kibana's interfaces. $ dynamite monitor -h usage: dynamite [-h] {install,uninstall,process} ... Monitor @ 192.168.194.143 positional arguments: {install,uninstall,process} install Install monitor components and configure this system to receive events and alerts from various agents. uninstall Uninstall the monitor components on this machine. process Manage Local Monitor processes. optional arguments: -h, --help show this help message and exit Installation sudo dynamite monitor install -h Configuration The monitor service does not present a wrapper interface for underlying configurations. These configurations must be accessed directly through the Elasticsearch or Kibana service commands. dynamite elasticsearch config -h dynamite kibana config -h Process Management dynamite agent process -h Troubleshooting Problem Description Solution Can't Access Web Interfaces You cannot access elasticsearch or kibana through their web-interfaces. Firstly, double check that the services are running: dynamite monitor process status . If they are, you may have a host based firewall enabled. By default, CentOS for example, enables firewalld service. You may need to modify the ACLs in order allow 9200 and 5601 accessible to outside hosts. Temporarily, you can disable firewalld via systemctl stop firwalld to verify this is the issue.","title":"Monitor"},{"location":"services/02_monitor/#monitor","text":"The monitor is a convenience service that provides a single interface around some of Elasticsearch and Kibana's interfaces. $ dynamite monitor -h usage: dynamite [-h] {install,uninstall,process} ... Monitor @ 192.168.194.143 positional arguments: {install,uninstall,process} install Install monitor components and configure this system to receive events and alerts from various agents. uninstall Uninstall the monitor components on this machine. process Manage Local Monitor processes. optional arguments: -h, --help show this help message and exit","title":"Monitor"},{"location":"services/02_monitor/#installation","text":"sudo dynamite monitor install -h","title":"Installation"},{"location":"services/02_monitor/#configuration","text":"The monitor service does not present a wrapper interface for underlying configurations. These configurations must be accessed directly through the Elasticsearch or Kibana service commands. dynamite elasticsearch config -h dynamite kibana config -h","title":"Configuration"},{"location":"services/02_monitor/#process-management","text":"dynamite agent process -h","title":"Process Management"},{"location":"services/02_monitor/#troubleshooting","text":"Problem Description Solution Can't Access Web Interfaces You cannot access elasticsearch or kibana through their web-interfaces. Firstly, double check that the services are running: dynamite monitor process status . If they are, you may have a host based firewall enabled. By default, CentOS for example, enables firewalld service. You may need to modify the ACLs in order allow 9200 and 5601 accessible to outside hosts. Temporarily, you can disable firewalld via systemctl stop firwalld to verify this is the issue.","title":"Troubleshooting"},{"location":"services/03_agent/","text":"Agent The agent is a convenience service that provides a single interface around some of Elasticsearch and Kibana's interfaces. $ dynamite agent -h usage: dynamite [-h] {install,uninstall,process,optimize} ... Agent @ 192.168.199.1 positional arguments: {install,uninstall,process,optimize} install Install agent components and configure this system as a sensor. uninstall Uninstall all the agent components on this machine. process Manage local Agent processes. optimize Automatically adjust how resources are allocated between Zeek and Suricata. optional arguments: -h, --help show this help message and exit Installation sudo dynamite agent install -h Configuration The agent service does not present a wrapper interface for underlying configurations. These configurations must be accessed directly through the Zeek , Suricata , or Filebeat service commands. dynamite elasticsearch config -h dynamite kibana config -h Process Management dynamite agent process -h","title":"Agent"},{"location":"services/03_agent/#agent","text":"The agent is a convenience service that provides a single interface around some of Elasticsearch and Kibana's interfaces. $ dynamite agent -h usage: dynamite [-h] {install,uninstall,process,optimize} ... Agent @ 192.168.199.1 positional arguments: {install,uninstall,process,optimize} install Install agent components and configure this system as a sensor. uninstall Uninstall all the agent components on this machine. process Manage local Agent processes. optimize Automatically adjust how resources are allocated between Zeek and Suricata. optional arguments: -h, --help show this help message and exit","title":"Agent"},{"location":"services/03_agent/#installation","text":"sudo dynamite agent install -h","title":"Installation"},{"location":"services/03_agent/#configuration","text":"The agent service does not present a wrapper interface for underlying configurations. These configurations must be accessed directly through the Zeek , Suricata , or Filebeat service commands. dynamite elasticsearch config -h dynamite kibana config -h","title":"Configuration"},{"location":"services/03_agent/#process-management","text":"dynamite agent process -h","title":"Process Management"},{"location":"services/04_elasticsearch/","text":"Elasticsearch Elasticsearch is a distributed, open-source search and analytics engine built on Apache Lucene and developed in Java. Within DynamiteNSM it is used to store all network events and alerts that have been acquired and normalized by the agent. DynamiteNSM pre-configures Elasticsearch with several useful defaults, and automatically optimizes its use of the JVM heap. $ sudo dynamite elasticsearch -h usage: dynamite [-h] {install,uninstall,process,config} ... Elasticsearch @ 192.168.194.143 positional arguments: {install,uninstall,process,config} install Install Elasticsearch as a standalone component. uninstall Uninstall Elasticsearch on this machine. process Manage local Elasticsearch node processes. config Modify Elasticsearch configurations. optional arguments: -h, --help show this help message and exit Installation sudo dynamite elasticsearch install -h Configuration dynamite elasticsearch config -h Process Management dynamite elasticsearch process -h Defaults Directories Configuration Directory: /etc/dynamite/elasticsearch/ Installation Directory: /opt/dynamite/elasticsearch/ Logs: /var/log/dynamite/elasticsearch/ JAVA_HOME: /usr/lib/jvm//jdk-13.0.1 Access API URL: https://<management-ip>:9200 Default User: admin Default Password: admin Troubleshooting Elasticsearch won't start Symptoms : You have started elasticsearch via the commandline utility or systemctl you wait 30 seconds and run the sudo dynamite elasticsearch process status command, and receive the following. \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 elasticsearch.process \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Running \u2502 no \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Enabled on Startup \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Logs \u2502 /var/log/dynamite/elasticsearch/ \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Command \u2502 sudo systemctl status elasticsearch.service \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Exit Code \u2502 3 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Problem Description Solution Out of Memory elasticsearch needs to be able to provision a certain amount of heap space (memory) at runtime. As the document store grows, various operations become more memory intensive and can prevent elasticsearch from starting Check the /var/log/dynamite/elasticsearch/dynamite-cluster.log for a message resembling the following: There is insufficient memory for the Java Runtime Environment to continue. If an entry like this is found you must increase the amount of memory on the machine. sudo systemctl status elasticsearch or sudo dynamite elasticsearch process status --verbose may also provide insights. Misconfiguration The elasticsearch.yaml controls the behavior of elasticsearch at runtime. It conforms to yaml format. If an invalid value is given or the yaml specification violated an error will be logged and elasticsearch will crash. Use a tool like yamlint to identify obvious issues. Check the Check the /var/log/dynamite/elasticsearch/dynamite-cluster.log for misconfiguration hints.","title":"Elasticsearch"},{"location":"services/04_elasticsearch/#elasticsearch","text":"Elasticsearch is a distributed, open-source search and analytics engine built on Apache Lucene and developed in Java. Within DynamiteNSM it is used to store all network events and alerts that have been acquired and normalized by the agent. DynamiteNSM pre-configures Elasticsearch with several useful defaults, and automatically optimizes its use of the JVM heap. $ sudo dynamite elasticsearch -h usage: dynamite [-h] {install,uninstall,process,config} ... Elasticsearch @ 192.168.194.143 positional arguments: {install,uninstall,process,config} install Install Elasticsearch as a standalone component. uninstall Uninstall Elasticsearch on this machine. process Manage local Elasticsearch node processes. config Modify Elasticsearch configurations. optional arguments: -h, --help show this help message and exit","title":"Elasticsearch"},{"location":"services/04_elasticsearch/#installation","text":"sudo dynamite elasticsearch install -h","title":"Installation"},{"location":"services/04_elasticsearch/#configuration","text":"dynamite elasticsearch config -h","title":"Configuration"},{"location":"services/04_elasticsearch/#process-management","text":"dynamite elasticsearch process -h","title":"Process Management"},{"location":"services/04_elasticsearch/#defaults","text":"","title":"Defaults"},{"location":"services/04_elasticsearch/#directories","text":"Configuration Directory: /etc/dynamite/elasticsearch/ Installation Directory: /opt/dynamite/elasticsearch/ Logs: /var/log/dynamite/elasticsearch/ JAVA_HOME: /usr/lib/jvm//jdk-13.0.1","title":"Directories"},{"location":"services/04_elasticsearch/#access","text":"API URL: https://<management-ip>:9200 Default User: admin Default Password: admin","title":"Access"},{"location":"services/04_elasticsearch/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"services/04_elasticsearch/#elasticsearch-wont-start","text":"Symptoms : You have started elasticsearch via the commandline utility or systemctl you wait 30 seconds and run the sudo dynamite elasticsearch process status command, and receive the following. \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502 Service \u2502 elasticsearch.process \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Running \u2502 no \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Enabled on Startup \u2502 yes \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Logs \u2502 /var/log/dynamite/elasticsearch/ \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Command \u2502 sudo systemctl status elasticsearch.service \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Exit Code \u2502 3 \u2502 \u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b Problem Description Solution Out of Memory elasticsearch needs to be able to provision a certain amount of heap space (memory) at runtime. As the document store grows, various operations become more memory intensive and can prevent elasticsearch from starting Check the /var/log/dynamite/elasticsearch/dynamite-cluster.log for a message resembling the following: There is insufficient memory for the Java Runtime Environment to continue. If an entry like this is found you must increase the amount of memory on the machine. sudo systemctl status elasticsearch or sudo dynamite elasticsearch process status --verbose may also provide insights. Misconfiguration The elasticsearch.yaml controls the behavior of elasticsearch at runtime. It conforms to yaml format. If an invalid value is given or the yaml specification violated an error will be logged and elasticsearch will crash. Use a tool like yamlint to identify obvious issues. Check the Check the /var/log/dynamite/elasticsearch/dynamite-cluster.log for misconfiguration hints.","title":"Elasticsearch won't start"},{"location":"services/04_kibana_package/","text":"Kibana DynamiteNSM ships with a simple package manager for managing the installation of visualizations, dashboards, and saved_searches. These objects are grouped into self-contained packages that can easily be installed to a specific tenant. $ sudo dynamite kibana package -h usage: dynamite package [-h] {install,list,list-tenants,list-saved-objects,uninstall} ... positional arguments: {install,list,list-tenants,list-saved-objects,uninstall} optional arguments: -h, --help show this help message and exit Package Installation dynamite kibana package install -h Package Uninstallation dynamite kibana package uninstall -h","title":"Kibana Package Manager"},{"location":"services/04_kibana_package/#kibana","text":"DynamiteNSM ships with a simple package manager for managing the installation of visualizations, dashboards, and saved_searches. These objects are grouped into self-contained packages that can easily be installed to a specific tenant. $ sudo dynamite kibana package -h usage: dynamite package [-h] {install,list,list-tenants,list-saved-objects,uninstall} ... positional arguments: {install,list,list-tenants,list-saved-objects,uninstall} optional arguments: -h, --help show this help message and exit","title":"Kibana"},{"location":"services/04_kibana_package/#package-installation","text":"dynamite kibana package install -h","title":"Package Installation"},{"location":"services/04_kibana_package/#package-uninstallation","text":"dynamite kibana package uninstall -h","title":"Package Uninstallation"},{"location":"services/05_logstash/","text":"Logstash Logstash is an open-source server-side data processing pipeline that allows you to collect data from a variety of sources, transform it on the fly, and send it to your desired destination. \u24d8 Logstash is an optional component within the Dynamite stack and is not installed as part of the monitor . DynamiteNSM does not currently provide an automated integration strategy with Logstash. Once installed, Logstash must be manually configured to listen for events from the agent , forwarding them downstream to a collector of your choice. DynamiteNSM pre-configures Logstash with several useful defaults, and automatically optimizes its use of the JVM heap. $ dynamite logstash -h usage: dynamite [-h] {install,uninstall,process,config} ... Logstash @ 192.168.194.143 positional arguments: {install,uninstall,process,config} install Install Logstash as a standalone component. uninstall Uninstall Logstash on this machine. process Manage local Logstash instance. config Modify Logstash configurations. optional arguments: -h, --help show this help message and exit Installation sudo dynamite logstash install -h Configuration dynamite logstash config -h Process Management dynamite logstash process -h Defaults Directories Configuration Directory: /etc/dynamite/logstash/ Installation Directory: /opt/dynamite/logstash/ Logs: /var/log/dynamite/logstash/ JAVA_HOME: /usr/lib/jvm//jdk-13.0.1","title":"Logstash"},{"location":"services/05_logstash/#logstash","text":"Logstash is an open-source server-side data processing pipeline that allows you to collect data from a variety of sources, transform it on the fly, and send it to your desired destination. \u24d8 Logstash is an optional component within the Dynamite stack and is not installed as part of the monitor . DynamiteNSM does not currently provide an automated integration strategy with Logstash. Once installed, Logstash must be manually configured to listen for events from the agent , forwarding them downstream to a collector of your choice. DynamiteNSM pre-configures Logstash with several useful defaults, and automatically optimizes its use of the JVM heap. $ dynamite logstash -h usage: dynamite [-h] {install,uninstall,process,config} ... Logstash @ 192.168.194.143 positional arguments: {install,uninstall,process,config} install Install Logstash as a standalone component. uninstall Uninstall Logstash on this machine. process Manage local Logstash instance. config Modify Logstash configurations. optional arguments: -h, --help show this help message and exit","title":"Logstash"},{"location":"services/05_logstash/#installation","text":"sudo dynamite logstash install -h","title":"Installation"},{"location":"services/05_logstash/#configuration","text":"dynamite logstash config -h","title":"Configuration"},{"location":"services/05_logstash/#process-management","text":"dynamite logstash process -h","title":"Process Management"},{"location":"services/05_logstash/#defaults","text":"","title":"Defaults"},{"location":"services/05_logstash/#directories","text":"Configuration Directory: /etc/dynamite/logstash/ Installation Directory: /opt/dynamite/logstash/ Logs: /var/log/dynamite/logstash/ JAVA_HOME: /usr/lib/jvm//jdk-13.0.1","title":"Directories"},{"location":"services/06_kibana/","text":"Kibana Kibana is a free and open-source frontend application that sits on top of Elasticsearch, and provides search and data visualization capabilities. DynamiteNSM automatically sets up Kibana with a rich collection of data visualizations and views useful for exploring your network from a variety of different perspectives. Additional packages can be installed via the kibana package utility. $ sudo dynamite kibana -h usage: dynamite [-h] {install,uninstall,process,config,package} ... Kibana @ 192.168.194.143 positional arguments: {install,uninstall,process,config,package} install Install Kibana as a standalone component. uninstall Uninstall Kibana on this machine. process Manage local Kibana node processes. config Modify Kibana configurations. package Add, remove, and manage packages created for Dynamite Kibana. optional arguments: -h, --help show this help message and exit Installation sudo dynamite kibana install -h Configuration dynamite kibana config -h Process Management dynamite kibana process -h Defaults Directories Configuration Directory: /etc/dynamite/kibana/ Installation Directory: /opt/dynamite/kibana/ Logs: /var/log/dynamite/kibana/ Access API URL: http://<management-ip>:5601 Default User: admin Default Password: admin","title":"Kibana"},{"location":"services/06_kibana/#kibana","text":"Kibana is a free and open-source frontend application that sits on top of Elasticsearch, and provides search and data visualization capabilities. DynamiteNSM automatically sets up Kibana with a rich collection of data visualizations and views useful for exploring your network from a variety of different perspectives. Additional packages can be installed via the kibana package utility. $ sudo dynamite kibana -h usage: dynamite [-h] {install,uninstall,process,config,package} ... Kibana @ 192.168.194.143 positional arguments: {install,uninstall,process,config,package} install Install Kibana as a standalone component. uninstall Uninstall Kibana on this machine. process Manage local Kibana node processes. config Modify Kibana configurations. package Add, remove, and manage packages created for Dynamite Kibana. optional arguments: -h, --help show this help message and exit","title":"Kibana"},{"location":"services/06_kibana/#installation","text":"sudo dynamite kibana install -h","title":"Installation"},{"location":"services/06_kibana/#configuration","text":"dynamite kibana config -h","title":"Configuration"},{"location":"services/06_kibana/#process-management","text":"dynamite kibana process -h","title":"Process Management"},{"location":"services/06_kibana/#defaults","text":"","title":"Defaults"},{"location":"services/06_kibana/#directories","text":"Configuration Directory: /etc/dynamite/kibana/ Installation Directory: /opt/dynamite/kibana/ Logs: /var/log/dynamite/kibana/","title":"Directories"},{"location":"services/06_kibana/#access","text":"API URL: http://<management-ip>:5601 Default User: admin Default Password: admin","title":"Access"},{"location":"services/07_zeek/","text":"Zeek Zeek (formerly Bro) is a free and open-source software network analysis framework. It provides an extremely powerful scripting language that can be used for everything from protocol parsing to file carving. Within DynamiteNSM, Zeek serves as the primary mechanism for harvesting metadata around network conversations. Through its scripting framework, Zeek is capable of generating many logs that provide rich context around and alerts. $ dynamite zeek -h usage: dynamite [-h] {install,uninstall,process,config,logs} ... Zeek @ 192.168.199.1 positional arguments: {install,uninstall,process,config,logs} install Install Zeek as a standalone component. uninstall Uninstall Zeek on this machine. process Manage local Zeek node processes. config Modify Zeek configurations. logs Attach to various Zeek logs. optional arguments: -h, --help show this help message and exit Installation sudo dynamite zeek install -h Configuration dynamite zeek config -h Scripts Configuration dynamite zeek config site scripts -h Process Management sudo dynamite zeek process -h View Logs dynamite zeek logs -h Defaults Directories Installation Directory: /opt/dynamite/zeek/ Configuration Directory: /etc/dynamite/zeek Logs: /opt/dynamite/zeek/logs/current/","title":"Zeek"},{"location":"services/07_zeek/#zeek","text":"Zeek (formerly Bro) is a free and open-source software network analysis framework. It provides an extremely powerful scripting language that can be used for everything from protocol parsing to file carving. Within DynamiteNSM, Zeek serves as the primary mechanism for harvesting metadata around network conversations. Through its scripting framework, Zeek is capable of generating many logs that provide rich context around and alerts. $ dynamite zeek -h usage: dynamite [-h] {install,uninstall,process,config,logs} ... Zeek @ 192.168.199.1 positional arguments: {install,uninstall,process,config,logs} install Install Zeek as a standalone component. uninstall Uninstall Zeek on this machine. process Manage local Zeek node processes. config Modify Zeek configurations. logs Attach to various Zeek logs. optional arguments: -h, --help show this help message and exit","title":"Zeek"},{"location":"services/07_zeek/#installation","text":"sudo dynamite zeek install -h","title":"Installation"},{"location":"services/07_zeek/#configuration","text":"dynamite zeek config -h","title":"Configuration"},{"location":"services/07_zeek/#scripts-configuration","text":"dynamite zeek config site scripts -h","title":"Scripts Configuration"},{"location":"services/07_zeek/#process-management","text":"sudo dynamite zeek process -h","title":"Process Management"},{"location":"services/07_zeek/#view-logs","text":"dynamite zeek logs -h","title":"View Logs"},{"location":"services/07_zeek/#defaults","text":"","title":"Defaults"},{"location":"services/07_zeek/#directories","text":"Installation Directory: /opt/dynamite/zeek/ Configuration Directory: /etc/dynamite/zeek Logs: /opt/dynamite/zeek/logs/current/","title":"Directories"},{"location":"services/08_suricata/","text":"Suricata Suricata is a leading independent open source threat detection engine that combines intrusion detection and community maintained rule-sets to quickly identify sophisticated threats. Within DynamiteNSM, Suricata is used primarily for identifying known suspicious or malicious activity. By default, Suricata runs the EmergingThreat Open rule-set , which is updated daily. To make sure you are running the latest rules run sudo dynamite suricata update . $ dynamite suricata -h usage: dynamite [-h] {install,uninstall,update,process,config,logs} ... Suricata @ 192.168.199.1 positional arguments: {install,uninstall,update,process,config,logs} install Install Suricata as a standalone component. uninstall Uninstall Suricata this machine. update Install the latest Suricata rule-sets. process Manage local Suricata node processes. config Modify Suricata configurations. logs Attach to various Suricata logs. optional arguments: -h, --help show this help message and exit Installation sudo dynamite suricata install -h Configuration dynamite suricata config -h Scripts Configuration dynamite suricata config main rules -h Process Management dynamite suricata process -h View Logs dynamite suricata logs -h Defaults Directories Installation Directory: /opt/dynamite/suricata/ Configuration Directory: /etc/dynamite/suricata/ Rules Directory: /etc/dynamite/suricata/rules Logs: /var/log/dynamite/suricata/","title":"Suricata"},{"location":"services/08_suricata/#suricata","text":"Suricata is a leading independent open source threat detection engine that combines intrusion detection and community maintained rule-sets to quickly identify sophisticated threats. Within DynamiteNSM, Suricata is used primarily for identifying known suspicious or malicious activity. By default, Suricata runs the EmergingThreat Open rule-set , which is updated daily. To make sure you are running the latest rules run sudo dynamite suricata update . $ dynamite suricata -h usage: dynamite [-h] {install,uninstall,update,process,config,logs} ... Suricata @ 192.168.199.1 positional arguments: {install,uninstall,update,process,config,logs} install Install Suricata as a standalone component. uninstall Uninstall Suricata this machine. update Install the latest Suricata rule-sets. process Manage local Suricata node processes. config Modify Suricata configurations. logs Attach to various Suricata logs. optional arguments: -h, --help show this help message and exit","title":"Suricata"},{"location":"services/08_suricata/#installation","text":"sudo dynamite suricata install -h","title":"Installation"},{"location":"services/08_suricata/#configuration","text":"dynamite suricata config -h","title":"Configuration"},{"location":"services/08_suricata/#scripts-configuration","text":"dynamite suricata config main rules -h","title":"Scripts Configuration"},{"location":"services/08_suricata/#process-management","text":"dynamite suricata process -h","title":"Process Management"},{"location":"services/08_suricata/#view-logs","text":"dynamite suricata logs -h","title":"View Logs"},{"location":"services/08_suricata/#defaults","text":"","title":"Defaults"},{"location":"services/08_suricata/#directories","text":"Installation Directory: /opt/dynamite/suricata/ Configuration Directory: /etc/dynamite/suricata/ Rules Directory: /etc/dynamite/suricata/rules Logs: /var/log/dynamite/suricata/","title":"Directories"},{"location":"services/09_filebeat/","text":"Filebeat Filebeat-OSS is a free and open-source log shipper written in GoLang. The utility is capable of forwarding logs to a variety of destination types. DynamiteNSM relies on Filebeat for some initial formatting and normalization of Zeek and Suricata logs and of course sending the logs on through a supported connector . $ sudo dynamite filebeat -h usage: dynamite [-h] {install,uninstall,process,config,logs} ... Filebeat @ 192.168.199.1 positional arguments: {install,uninstall,process,config,logs} install Install Filebeat as a standalone component. uninstall Uninstall Filebeat on this machine. process Manage local Filebeat processes. config Modify Filebeat configuration logs Attach to various Filebeat logs. optional arguments: -h, --help show this help message and exit Installation sudo dynamite filebeat install -h Configuration dynamite filebeat config -h Process Management dynamite filebeat process -h View Logs dynamite suricata logs -h Defaults Directories Installation Directory: /opt/dynamite/filebeat/ Files Configuration: /opt/dynamite/filebeat/filebeat.yml Logs: /opt/dynamite/filebeat/logs/filebeat","title":"Filebeat"},{"location":"services/09_filebeat/#filebeat","text":"Filebeat-OSS is a free and open-source log shipper written in GoLang. The utility is capable of forwarding logs to a variety of destination types. DynamiteNSM relies on Filebeat for some initial formatting and normalization of Zeek and Suricata logs and of course sending the logs on through a supported connector . $ sudo dynamite filebeat -h usage: dynamite [-h] {install,uninstall,process,config,logs} ... Filebeat @ 192.168.199.1 positional arguments: {install,uninstall,process,config,logs} install Install Filebeat as a standalone component. uninstall Uninstall Filebeat on this machine. process Manage local Filebeat processes. config Modify Filebeat configuration logs Attach to various Filebeat logs. optional arguments: -h, --help show this help message and exit","title":"Filebeat"},{"location":"services/09_filebeat/#installation","text":"sudo dynamite filebeat install -h","title":"Installation"},{"location":"services/09_filebeat/#configuration","text":"dynamite filebeat config -h","title":"Configuration"},{"location":"services/09_filebeat/#process-management","text":"dynamite filebeat process -h","title":"Process Management"},{"location":"services/09_filebeat/#view-logs","text":"dynamite suricata logs -h","title":"View Logs"},{"location":"services/09_filebeat/#defaults","text":"","title":"Defaults"},{"location":"services/09_filebeat/#directories","text":"Installation Directory: /opt/dynamite/filebeat/","title":"Directories"},{"location":"services/09_filebeat/#files","text":"Configuration: /opt/dynamite/filebeat/filebeat.yml Logs: /opt/dynamite/filebeat/logs/filebeat","title":"Files"},{"location":"services/10_auth/","text":"Authentication Being able to manage multiple DynamiteNSM instances remotely is critical for large scale deployments. The auth service allows users to control which hosts can remotely manage a local instance. The service provides the ability to install Authentication Packages created on one or more remote manager machines. Once installed, the remote machine can invoke dynamite commands on enabled instances. $ sudo dynamite auth -h usage: dynamite [-h] {install,uninstall} ... Dynamite Authentication Manager @ 192.168.86.222 positional arguments: {install,uninstall} install Install a remote manager authentication package. uninstall Uninstall a remote manager authentication package. Before you Begin Disambiguation The dynamite auth command that should be invoked on the machine you wish to control from an external host. It is responsible for installing an authentication package generated by the separate, dynamite-remote utility . The dynamite-remote utility is a self-contained script that also ships with dynamite-nsm. The utility allows administrators to create Authentication packages that once installed on remote instances allow remote management. The dynamite-remote script works on most *NIX operating systems with openssh-client installed. Generate an Authentication Package The dynamite-remote utility generates key-pairs then packages the public key along with some helpful metadata into an archive that can be installed on any instance with both openssh-server and dynamite-nsm installed. dynamite-remote create --name agent-lab-lan --host agent.lab.local --description \"Lab Environment Agent - Zeek and Suricata Copy the agent.lab.local.tar.gz to your remote node using a tool such as scp . Authentication Package Installation sudo dynamite auth install --archive agent.lab.local.tar.gz Remote Command Execution On the manager you should now be able to run commands like the one below. dynamite-remote execute nsm-es \"agent -h\" \u26a0\ufe0fAlways encapsulate the command in quotes to prevent dynamite-remote from misinterpreting commandline flags.","title":"Authentication"},{"location":"services/10_auth/#authentication","text":"Being able to manage multiple DynamiteNSM instances remotely is critical for large scale deployments. The auth service allows users to control which hosts can remotely manage a local instance. The service provides the ability to install Authentication Packages created on one or more remote manager machines. Once installed, the remote machine can invoke dynamite commands on enabled instances. $ sudo dynamite auth -h usage: dynamite [-h] {install,uninstall} ... Dynamite Authentication Manager @ 192.168.86.222 positional arguments: {install,uninstall} install Install a remote manager authentication package. uninstall Uninstall a remote manager authentication package.","title":"Authentication"},{"location":"services/10_auth/#before-you-begin","text":"","title":"Before you Begin"},{"location":"services/10_auth/#disambiguation","text":"The dynamite auth command that should be invoked on the machine you wish to control from an external host. It is responsible for installing an authentication package generated by the separate, dynamite-remote utility . The dynamite-remote utility is a self-contained script that also ships with dynamite-nsm. The utility allows administrators to create Authentication packages that once installed on remote instances allow remote management. The dynamite-remote script works on most *NIX operating systems with openssh-client installed.","title":"Disambiguation"},{"location":"services/10_auth/#generate-an-authentication-package","text":"The dynamite-remote utility generates key-pairs then packages the public key along with some helpful metadata into an archive that can be installed on any instance with both openssh-server and dynamite-nsm installed. dynamite-remote create --name agent-lab-lan --host agent.lab.local --description \"Lab Environment Agent - Zeek and Suricata Copy the agent.lab.local.tar.gz to your remote node using a tool such as scp .","title":"Generate an Authentication Package"},{"location":"services/10_auth/#authentication-package-installation","text":"sudo dynamite auth install --archive agent.lab.local.tar.gz","title":"Authentication Package Installation"},{"location":"services/10_auth/#remote-command-execution","text":"On the manager you should now be able to run commands like the one below. dynamite-remote execute nsm-es \"agent -h\" \u26a0\ufe0fAlways encapsulate the command in quotes to prevent dynamite-remote from misinterpreting commandline flags.","title":"Remote Command Execution"}]}